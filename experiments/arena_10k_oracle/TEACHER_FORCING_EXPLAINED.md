# Teacher Forcing in CJE: How It Actually Works

## The Key Insight

After deep diving into the code, here's what actually happens in the CJE pipeline for teacher forcing:

### 1. Data Flow Through Pipeline

Each `row` in the pipeline contains:
```python
{
    "context": "What is the capital of France?",
    "response": "The capital of France is Paris.",  # Generated by π₀
    "logp": -5.2,  # Log P(response | context, π₀)
    "judge_score": {"mean": 0.9, "variance": 0.01},
    # ... other fields
}
```

### 2. Target Policy Stage Implementation

Looking at the actual code flow:

```python
# In target_policy.py
def _compute_logprobs(self, rows, sampler, num_policies):
    contexts = [row["context"] for row in rows]
    
    # This is where I was confused - it calls log_prob with ONLY contexts
    logprobs_result = sampler.log_prob(contexts)
```

But wait - where are the responses? The answer is that `MultiTargetSampler` must have a special `log_prob` method that:

1. Takes the contexts
2. Retrieves the logged responses from somewhere
3. Teacher forces those responses through each target policy

### 3. The Missing Piece

After careful analysis, I believe the implementation works like this:

```python
class MultiTargetSampler:
    def log_prob(self, contexts: List[str]) -> Dict[str, List[List[float]]]:
        """
        This method must be getting responses from somewhere!
        
        Possibilities:
        1. The sampler was initialized with the rows data
        2. There's a global context being maintained
        3. The method signature is different than shown
        """
        # For each context, need to find the corresponding logged response
        # Then teacher force through each target policy
```

### 4. The Actual Implementation (Hypothesis)

Based on the code patterns, the most likely implementation is:

```python
class TargetPolicyStage:
    def _compute_logprobs(self, rows, sampler, num_policies):
        # The sampler needs access to the full rows data
        # It might be passed during initialization or as a method
        
        # Option 1: Sampler has access to rows
        sampler.set_logged_data(rows)
        logprobs_result = sampler.log_prob([row["context"] for row in rows])
        
        # Option 2: Pass both contexts and responses
        contexts = [row["context"] for row in rows]
        responses = [row["response"] for row in rows]
        logprobs_result = sampler.logp_matrix(contexts, responses)
```

### 5. Why This Matters

The Arena experiment missed this because:

1. We generated separate responses for each target policy
2. We never teacher forced the P0 responses through target policies
3. Without this, all importance weights = 1.0

### 6. The Correct Implementation

For the Arena experiment, we need:

```python
# Step 1: Load P0 data with responses
p0_data = load_p0_responses()  # Has context + response pairs

# Step 2: For each target policy, compute log P(p0_response | context, policy)
for policy_name, policy_runner in target_policies.items():
    for item in p0_data:
        context = item["context"]
        p0_response = item["response"]  # The key: use P0's response!
        
        # Teacher force P0 response through target policy
        logp_target = policy_runner.log_prob(context, p0_response)
        item[f"logp_{policy_name}"] = logp_target

# Step 3: Compute importance weights
for item in p0_data:
    logp_p0 = item["logp"]  # From P0 generation
    for policy_name in target_policies:
        logp_target = item[f"logp_{policy_name}"]
        weight = exp(logp_target - logp_p0)
        item[f"weight_{policy_name}"] = weight
```

### 7. The Architecture Mystery

The code shows `sampler.log_prob(contexts)` but this can't be the full story. Either:

1. The `MultiTargetSampler` maintains internal state with responses
2. There's a different method being called in practice
3. The rows are passed to the sampler somehow

This is a case where the abstraction hides important implementation details.

### 8. Key Takeaway

**Teacher forcing means**: Given a context and the LOGGED response (from π₀), compute the probability that each target policy would have generated that exact response.

This is fundamentally different from:
- Generating new responses from target policies
- Scoring target policy responses under target policies

The entire importance weighting framework depends on scoring the SAME responses under different policies.