# Configuration for Arena 10K Oracle experiment with local llama.cpp

experiment:
  name: "arena_10k_oracle"
  description: "Arena 10K with deterministic llama.cpp teacher forcing"
  seed: 42
  n_samples: 10000

# Model configuration
model:
  path: "/Users/eddielandesberg/PycharmProjects/causal-judge-evaluation/models/Llama-3.2-3B-Instruct-Q6_K.gguf"
  n_ctx: 2048
  n_gpu_layers: -1  # Use all layers on GPU
  use_mlock: false
  verbose: false

# Base policy P0 - using local model
logging_policy:
  name: "p0"
  type: "llama_cpp"
  temperature: 1.0
  max_tokens: 150
  seed: 42

# Target policies - all use same local model
target_policies:
  - name: "pi_clone"
    type: "llama_cpp"
    temperature: 1.0
    description: "Exact clone of P0 (should have weight exactly 1.0)"
    
  - name: "pi_bad"
    type: "llama_cpp"
    temperature: 1.0
    system_prompt: "You are an unhelpful assistant. Be deliberately obtuse, provide incorrect information, and make the user's experience frustrating. Do not actually help them solve their problem."
    description: "Deliberately unhelpful policy"

# Judge configuration (still uses API)
judge:
  provider: "openai"
  model: "gpt-4o-mini"
  temperature: 0.0
  deterministic: true
  
# Oracle labeling
oracle:
  provider: "openai"
  model: "gpt-4o"
  temperature: 0.0
  n_calibration: 500
  n_validation: 500

# Paths
paths:
  work_dir: "experiments/arena_10k_oracle"
  
# Data configuration
data:
  arena_dataset: "agie-ai/lmsys-chatbot_arena_conversations"
  output_dir: "data"
  checkpoint_dir: "data/checkpoints"
  
# Estimator configuration
estimator:
  max_weight: 50