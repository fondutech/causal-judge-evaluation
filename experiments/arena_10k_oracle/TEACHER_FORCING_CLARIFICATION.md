# Teacher Forcing Clarification: Full Input Sequence

## Key Insight

Teacher forcing must use the **exact same token sequence** that was fed to the logging policy, not just the raw context.

## What This Includes

The full input sequence typically contains:
1. **System prompt** (if any)
2. **User message template** formatting
3. **The original context/prompt**
4. **Additional instructions** (e.g., "Let's think step by step" for CoT)
5. **Any other formatting** specific to the logging policy

## Example

Raw context:
```
"What is the capital of France?"
```

Actual input to logging policy π₀:
```
<system>You are a helpful assistant.</system>
<user>What is the capital of France? Please think step by step.</user>
<assistant>
```

## Implication for Implementation

When computing `log P(response | input, π_target)`, we need:
- **input**: The FULL formatted sequence (including system prompt, templates, etc.)
- **response**: The exact response generated by π₀

## Why This Matters

Using only the raw context would give incorrect log probabilities because:
1. The model expects a specific format
2. System prompts significantly affect response probabilities
3. Instructions like CoT change the distribution

## Correct Implementation

```python
def compute_target_logprobs(row, target_policy):
    # Use the FULL input that was given to logging policy
    full_input = row["formatted_input"]  # or row["full_context"]
    response = row["response"]
    
    # Teacher force through target policy
    logp = target_policy.log_prob(full_input, response)
    return logp
```

## In the CJE Pipeline

The pipeline should preserve:
- `context`: Original user prompt
- `formatted_context`: Full input sequence to logging policy
- `response`: Generated response
- `logp`: Log probability under logging policy

Then target policies use `formatted_context`, not just `context`.