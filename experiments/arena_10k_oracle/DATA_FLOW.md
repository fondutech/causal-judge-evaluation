# Data Flow

## Overview

This experiment has TWO separate data streams that must not be confused:

1. **CALIBRATION STREAM**: For calibrating judge scores to human preferences
2. **VALIDATION STREAM**: For validating CJE's predictions

## File Inventory

### Input Files
- `data/prompts.jsonl` - 10,000 ChatBot Arena prompts (source for both streams)

### Calibration Stream Files
- `data/p0_replies.jsonl` - 10,000 π₀ (logging policy) responses
  - Generated by: `phase1_dataset_preparation/02_generate_logs.py`
  - Used for: Judge calibration
  - Oracle labels: 2,500 (25% sample)

### Validation Stream Files  
- `data/target_ground_truth.jsonl` - 1,500 target policy responses
  - Generated by: `phase1_dataset_preparation/02b_generate_target_ground_truth.py`
  - Contains: 500 prompts × 3 policies (π_cot, π_bigger_model, π_bad)
  - Oracle labels: 1,500 (100%)

### Oracle Label Files
- `data/labeling/oracle_labels_calibration_detailed.jsonl` - Calibration oracle labels
- `data/labeling/oracle_labels_validation_detailed.jsonl` - Validation oracle labels
  - Generated by: `phase1_dataset_preparation/03_generate_oracle_labels.py`
  - Total: 4,000 labels (2,500 calibration + 1,500 validation)
- `data/labeling/oracle_labels.csv` - Combined CSV format (generated by phase2 scripts)

## Key Points to Remember

1. **π₀ is ONLY for calibration** - it's the logging policy, not a target policy
2. **Target policies are π_cot, π_bigger_model, π_bad** - these are what we're evaluating
3. **Don't mix streams** - calibration data trains the calibrator, validation data tests CJE
4. **Oracle labels serve different purposes**:
   - Calibration labels: Train isotonic regression
   - Validation labels: Measure CJE accuracy

## Common Confusions to Avoid

❌ **Wrong**: "Why isn't π₀ in the validation set?"
✅ **Right**: π₀ is the logging policy used for calibration only

❌ **Wrong**: Using all 10,000 π₀ responses for oracle labeling
✅ **Right**: Only 25% (2,500) need oracle labels for calibration

❌ **Wrong**: Mixing calibration and validation metrics
✅ **Right**: Report them separately - they measure different things

## Pipeline Commands

```bash
# 1. Generate responses (if not already done)
cd phase1_dataset_preparation
python 02_generate_logs.py              # → p0_replies.jsonl
python 02b_generate_target_ground_truth.py  # → target_ground_truth.jsonl

# 2. Generate oracle labels for BOTH streams
# First, set your API key
export OPENAI_API_KEY="your-api-key-here"

# Run oracle labeling (takes ~2-3 hours for 4,000 labels)
python 03_generate_oracle_labels.py \
    --model o3-2025-04-16 \
    --temperature 1.0

# For testing with a small sample:
python 03_generate_oracle_labels.py \
    --model gpt-4o \
    --calibration-fraction 0.01 \
    --validation-fraction 0.1

# Resume from checkpoint if interrupted:
# Just run the same command - it will auto-resume

# 3. Run judge scoring (both methods)
python 04a_deterministic_judge_scores.py   # → p0_scored_deterministic.jsonl
python 04b_uncertainty_judge_scores.py     # → p0_scored_uncertainty.jsonl

# 4. Finalize dataset
python 05_finalize_dataset.py

# 5. Run CJE ablations (Phase 2)
cd ../phase2_cje_ablations
python run_ablations.py

# 5. Analyze results
# Compare CJE predictions to validation oracle labels
```