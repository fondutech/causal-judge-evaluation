# Example configuration with uncertainty-aware judge evaluation

defaults:
  - base  # Inherit common settings
  - _self_

# Paths configuration
paths:
  work_dir: "outputs/arena_test_uncertainty"

# Dataset configuration
dataset:
  name: "chatbot_arena"
  split: "val"
  sample_limit: 20  # Small sample for testing

# Logging policy configuration
logging_policy:
  provider: "fireworks"
  model_name: "accounts/fireworks/models/llama-v3p3-8b-instruct"
  temperature: 0.1
  max_new_tokens: 1024

# Target policies (can have multiple)
target_policies:
  - name: "higher_temp"
    provider: "fireworks"
    model_name: "accounts/fireworks/models/llama-v3p3-8b-instruct"
    temperature: 0.7
    max_new_tokens: 1024

# Judge configuration
# Note: To use uncertainty-aware judging, use the cje.uncertainty module directly
# This config uses the standard judge without uncertainty
judge:
  provider: "fireworks"
  model_name: "accounts/fireworks/models/llama4-scout-instruct-basic"
  template: "quick_judge"
  temperature: 0.0
  structured_output_schema: "JudgeScore"  # Standard schema

# Estimator configuration
estimator:
  name: "DRCPO"
  k: 5
  samples_per_policy: 2
  
  # Standard calibration settings
  calibrate_weights: true
  calibrate_outcome: true

# For uncertainty-aware evaluation, see examples/clean_uncertainty_api.py
# which demonstrates using the new cje.uncertainty module