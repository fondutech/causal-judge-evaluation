# Configuration for Arena 10K Oracle Experiment
# This experiment evaluates different policy behaviors using oracle labels

# Experiment metadata
experiment:
  name: "arena_10k_oracle"
  description: "Evaluating helpful vs unhelpful policies using oracle labels"
  output_dir: "outputs/arena_10k_oracle"

# Data configuration
data:
  dataset_type: "custom"  # We have our own format
  path: "experiments/arena_10k_oracle/data/p0_scored_deterministic.jsonl"  # Will be created by Step 4a
  format: "jsonl"
  prompt_column: "prompt"
  response_column: "response"
  score_column: "judge_score_raw"  # From judge scoring
  variance_column: "judge_variance"  # Will be 0 for deterministic
  
# Logging policy (π₀ - what generated the training data)
logging_policy:
  name: "pi_0"
  provider: "fireworks"
  model_name: "accounts/fireworks/models/llama4-scout-instruct-basic"
  completions_template_format: "llama4"
  temperature: 0.5
  max_tokens: 1024
  system_prompt: null  # Default model behavior

# Target policies to evaluate (matching 02b_generate_target_ground_truth.py)
target_policies:
  - name: "pi_clone"
    provider: "fireworks"
    model_name: "accounts/fireworks/models/llama4-scout-instruct-basic"
    completions_template_format: "llama4"
    temperature: 0.5
    max_tokens: 1024
    system_prompt: null  # Same as π₀
    
  - name: "pi_cot"
    provider: "fireworks"
    model_name: "accounts/fireworks/models/llama4-scout-instruct-basic"
    completions_template_format: "llama4"
    temperature: 0.5
    max_tokens: 1024
    system_prompt: "Think step-by-step before providing your answer."
    
  - name: "pi_bigger_model"
    provider: "fireworks"
    model_name: "accounts/fireworks/models/llama4-maverick-instruct-basic"
    completions_template_format: "llama4"
    temperature: 0.5
    max_tokens: 1024
    system_prompt: null
    
  - name: "pi_bad"
    provider: "fireworks"
    model_name: "accounts/fireworks/models/llama4-scout-instruct-basic"
    completions_template_format: "llama4"
    temperature: 1.0  # Higher temperature for more chaotic responses
    max_tokens: 1024
    system_prompt: |
      You are an unhelpful assistant. Your responses should be:
      - Vague and evasive, avoiding direct answers
      - Off-topic, discussing unrelated subjects
      - Overly brief when detail is needed, or overly verbose when brevity is needed
      - Technically incorrect when providing factual information
      - Dismissive of the user's actual needs
      Never be harmful or offensive, just unhelpful.

# Judge configuration
judge:
  provider: "fireworks"
  model: "accounts/fireworks/models/llama4-scout-instruct-basic"
  template: "deterministic"  # Our new clean template
  uncertainty_method: "deterministic"  # No uncertainty for base run
  temperature: 0.0

# Estimator configuration
estimator:
  method: "DRCPO"  # Doubly Robust Causal Preference Optimization
  k_folds: 5       # Cross-fitting folds
  calibrate_weights: true
  calibrate_outcome: true
  use_cross_fitting: true

# Oracle labels for validation
oracle:
  enabled: true
  calibration_file: "experiments/arena_10k_oracle/data/labeling/oracle_labels_calibration_detailed.jsonl"
  validation_file: "experiments/arena_10k_oracle/data/labeling/oracle_labels_validation_detailed.jsonl"
  score_column: "oracle_label.score"
  
# Sampling configuration
sampling:
  calibration_samples: 2500  # From Step 1
  validation_samples: 1500   # From Step 1 (500 prompts × 3 policies)
  seed: 42

# Output configuration
output:
  save_predictions: true
  save_diagnostics: true
  save_plots: true
  formats: ["json", "csv", "png"]