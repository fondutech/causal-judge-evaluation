# Example configuration file for use with cje.uncertainty module
# This is not a Hydra config - it's for reference when using the uncertainty API

# Uncertainty judge configuration
uncertainty_judge:
  provider: "fireworks"
  model_name: "accounts/fireworks/models/llama4-scout-instruct-basic"
  api_key: "${env:FIREWORKS_API_KEY}"
  temperature: 0.0
  template: "quick_judge"
  
  # Uncertainty-specific options
  include_uncertainty_prompt: true  # Add uncertainty instructions to prompt
  structured_output_method: "json_schema"  # Recommended for Fireworks

# Uncertainty estimator configuration  
uncertainty_estimator:
  k_folds: 5  # Cross-fitting folds
  use_variance_shrinkage: true
  shrinkage_method: "adaptive"  # Options: "optimal", "adaptive", "fixed"
  fixed_shrinkage_lambda: 1.0  # Only used if method="fixed"
  
  # Calibration options
  calibrate_variance: true  # Compute gamma calibration factor
  min_ess_ratio: 0.2  # Minimum effective sample size ratio
  
  # Diagnostics
  compute_diagnostics: true
  diagnostic_plots: false  # Set true to save diagnostic plots

# Example usage in Python:
# 
# from cje.uncertainty import UncertaintyAPIJudge, UncertaintyAwareDRCPO
# from cje.uncertainty.judge import UncertaintyJudgeConfig
# from cje.uncertainty.estimator import UncertaintyEstimatorConfig
#
# # Create judge
# judge_config = UncertaintyJudgeConfig(**config["uncertainty_judge"])
# judge = UncertaintyAPIJudge(judge_config)
#
# # Score samples
# samples = [{"context": "...", "response": "..."}, ...]
# judge_scores = judge.score_batch(samples)
#
# # Create estimator
# est_config = UncertaintyEstimatorConfig(**config["uncertainty_estimator"])
# estimator = UncertaintyAwareDRCPO(est_config)
#
# # Fit estimator
# result = estimator.fit(
#     X=None,
#     judge_scores=judge_scores,
#     oracle_rewards=oracle_rewards,
#     importance_weights=weights,
#     policy_names=["policy1", "policy2"]
# )