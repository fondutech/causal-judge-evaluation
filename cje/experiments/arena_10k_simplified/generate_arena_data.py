#!/usr/bin/env python3
"""
Production pipeline for preparing Arena experiment data.

This script runs the data generation pipeline for CJE experiments:
1. Extract prompts from ChatBot Arena dataset
2. Generate responses using different policies
3. Add judge scores (lightweight evaluation)
4. Add oracle labels (high-quality evaluation)
5. Compute log probabilities
6. Combine into a single dataset file

The prepared data can then be analyzed with analyze_dataset.py,
which handles calibration and CJE estimation.
"""

import argparse
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Optional

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from policy_config import POLICY_NAMES


def run_command(
    cmd: str, check: bool = True, skip_if_exists: Optional[Path] = None
) -> subprocess.CompletedProcess:
    """Run a shell command and return the result.

    Args:
        cmd: Command to run
        check: Whether to raise error on non-zero exit
        skip_if_exists: Skip command if this file exists
    """
    # Check if we should skip
    if skip_if_exists and skip_if_exists.exists():
        print(f"‚è≠Ô∏è  Skipping (output exists): {skip_if_exists}")
        return subprocess.CompletedProcess(args=cmd, returncode=0)

    # If command starts with "python ", prepend "poetry run "
    if cmd.strip().startswith("python "):
        cmd = "poetry run " + cmd

    print(f"\nüìç Running: {cmd}")
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

    if result.stdout.strip():
        # Show first 500 chars of output
        output = result.stdout[:500]
        if len(result.stdout) > 500:
            output += "..."
        print(f"Output: {output}")

    if check and result.returncode != 0:
        print(f"‚ùå Command failed with return code {result.returncode}")
        print(f"STDOUT: {result.stdout}")
        print(f"STDERR: {result.stderr}")
        raise RuntimeError(f"Command failed: {cmd}")

    return result


def create_prompts(
    output_dir: Path, n_samples: int, seed: int, skip_existing: bool
) -> None:
    """Extract prompts from ChatBot Arena dataset.

    Args:
        output_dir: Directory to save prompts
        n_samples: Number of prompts to extract
        seed: Random seed for reproducibility
        skip_existing: Whether to skip if prompts file exists
    """
    prompts_file = output_dir / "prompts.jsonl"

    if skip_existing and prompts_file.exists():
        print(f"‚è≠Ô∏è  Skipping prompt extraction (file exists): {prompts_file}")
        return

    print(f"Preparing {n_samples} prompts from ChatBot Arena dataset...")

    # Import the prepare function
    from pipeline_steps.prepare_arena_data import prepare_arena_prompts

    # Generate prompts
    prompts = prepare_arena_prompts(
        n_samples=n_samples,
        output_file=str(prompts_file),
        seed=seed,
    )

    if len(prompts) < n_samples:
        print(f"‚ö†Ô∏è  Warning: Only got {len(prompts)} prompts, expected {n_samples}")

    print(f"‚úÖ Extracted {len(prompts)} Arena prompts: {prompts_file}")


def main() -> None:
    """Run the production data preparation pipeline."""
    parser = argparse.ArgumentParser(
        description="Prepare Arena experiment data for CJE analysis"
    )
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Directory for output files (default: data)",
    )
    parser.add_argument(
        "--n-samples",
        type=int,
        default=1000,
        help="Number of samples to generate (default: 1000)",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=512,
        help="Maximum tokens per response (default: 256)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility (default: 42)",
    )
    parser.add_argument(
        "--skip-existing",
        "--resume",
        action="store_true",
        default=True,
        help="Skip steps where output files already exist (default: True)",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force overwrite existing files (disables resume/skip-existing)",
    )
    parser.add_argument(
        "--no-resume",
        dest="skip_existing",
        action="store_false",
        help="Disable resume/skip-existing behavior",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=20,
        help="Save progress every N samples for resilience (default: 20, set to 0 to disable)",
    )
    args = parser.parse_args()

    # If --force is set, disable skip_existing
    if args.force:
        args.skip_existing = False

    print("üöÄ Starting Arena experiment data preparation...")
    print(f"   Samples: {args.n_samples}")
    print(f"   Max tokens: {args.max_tokens}")
    print(f"   Batch size: {args.batch_size if args.batch_size > 0 else 'disabled'}")
    print(
        f"   Mode: {'Force overwrite' if args.force else 'Resume (skip existing)' if args.skip_existing else 'Overwrite'}"
    )
    print(f"   Output directory: {args.data_dir}")

    # Setup data directory
    data_dir = Path(args.data_dir)
    data_dir.mkdir(exist_ok=True)

    # Check for existing data and warn user
    existing_files = []
    important_files = [
        data_dir / "cje_dataset.jsonl",
        data_dir / "prompts.jsonl",
    ]

    # Check for response files
    for policy in POLICY_NAMES:
        important_files.append(data_dir / "responses" / f"{policy}_responses.jsonl")

    for file in important_files:
        if file.exists():
            existing_files.append(file)

    # Only warn if we're about to overwrite (not resuming and not forcing)
    if existing_files and not args.skip_existing and not args.force:
        print("\n‚ö†Ô∏è  WARNING: The following files already exist:")
        for file in existing_files:
            print(f"   - {file}")
        print("\nThis operation will OVERWRITE these files!")
        print("Options:")
        print("  1. Continue and overwrite (type 'yes')")
        print("  2. Resume from existing files (default behavior, or use --resume)")
        print("  3. Force overwrite without asking (rerun with --force)")
        print("  4. Cancel (any other input)")

        response = (
            input("\nDo you want to continue and OVERWRITE? (yes/no): ").strip().lower()
        )
        if response != "yes":
            print("‚ùå Operation cancelled. No files were modified.")
            sys.exit(0)
        print("‚ö†Ô∏è  Proceeding with overwrite...")

    # Create subdirectories
    (data_dir / "responses").mkdir(exist_ok=True)
    (data_dir / "logprobs").mkdir(exist_ok=True)

    # Step 1: Extract prompts from Arena dataset
    print("\n" + "=" * 60)
    print("Step 1: Extract prompts from ChatBot Arena")
    print("=" * 60)
    create_prompts(
        data_dir,
        n_samples=args.n_samples,
        seed=args.seed,
        skip_existing=args.skip_existing,
    )

    # Step 2: Generate responses
    print("\n" + "=" * 60)
    print("Step 2: Generate responses for each policy")
    print("=" * 60)

    # Check if all response files exist
    policies = POLICY_NAMES
    all_responses_exist = all(
        (data_dir / "responses" / f"{policy}_responses.jsonl").exists()
        for policy in policies
    )

    if args.skip_existing and all_responses_exist:
        print("‚è≠Ô∏è  Skipping response generation (all files exist)")
    else:
        cmd = (
            f"python pipeline_steps/generate_responses.py "
            f"--prompts {data_dir}/prompts.jsonl "
            f"--output-dir {data_dir}/responses "
            f"--max-responses {args.n_samples} "
            f"--max-tokens {args.max_tokens}"
        )
        if args.batch_size > 0:
            cmd += f" --batch-size {args.batch_size}"
        run_command(cmd)

    # Step 3: Add judge scores
    print("\n" + "=" * 60)
    print("Step 3: Add judge scores (lightweight evaluation)")
    print("=" * 60)

    for policy in policies:
        response_file = data_dir / "responses" / f"{policy}_responses.jsonl"

        # Check if judge scores already exist
        if args.skip_existing and response_file.exists():
            with open(response_file) as f:
                first_line = json.loads(f.readline())
                if "judge_score" in first_line.get("metadata", {}):
                    print(f"‚è≠Ô∏è  Skipping judge scoring for {policy} (scores exist)")
                    continue

        run_command(
            f"python pipeline_steps/add_judge_scores.py " f"--input {response_file}"
        )

    # Step 4: Add oracle labels
    print("\n" + "=" * 60)
    print("Step 4: Add oracle labels (high-quality evaluation)")
    print("=" * 60)

    for policy in policies:
        response_file = data_dir / "responses" / f"{policy}_responses.jsonl"

        # Check if oracle labels already exist
        if args.skip_existing and response_file.exists():
            with open(response_file) as f:
                first_line = json.loads(f.readline())
                if "oracle_label" in first_line.get("metadata", {}):
                    print(f"‚è≠Ô∏è  Skipping oracle labeling for {policy} (labels exist)")
                    continue

        run_command(
            f"python pipeline_steps/add_oracle_labels.py " f"--input {response_file}"
        )

    # Step 5: Compute log probabilities
    print("\n" + "=" * 60)
    print("Step 5: Compute log probabilities")
    print("=" * 60)

    # Check if all logprob files exist
    all_logprobs_exist = all(
        (data_dir / "logprobs" / f"{policy}_logprobs.jsonl").exists()
        for policy in policies
    )

    if args.skip_existing and all_logprobs_exist:
        print("‚è≠Ô∏è  Skipping logprob computation (all files exist)")
    else:
        cmd = (
            f"python pipeline_steps/compute_logprobs.py "
            f"--responses-dir {data_dir}/responses "
            f"--output-dir {data_dir}/logprobs"
        )
        if args.batch_size > 0:
            cmd += f" --batch-size {args.batch_size}"
        run_command(cmd)

    # Step 6: Prepare CJE dataset
    print("\n" + "=" * 60)
    print("Step 6: Combine data into CJE dataset")
    print("=" * 60)

    cje_dataset_file = data_dir / "cje_dataset.jsonl"

    run_command(
        f"python pipeline_steps/prepare_cje_data.py "
        f"--responses-dir {data_dir}/responses "
        f"--logprobs-dir {data_dir}/logprobs "
        f"--output {cje_dataset_file}",
        skip_if_exists=cje_dataset_file if args.skip_existing else None,
    )

    print("\n‚úÖ Data generation completed successfully!")
    print(f"üìÅ Output directory: {data_dir}")
    print(f"üìä Dataset file: {cje_dataset_file}")
    print("\nNext step:")
    print(f"  poetry run python analyze_dataset.py --data {cje_dataset_file}")


if __name__ == "__main__":
    # Check for API keys
    if not os.getenv("FIREWORKS_API_KEY"):
        print("‚ùå Error: FIREWORKS_API_KEY environment variable not set")
        print("Please run: source /path/to/set_secrets.sh")
        sys.exit(1)

    if not os.getenv("OPENAI_API_KEY"):
        print(
            "‚ö†Ô∏è  Warning: OPENAI_API_KEY not set (required for judge/oracle evaluation)"
        )
        print("Continuing anyway - evaluation steps will fail")

    main()
