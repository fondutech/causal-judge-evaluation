"""
Target policy stage - Handles target policy log probability computation.

This stage implements teacher forcing: it computes log P(response | context, target_policy)
for responses that were generated by the logging policy. This is essential for
importance weighting in off-policy evaluation.
"""

import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
import numpy as np

from rich.console import Console
from rich.table import Table

from ...loggers.multi_target_sampler import make_multi_sampler
from ...cache import compute_target_logprobs_hash, chunk_exists, load_chunk, save_chunk
from ...validation import validate_target_policy_computation
from ...utils.error_handling import safe_call, PolicyError

logger = logging.getLogger(__name__)


class TargetPolicyStage:
    """Handles target policy log probability computation with caching."""

    def __init__(self, work_dir: Path, console: Optional[Console] = None):
        self.work_dir = work_dir
        self.console = console or Console()

    def run(
        self,
        rows: List[Dict[str, Any]],
        target_policies_config: List[Dict[str, Any]],
        contexts_hash: str,
    ) -> List[Dict[str, Any]]:
        """
        Compute target policy log probabilities for all contexts.

        Args:
            rows: Input rows with contexts
            target_policies_config: List of target policy configurations
            contexts_hash: Hash of contexts for cache dependency

        Returns:
            Rows with target policy log probabilities added
        """
        if not target_policies_config:
            logger.warning("No target policies configured")
            self.console.print("[yellow]âš ï¸  No target policies configured[/yellow]")
            return rows

        logger.info(
            f"Computing log probabilities for {len(target_policies_config)} target policies"
        )
        self.console.print(
            f"[bold blue]ðŸŽ¯ Computing Target Policy Log Probabilities "
            f"({len(target_policies_config)} policies)[/bold blue]"
        )

        # Compute hash for caching
        logprobs_hash = compute_target_logprobs_hash(
            contexts_hash, target_policies_config
        )

        # Check cache
        if chunk_exists(self.work_dir, "target_logprobs", logprobs_hash):
            logger.info(f"Using cached target log probabilities: {logprobs_hash}")
            self.console.print(
                f"[green]ðŸ“¦ Using cached target log probabilities: {logprobs_hash}[/green]"
            )
            cached_rows = load_chunk(self.work_dir, "target_logprobs", logprobs_hash)

            # Validate cached data
            try:
                validate_target_policy_computation(cached_rows)
                return cached_rows
            except Exception as e:
                logger.warning(f"Cached data validation failed: {e}")
                logger.info("Recomputing target log probabilities")

        # Create multi-target sampler
        sampler = self._create_multi_sampler(target_policies_config)

        # Compute log probabilities
        rows_with_logprobs = self._compute_logprobs(
            rows, sampler, len(target_policies_config)
        )

        # Validate results
        validate_target_policy_computation(rows_with_logprobs)

        # Save to cache
        self._save_to_cache(rows_with_logprobs, logprobs_hash, target_policies_config)

        # Log statistics
        self._log_policy_statistics(rows_with_logprobs, target_policies_config)

        return rows_with_logprobs

    def _create_multi_sampler(self, policies_config: List[Dict[str, Any]]) -> Any:
        """Create multi-target sampler from configuration."""
        # Convert config dicts to proper format
        policy_configs = []

        for i, policy in enumerate(policies_config):
            policy_dict = {
                "provider": policy.get("provider", "hf"),
                "model_name": policy["model_name"],
            }

            # Add optional parameters
            for param in ["max_new_tokens", "temperature", "top_p", "batch_size"]:
                if param in policy:
                    policy_dict[param] = policy[param]

            policy_configs.append(policy_dict)

        # Create sampler
        return make_multi_sampler(policy_configs)

    def _compute_logprobs(
        self, rows: List[Dict[str, Any]], sampler: Any, num_policies: int
    ) -> List[Dict[str, Any]]:
        """Compute log probabilities for all contexts using teacher forcing.

        For each row containing (context, response) from the logging policy,
        compute log P(response | context, target_policy_k) for all k target policies.
        This is the key step for importance weighting.
        """
        # Validate that responses exist
        for i, row in enumerate(rows[:5]):  # Check first few rows
            if "response" not in row:
                raise ValueError(
                    f"Row {i} missing 'response' field. "
                    f"Ensure logging policy stage has run. "
                    f"Available fields: {list(row.keys())}"
                )

        contexts = [row["context"] for row in rows]
        responses = [row["response"] for row in rows]  # Extract logged responses

        logger.info(f"Computing log probabilities for {len(contexts)} contexts")

        with self.console.status(
            f"[bold blue]Computing log probabilities for {len(contexts)} contexts..."
        ) as status:
            try:
                # Compute log probabilities using teacher forcing
                # This computes log P(response | context, target_policy) for each target
                logprobs_matrix = sampler.logp_matrix(contexts, responses)

                # logprobs_matrix is shape (n_contexts, n_policies)
                # Convert to list format expected by the rest of the pipeline
                all_logprobs = logprobs_matrix.tolist()
                # No new responses generated - we're scoring existing ones
                all_responses = [[None] * num_policies] * len(contexts)

            except Exception as e:
                logger.error(f"Failed to compute log probabilities: {e}")
                raise PolicyError(f"Target policy computation failed: {e}")

        # Add log probabilities to rows
        result_rows = rows.copy()

        for i, row in enumerate(result_rows):
            # Get log probs for this context
            context_logprobs = all_logprobs[i]
            context_responses = (
                all_responses[i] if i < len(all_responses) else [None] * num_policies
            )

            # Validate shape
            if len(context_logprobs) != num_policies:
                raise ValueError(
                    f"Expected {num_policies} log probs for context {i}, "
                    f"got {len(context_logprobs)}"
                )

            # Store log probabilities
            row["logp_target_all"] = [float(lp) for lp in context_logprobs]

            # Store sampled responses if available
            if any(r is not None for r in context_responses):
                row["responses_target_all"] = context_responses

            # Add metadata
            row["num_target_policies"] = num_policies

        return result_rows

    def _save_to_cache(
        self,
        rows: List[Dict[str, Any]],
        logprobs_hash: str,
        policies_config: List[Dict[str, Any]],
    ) -> None:
        """Save target log probabilities to cache."""
        metadata = {
            "num_policies": len(policies_config),
            "sample_count": len(rows),
            "policy_names": [p["model_name"] for p in policies_config],
        }

        save_chunk(
            self.work_dir, "target_logprobs", logprobs_hash, rows, metadata=metadata
        )

        logger.info(f"Target log probabilities cached: {logprobs_hash}")
        self.console.print(
            f"[green]âœ… Target log probabilities cached: {logprobs_hash}[/green]"
        )

    def _log_policy_statistics(
        self, rows: List[Dict[str, Any]], policies_config: List[Dict[str, Any]]
    ) -> None:
        """Log statistics about target policies."""
        # Collect log probabilities by policy
        policy_logprobs: List[List[float]] = [[] for _ in policies_config]

        for row in rows:
            if "logp_target_all" in row:
                for i, lp in enumerate(row["logp_target_all"]):
                    policy_logprobs[i].append(lp)

        # Create statistics table
        table = Table(title="Target Policy Statistics")
        table.add_column("Policy", style="cyan")
        table.add_column("Model", style="green")
        table.add_column("Mean LogP", style="yellow")
        table.add_column("Std LogP", style="yellow")

        for i, (policy_config, logprobs) in enumerate(
            zip(policies_config, policy_logprobs)
        ):
            if logprobs:
                mean_lp = np.mean(logprobs)
                std_lp = np.std(logprobs)

                table.add_row(
                    f"Ï€_{i}",
                    policy_config["model_name"].split("/")[-1][:30],
                    f"{mean_lp:.3f}",
                    f"{std_lp:.3f}",
                )

        self.console.print(table)

        # Log summary
        logger.info(
            f"Target policy computation complete: "
            f"{len(policies_config)} policies, {len(rows)} contexts"
        )
