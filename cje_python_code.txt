CJE PYTHON CODE EXPORT
======================

This file contains all Python (.py) files from the CJE core library directory,
excluding test files and __pycache__ directories.

Generated on: Wed Aug 27 14:06:35 PDT 2025

TABLE OF CONTENTS
=================

The following files are included:

cje/
  ├── __init__.py
  ├── __main__.py
  ├── advanced.py

cje/calibration/
  ├── __init__.py
  ├── dataset.py
  ├── flexible_calibrator.py
  ├── iic.py
  ├── isotonic.py
  ├── judge.py
  ├── oracle_slice.py
  ├── simcal.py

cje/cfbits/
  ├── __init__.py
  ├── core.py
  ├── identification.py
  ├── overlap.py
  ├── playbooks.py
  ├── sampling.py

cje/data/
  ├── __init__.py
  ├── factory.py
  ├── folds.py
  ├── fresh_draw_utils.py
  ├── fresh_draws.py
  ├── loaders.py
  ├── models.py
  ├── precomputed_sampler.py
  ├── reward_utils.py
  ├── validation.py

cje/diagnostics/
  ├── __init__.py
  ├── display.py
  ├── dr.py
  ├── models.py
  ├── overlap.py
  ├── robust_inference.py
  ├── stability.py
  ├── weights.py

cje/estimators/
  ├── __init__.py
  ├── base_estimator.py
  ├── calibrated_ips.py
  ├── dr_base.py
  ├── mrdr.py
  ├── outcome_models.py
  ├── stacking.py
  ├── tmle.py

cje/experiments/arena_10k_simplified/ablations/
  ├── analyze_results.py

cje/experiments/arena_10k_simplified/ablations/core/
  ├── __init__.py
  ├── base.py
  ├── diagnostics.py
  ├── schemas.py

cje/experiments/arena_10k_simplified/ablations/
  ├── estimator_comparison.py
  ├── fresh_draws.py
  ├── iic_effect_minimal.py
  ├── interaction.py
  ├── oracle_coverage.py
  ├── oracle_coverage_simple.py
  ├── run_all_ablations.py
  ├── sample_size.py
  ├── simcal_rho.py

cje/experiments/arena_10k_simplified/analysis/
  ├── __init__.py
  ├── calibration.py
  ├── diagnostics.py
  ├── estimation.py
  ├── export.py
  ├── loading.py

cje/experiments/arena_10k_simplified/analysis/multiple_passes/
  ├── analyze_variance_decomposition.py

cje/experiments/arena_10k_simplified/analysis/
  ├── results.py
  ├── visualization.py

cje/experiments/arena_10k_simplified/
  ├── analyze_dataset.py

cje/experiments/arena_10k_simplified/data_generation/
  ├── __init__.py
  ├── add_scores_with_resume.py
  ├── compute_logprobs.py
  ├── generate_additional_passes.py
  ├── generate_responses.py
  ├── prepare_arena_data.py
  ├── prepare_cje_data.py

cje/experiments/arena_10k_simplified/
  ├── evaluation_utils.py
  ├── experiment_config.py
  ├── generate_arena_data.py

cje/interface/
  ├── __init__.py
  ├── analysis.py
  ├── cli.py

cje/
  ├── research.py

cje/teacher_forcing/
  ├── __init__.py

cje/teacher_forcing/api/
  ├── __init__.py
  ├── fireworks.py

cje/teacher_forcing/
  ├── chat.py

cje/teacher_forcing/templates/
  ├── __init__.py
  ├── base.py
  ├── fireworks.py
  ├── huggingface.py
  ├── llama.py

cje/utils/
  ├── __init__.py
  ├── export.py
  ├── extreme_weights_analysis.py

cje/visualization/
  ├── __init__.py
  ├── calibration.py
  ├── dr_dashboards.py
  ├── estimates.py
  ├── weight_dashboards.py

Total: 98 Python files from the cje/ directory
Files are presented in alphabetical order within each directory.

========================================


=== ./cje/__init__.py ===

"""CJE: Causal Judge Evaluation - Unbiased LLM Policy Evaluation.

Simple API for off-policy evaluation with judge scores.

Example:
    from cje import analyze_dataset

    results = analyze_dataset(
        "data.jsonl",
        estimator="calibrated-ips",
        oracle_coverage=0.5
    )
    print(results.summary())
"""

__version__ = "0.2.0"

# Simple API - what 90% of users need
from .interface import analyze_dataset

# Core data structures
from .data import Dataset, Sample, EstimationResult

# Simple data loading
from .data import load_dataset_from_jsonl

__all__ = [
    # Simple API
    "analyze_dataset",
    # Core data structures
    "Dataset",
    "Sample",
    "EstimationResult",
    # Data loading
    "load_dataset_from_jsonl",
]


=== ./cje/__main__.py ===

"""Entry point for running CJE as a module."""

from .interface.cli import main

if __name__ == "__main__":
    exit(main())


=== ./cje/advanced.py ===

"""Advanced CJE API for power users.

This module exposes additional functionality for users who need more control.
Import from here when you need to:
- Use specific estimators directly
- Customize calibration behavior
- Access diagnostic tools
- Build custom pipelines

Example:
    from cje.advanced import (
        PrecomputedSampler,
        CalibratedIPS,
        calibrate_dataset,
        IPSDiagnostics
    )

    # Custom pipeline with manual control
    dataset = load_dataset_from_jsonl("data.jsonl")
    calibrated, cal_result = calibrate_dataset(dataset, oracle_coverage=0.1)
    sampler = PrecomputedSampler(calibrated)
    estimator = CalibratedIPS(sampler, var_cap=10.0)
    results = estimator.fit_and_estimate()
"""

# Estimators
from .estimators import (
    BaseCJEEstimator,
    CalibratedIPS,
)

# Data components
from .data import (
    PrecomputedSampler,
    Dataset,
    Sample,
    EstimationResult,
    DatasetFactory,
    default_factory,
)

# Calibration
from .calibration import (
    calibrate_dataset,
    calibrate_judge_scores,
    JudgeCalibrator,
    CalibrationResult,
)

# Diagnostics
from .diagnostics import (
    IPSDiagnostics,
    DRDiagnostics,
    Status,
)

# Utilities
from .utils import (
    create_weight_summary_table,
    analyze_extreme_weights,
)
from .utils.export import (
    export_results_json,
    export_results_csv,
)

# DR estimators (if available)
try:
    from .estimators.dr_base import DRCPOEstimator
    from .estimators.mrdr import MRDREstimator
    from .estimators.tmle import TMLEEstimator
    from .data.fresh_draws import (
        FreshDrawDataset,
        load_fresh_draws_from_jsonl,
        load_fresh_draws_auto,
    )

    _dr_available = True
except ImportError:
    _dr_available = False

# Visualization (if available)
try:
    from .visualization import (
        plot_weight_dashboard_summary,
        plot_calibration_comparison,
        plot_policy_estimates,
    )

    _viz_available = True
except ImportError:
    _viz_available = False

__all__ = [
    # Estimators
    "BaseCJEEstimator",
    "CalibratedIPS",
    # Data
    "PrecomputedSampler",
    "Dataset",
    "Sample",
    "EstimationResult",
    "DatasetFactory",
    "default_factory",
    # Calibration
    "calibrate_dataset",
    "calibrate_judge_scores",
    "JudgeCalibrator",
    "CalibrationResult",
    # Diagnostics
    "IPSDiagnostics",
    "DRDiagnostics",
    "Status",
    # Utilities
    "create_weight_summary_table",
    "analyze_extreme_weights",
    "export_results_json",
    "export_results_csv",
]

if _dr_available:
    __all__.extend(
        [
            "DRCPOEstimator",
            "MRDREstimator",
            "TMLEEstimator",
            "FreshDrawDataset",
            "load_fresh_draws_from_jsonl",
            "load_fresh_draws_auto",
        ]
    )

if _viz_available:
    __all__.extend(
        [
            "plot_weight_dashboard_summary",
            "plot_calibration_comparison",
            "plot_policy_estimates",
        ]
    )


=== ./cje/calibration/__init__.py ===

"""Calibration utilities for CJE.

This module contains all calibration functionality:
- Optimized isotonic regression for weight calibration
- Judge score calibration to match oracle labels
- Dataset calibration workflows
- Oracle slice uncertainty augmentation
"""

from .isotonic import (
    calibrate_to_target_mean,
)
from .simcal import (
    SIMCalibrator,
    SimcalConfig,
)
from .judge import (
    JudgeCalibrator,
    calibrate_judge_scores,
    CalibrationResult,
)
from .dataset import (
    calibrate_dataset,
    calibrate_from_raw_data,
)
from .oracle_slice import (
    OracleSliceAugmentation,
    OracleSliceConfig,
)
from .iic import (
    IsotonicInfluenceControl,
    IICConfig,
)

__all__ = [
    # Isotonic regression utilities
    "calibrate_to_target_mean",
    # SIMCal
    "SIMCalibrator",
    "SimcalConfig",
    # Judge calibration
    "JudgeCalibrator",
    "calibrate_judge_scores",
    "CalibrationResult",
    # Dataset calibration
    "calibrate_dataset",
    "calibrate_from_raw_data",
    # Oracle slice augmentation
    "OracleSliceAugmentation",
    "OracleSliceConfig",
    # Isotonic Influence Control (IIC)
    "IsotonicInfluenceControl",
    "IICConfig",
]


=== ./cje/calibration/dataset.py ===

"""Dataset calibration utilities.

This module provides functions to calibrate datasets with judge scores
to match oracle labels, creating calibrated rewards for CJE analysis.
"""

from typing import Dict, List, Any, Optional, Tuple, Literal
from copy import deepcopy
import numpy as np
from ..data.models import Dataset, Sample
from .judge import JudgeCalibrator, CalibrationResult


def calibrate_dataset(
    dataset: Dataset,
    judge_field: str = "judge_score",
    oracle_field: str = "oracle_label",
    enable_cross_fit: bool = False,
    n_folds: int = 5,
    calibration_mode: Optional[str] = None,
    random_seed: int = 42,
) -> Tuple[Dataset, CalibrationResult]:
    """Calibrate judge scores in a dataset to match oracle labels.

    This function extracts judge scores and oracle labels from the dataset,
    calibrates the judge scores to match the oracle distribution, and returns
    a new dataset with calibrated rewards. By default, uses auto mode to
    automatically select between monotone and flexible calibration.

    Args:
        dataset: Dataset containing judge scores and oracle labels
        judge_field: Field name in metadata containing judge scores
        oracle_field: Field name in metadata containing oracle labels
        enable_cross_fit: If True, also fits cross-fitted models for DR
        n_folds: Number of CV folds (only used if enable_cross_fit=True)
        calibration_mode: Calibration mode ('auto', 'monotone', 'two_stage').
                         If None, defaults to 'auto' for cross-fit, 'monotone' otherwise.

    Returns:
        Tuple of (calibrated_dataset, calibration_result)

    Example:
        >>> # Load dataset with judge scores
        >>> dataset = load_dataset_from_jsonl("data.jsonl", reward_field="judge_score")
        >>>
        >>> # Calibrate judge scores to oracle labels
        >>> calibrated_dataset, stats = calibrate_dataset(
        ...     dataset,
        ...     judge_field="judge_score",
        ...     oracle_field="oracle_label"
        ... )
    """
    # Extract judge scores, oracle labels, and prompt_ids
    judge_scores = []
    oracle_labels = []
    oracle_mask = []
    prompt_ids = []

    # Forbid judge_field="reward" to avoid confusion
    if judge_field == "reward":
        raise ValueError(
            "judge_field='reward' is not allowed to avoid confusion between "
            "raw and calibrated values. Use a different field name in metadata."
        )

    for sample in dataset.samples:
        # Look for judge score in metadata
        if judge_field not in sample.metadata:
            raise ValueError(
                f"Judge field '{judge_field}' not found in sample metadata"
            )

        judge_score = sample.metadata[judge_field]
        judge_scores.append(float(judge_score))
        prompt_ids.append(sample.prompt_id)

        # Look for oracle label
        if (
            oracle_field in sample.metadata
            and sample.metadata[oracle_field] is not None
        ):
            oracle_labels.append(float(sample.metadata[oracle_field]))
            oracle_mask.append(True)
        else:
            oracle_mask.append(False)

    # Convert to arrays
    judge_scores_array = np.array(judge_scores)
    oracle_labels_array = np.array(oracle_labels) if oracle_labels else None
    oracle_mask_array = np.array(oracle_mask)

    if not np.any(oracle_mask_array):
        raise ValueError(f"No oracle labels found in field '{oracle_field}'")

    # Determine calibration mode
    if calibration_mode is None:
        # Default to auto for cross-fit (better for DR), monotone otherwise
        calibration_mode = "auto" if enable_cross_fit else "monotone"

    # Calibrate judge scores
    calibrator = JudgeCalibrator(
        calibration_mode=calibration_mode, random_seed=random_seed
    )
    if enable_cross_fit:
        # Use cross-fitted calibration for DR support
        # Pass prompt_ids to enable unified fold system
        result = calibrator.fit_cv(
            judge_scores_array,
            oracle_labels_array,
            oracle_mask_array,
            n_folds,
            prompt_ids=prompt_ids,
        )
    else:
        # Use standard calibration (backward compatible)
        result = calibrator.fit_transform(
            judge_scores_array, oracle_labels_array, oracle_mask_array
        )

    # Create new samples with calibrated rewards
    calibrated_samples = []
    oracle_idx = 0
    for i, sample in enumerate(dataset.samples):
        # Create new sample with calibrated reward
        new_metadata = sample.metadata.copy()
        new_metadata[judge_field] = judge_scores[i]  # Preserve original
        if oracle_mask[i]:
            new_metadata[oracle_field] = oracle_labels[oracle_idx]
            oracle_idx += 1

        # Note: We no longer store cv_fold in metadata
        # Folds are computed on-demand from prompt_id using the unified system

        calibrated_sample = Sample(
            prompt_id=sample.prompt_id,
            prompt=sample.prompt,
            response=sample.response,
            reward=float(result.calibrated_scores[i]),
            base_policy_logprob=sample.base_policy_logprob,
            target_policy_logprobs=sample.target_policy_logprobs,
            metadata=new_metadata,
        )
        calibrated_samples.append(calibrated_sample)

    # Create new dataset with calibration info in metadata
    dataset_metadata = dataset.metadata.copy()

    # Add calibration summary for downstream diagnostics
    dataset_metadata["calibration_info"] = {
        "rmse": result.calibration_rmse,
        "coverage": result.coverage_at_01,
        "n_oracle": result.n_oracle,
        "n_total": len(judge_scores),
        "method": "cross_fitted_isotonic" if enable_cross_fit else "isotonic",
        "n_folds": n_folds if enable_cross_fit else None,
        "oof_rmse": result.oof_rmse if enable_cross_fit else None,
        "oof_coverage": result.oof_coverage_at_01 if enable_cross_fit else None,
        "calibration_mode": calibration_mode,
    }

    # Store fold configuration for reproducibility
    dataset_metadata["n_folds"] = n_folds
    dataset_metadata["fold_seed"] = random_seed

    # Store selected calibration mode if auto was used
    if (
        hasattr(calibrator, "_flexible_calibrator")
        and calibrator._flexible_calibrator is not None
    ):
        dataset_metadata["calibration_info"][
            "selected_mode"
        ] = calibrator._flexible_calibrator.selected_mode
    elif calibration_mode == "auto" and hasattr(calibrator, "selected_mode"):
        dataset_metadata["calibration_info"]["selected_mode"] = calibrator.selected_mode

    calibrated_dataset = Dataset(
        samples=calibrated_samples,
        target_policies=dataset.target_policies,
        metadata=dataset_metadata,
    )

    return calibrated_dataset, result


def calibrate_from_raw_data(
    data: List[Dict[str, Any]],
    judge_field: str = "judge_score",
    oracle_field: str = "oracle_label",
    reward_field: str = "reward",
    calibration_mode: Optional[Literal["auto", "monotone", "two_stage"]] = "monotone",
    random_seed: int = 42,
) -> Tuple[List[Dict[str, Any]], CalibrationResult]:
    """Calibrate judge scores in raw data to create calibrated rewards.

    This is a lower-level function that works with raw dictionaries
    instead of Dataset objects.

    Args:
        data: List of dictionaries containing judge scores and oracle labels
        judge_field: Field name containing judge scores
        oracle_field: Field name containing oracle labels
        reward_field: Field name to store calibrated rewards
        calibration_mode: Calibration mode ('auto', 'monotone', 'two_stage').
                         Defaults to 'monotone' for backward compatibility.

    Returns:
        Tuple of (calibrated_data, calibration_result)
    """
    # Extract judge scores and oracle labels
    judge_scores = []
    oracle_labels = []
    oracle_mask = []

    for record in data:
        # Extract judge score
        judge_score = record.get(judge_field)
        if judge_score is None:
            raise ValueError(f"Judge field '{judge_field}' not found in record")

        if isinstance(judge_score, dict):
            judge_score = judge_score.get("mean", judge_score.get("value"))
        judge_scores.append(float(judge_score))

        # Check for oracle label
        oracle_label = record.get(oracle_field)
        if oracle_label is not None:
            oracle_labels.append(float(oracle_label))
            oracle_mask.append(True)
        else:
            oracle_mask.append(False)

    # Convert to arrays
    judge_scores_array = np.array(judge_scores)
    oracle_labels_array = np.array(oracle_labels) if oracle_labels else None
    oracle_mask_array = np.array(oracle_mask)

    if not np.any(oracle_mask_array):
        raise ValueError(f"No oracle labels found in field '{oracle_field}'")

    # Calibrate judge scores
    calibrator = JudgeCalibrator(
        calibration_mode=calibration_mode, random_seed=random_seed
    )
    result = calibrator.fit_transform(
        judge_scores_array, oracle_labels_array, oracle_mask_array
    )

    # Add calibrated rewards to data
    calibrated_data = []
    for i, record in enumerate(data):
        record_copy = record.copy()
        record_copy[reward_field] = float(result.calibrated_scores[i])

        # Deep copy metadata to avoid mutating caller's nested dict
        metadata = deepcopy(record_copy.get("metadata", {}))
        metadata[judge_field] = judge_scores[i]
        if oracle_mask[i]:
            # Find the index of this oracle label
            oracle_idx = np.sum(oracle_mask_array[: i + 1]) - 1
            metadata[oracle_field] = (
                float(oracle_labels_array[oracle_idx])
                if oracle_labels_array is not None
                else None
            )
        record_copy["metadata"] = metadata

        calibrated_data.append(record_copy)

    return calibrated_data, result


=== ./cje/calibration/flexible_calibrator.py ===

"""Flexible calibration modes for non-monotone relationships.

This module extends judge calibration to handle non-monotone relationships
through flexible shape fitting while maintaining cross-fitting support.
"""

import numpy as np
from typing import Optional, Tuple, Dict, Any, Literal, Callable
from sklearn.isotonic import IsotonicRegression
from sklearn.preprocessing import SplineTransformer
from sklearn.linear_model import RidgeCV
from sklearn.pipeline import make_pipeline
from scipy import stats
import logging

logger = logging.getLogger(__name__)


def _fit_ecdf(x: np.ndarray) -> Callable[[np.ndarray], np.ndarray]:
    """Fit empirical CDF for consistent ranking.

    Args:
        x: Training data to build ECDF from

    Returns:
        Function that maps values to their empirical CDF ranks in (0,1)
    """
    xs = np.sort(x)
    n = xs.size

    def F(z: np.ndarray) -> np.ndarray:
        # Right-continuous empirical CDF, mapped to mid-ranks in (0,1)
        idx = np.searchsorted(xs, z, side="right")
        return (idx - 0.5) / n

    return F


class FlexibleCalibrator:
    """Flexible calibration supporting monotone and non-monotone relationships.

    Modes:
    - 'monotone': Standard isotonic regression (current default)
    - 'two_stage': Learn smooth g(S) then isotonic on g(S)
    - 'auto': Automatically select based on cross-validation
    """

    def __init__(
        self,
        mode: Literal["monotone", "two_stage", "auto"] = "monotone",
        n_splines: int = 8,
        random_seed: int = 42,
    ):
        """Initialize flexible calibrator.

        Args:
            mode: Calibration mode
            n_splines: Number of splines for two_stage mode
            random_seed: Random seed for reproducibility
        """
        self.mode = mode
        self.n_splines = n_splines
        self.random_seed = random_seed
        self.selected_mode: Optional[Literal["monotone", "two_stage"]] = None  # For auto mode

        # Storage for fitted models
        self._monotone_models: Dict[int, Any] = {}
        self._g_models: Dict[int, Any] = {}
        self._iso_models: Dict[int, Any] = {}
        self._ecdf_models: Dict[int, Callable] = {}  # Per-fold ECDFs

        # Full models for inference (no folds)
        self._full_monotone_model: Optional[Any] = None
        self._full_g_model: Optional[Any] = None
        self._full_iso_model: Optional[Any] = None
        self._full_ecdf: Optional[Callable] = None

    def fit(
        self, S: np.ndarray, Y: np.ndarray, folds: np.ndarray
    ) -> "FlexibleCalibrator":
        """Fit the calibrator with cross-fitting.

        Args:
            S: Judge scores
            Y: Oracle labels
            folds: Fold assignments for cross-fitting

        Returns:
            Self for chaining
        """
        unique_folds = np.unique(folds)
        n_samples = len(S)

        logger.debug(
            f"FlexibleCalibrator.fit: {n_samples} samples, {len(unique_folds)} folds, mode={self.mode}"
        )

        if self.mode == "auto":
            logger.debug("Auto mode: evaluating monotone fit first")
            # Always fit monotone (it's fast)
            self._fit_monotone(S, Y, folds)

            # Quick check: if monotone fit is very good, skip two-stage
            pred_mono = self._predict_monotone(S, folds)
            rmse_mono = np.sqrt(np.mean((Y - pred_mono) ** 2))

            # Check for clear non-monotonicity via regional performance
            sort_idx = np.argsort(S)
            n_third = len(S) // 3
            low_mask = sort_idx[:n_third]
            mid_mask = sort_idx[n_third : 2 * n_third]
            high_mask = sort_idx[2 * n_third :]

            rmse_low = np.sqrt(np.mean((Y[low_mask] - pred_mono[low_mask]) ** 2))
            rmse_mid = np.sqrt(np.mean((Y[mid_mask] - pred_mono[mid_mask]) ** 2))
            rmse_high = np.sqrt(np.mean((Y[high_mask] - pred_mono[high_mask]) ** 2))

            # Only fit two-stage if there's evidence of non-monotonicity
            # (significant regional performance differences)
            max_regional_diff = max(rmse_low, rmse_mid, rmse_high) - min(
                rmse_low, rmse_mid, rmse_high
            )
            if max_regional_diff > 0.05 * rmse_mono:  # 5% threshold
                logger.debug(
                    f"Regional RMSE differences detected ({max_regional_diff:.3f}), fitting two-stage"
                )
                self._fit_two_stage(S, Y, folds)
                self.selected_mode = self._select_best_mode(S, Y, folds)
            else:
                logger.debug(
                    f"Monotone fit sufficient (regional diff {max_regional_diff:.3f} < threshold)"
                )
                self.selected_mode = "monotone"
        elif self.mode == "monotone":
            logger.debug("Fitting monotone calibration only")
            self._fit_monotone(S, Y, folds)
            self.selected_mode = "monotone"
        elif self.mode == "two_stage":
            logger.debug("Fitting two-stage calibration only")
            self._fit_two_stage(S, Y, folds)
            self.selected_mode = "two_stage"
        else:
            raise ValueError(f"Unknown mode: {self.mode}")

        # Also fit full models for inference (no folds)
        logger.debug("Fitting full models for inference")
        self._fit_full_models(S, Y)

        return self

    def _fit_monotone(self, S: np.ndarray, Y: np.ndarray, folds: np.ndarray) -> None:
        """Fit standard monotone isotonic regression."""
        for k in np.unique(folds):
            train_mask = folds != k
            iso = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds="clip")
            iso.fit(S[train_mask], Y[train_mask])
            self._monotone_models[k] = iso

    def _fit_two_stage(self, S: np.ndarray, Y: np.ndarray, folds: np.ndarray) -> None:
        """Fit two-stage calibrator: g(S) -> isotonic."""
        unique_folds = np.unique(folds)

        # Step 1: Fit smooth g(S) and ECDF for each fold
        for k in unique_folds:
            train_mask = folds != k
            S_train = S[train_mask]
            Y_train = Y[train_mask]

            # Skip if too few training samples
            if len(S_train) < 20:
                # Fallback to monotone for small folds
                iso = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds="clip")
                iso.fit(S_train, Y_train)
                self._g_models[k] = None
                self._iso_models[k] = iso
                self._ecdf_models[k] = _fit_ecdf(
                    S_train
                )  # Still fit ECDF for consistency
                continue

            # Fit spline + ridge for smooth transformation
            n_knots = min(max(5, self.n_splines), len(S_train) // 4)  # Minimum 5 knots
            spline = SplineTransformer(n_knots=n_knots, degree=3, include_bias=False)
            ridge = RidgeCV(alphas=np.logspace(-3, 3, 13), store_cv_values=False)
            g_model = make_pipeline(spline, ridge)

            # Fit g(S) to predict Y
            g_model.fit(S_train.reshape(-1, 1), Y_train)
            self._g_models[k] = g_model

            # Fit ECDF on g(S) predictions for this fold's training data
            g_train = g_model.predict(S_train.reshape(-1, 1))
            self._ecdf_models[k] = _fit_ecdf(g_train)

        # Step 2: Fit isotonic on rank-transformed space for each fold
        for k in unique_folds:
            train_mask = folds != k

            if self._g_models.get(k) is not None:
                # Transform training data through g and ECDF
                g_train = self._g_models[k].predict(S[train_mask].reshape(-1, 1))
                T_ranked_train = self._ecdf_models[k](g_train)
            else:
                # Fallback: use ECDF on original scores
                T_ranked_train = self._ecdf_models[k](S[train_mask])

            # Fit isotonic on ranked space
            iso = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds="clip")
            iso.fit(T_ranked_train, Y[train_mask])
            self._iso_models[k] = iso

    def _fit_full_models(self, S: np.ndarray, Y: np.ndarray) -> None:
        """Fit full models on all data for inference."""
        if self.selected_mode == "monotone" or self.mode == "monotone":
            # Fit full monotone model
            self._full_monotone_model = IsotonicRegression(
                y_min=0.0, y_max=1.0, out_of_bounds="clip"
            )
            self._full_monotone_model.fit(S, Y)

        if (
            self.selected_mode == "two_stage"
            or self.mode == "two_stage"
            or self.mode == "auto"
        ):
            # Fit full two-stage model
            if len(S) >= 20:
                # Fit g(S)
                n_knots = min(max(5, self.n_splines), len(S) // 4)
                spline = SplineTransformer(
                    n_knots=n_knots, degree=3, include_bias=False
                )
                ridge = RidgeCV(alphas=np.logspace(-3, 3, 13), store_cv_values=False)
                self._full_g_model = make_pipeline(spline, ridge)
                self._full_g_model.fit(S.reshape(-1, 1), Y)

                # Fit ECDF on g(S)
                g_full = self._full_g_model.predict(S.reshape(-1, 1))
                self._full_ecdf = _fit_ecdf(g_full)

                # Fit isotonic on ranked space
                T_ranked = self._full_ecdf(g_full)
                self._full_iso_model = IsotonicRegression(
                    y_min=0.0, y_max=1.0, out_of_bounds="clip"
                )
                self._full_iso_model.fit(T_ranked, Y)
            else:
                # Fallback to monotone
                self._full_monotone_model = IsotonicRegression(
                    y_min=0.0, y_max=1.0, out_of_bounds="clip"
                )
                self._full_monotone_model.fit(S, Y)

    def predict(self, S: np.ndarray, folds: Optional[np.ndarray] = None) -> np.ndarray:
        """Predict calibrated values.

        Args:
            S: Judge scores to calibrate
            folds: Optional fold assignments for OOF prediction

        Returns:
            Calibrated predictions
        """
        mode = self.selected_mode or self.mode

        if mode == "monotone":
            return self._predict_monotone(S, folds)
        elif mode == "two_stage":
            return self._predict_two_stage(S, folds)
        else:
            raise ValueError(f"No fitted models for mode: {mode}")

    def _predict_monotone(
        self, S: np.ndarray, folds: Optional[np.ndarray] = None
    ) -> np.ndarray:
        """Predict using monotone models."""
        if folds is None:
            # Use full model for inference
            if self._full_monotone_model is not None:
                return np.asarray(self._full_monotone_model.predict(S))
            else:
                # Fallback to ensemble average if full model not fitted
                preds = []
                for model in self._monotone_models.values():
                    preds.append(model.predict(S))
                return np.asarray(np.mean(preds, axis=0))
        else:
            # OOF prediction
            Y_hat = np.zeros_like(S)
            for k in np.unique(folds):
                mask = folds == k
                if k in self._monotone_models:
                    Y_hat[mask] = self._monotone_models[k].predict(S[mask])
                else:
                    # Fallback to full model if available
                    if self._full_monotone_model is not None:
                        Y_hat[mask] = self._full_monotone_model.predict(S[mask])
                    else:
                        # Last resort: ensemble average
                        preds = []
                        for model in self._monotone_models.values():
                            preds.append(model.predict(S[mask]))
                        Y_hat[mask] = np.mean(preds, axis=0)
            return Y_hat

    def _predict_two_stage(
        self, S: np.ndarray, folds: Optional[np.ndarray] = None
    ) -> np.ndarray:
        """Predict using two-stage models."""
        if folds is None:
            # Use full model for inference
            if self._full_g_model is not None and self._full_iso_model is not None and self._full_ecdf is not None:
                g_pred = self._full_g_model.predict(S.reshape(-1, 1))
                T_ranked = self._full_ecdf(g_pred)
                return np.asarray(self._full_iso_model.predict(T_ranked))
            elif self._full_monotone_model is not None:
                # Fallback to monotone if two-stage wasn't fitted
                return np.asarray(self._full_monotone_model.predict(S))
            else:
                # Last resort: ensemble average
                preds = []
                for k in self._g_models.keys():
                    if k in self._ecdf_models and k in self._iso_models:
                        g_model = self._g_models[k]
                        iso_model = self._iso_models[k]
                        if g_model is not None:
                            g_pred = g_model.predict(S.reshape(-1, 1))
                            T_ranked = self._ecdf_models[k](g_pred)
                        else:
                            T_ranked = self._ecdf_models[k](S)
                        preds.append(iso_model.predict(T_ranked))
                if preds:
                    return np.asarray(np.mean(preds, axis=0))
                else:
                    # Ultimate fallback: return mean of training labels
                    return np.full_like(S, 0.5)
        else:
            # OOF prediction
            Y_hat = np.zeros_like(S)
            for k in np.unique(folds):
                mask = folds == k
                if (
                    k in self._g_models
                    and k in self._iso_models
                    and k in self._ecdf_models
                ):
                    if self._g_models[k] is not None:
                        g_pred = self._g_models[k].predict(S[mask].reshape(-1, 1))
                        T_ranked = self._ecdf_models[k](g_pred)
                    else:
                        T_ranked = self._ecdf_models[k](S[mask])
                    Y_hat[mask] = self._iso_models[k].predict(T_ranked)
                else:
                    # Fallback to full model if available
                    if (
                        self._full_g_model is not None
                        and self._full_iso_model is not None
                        and self._full_ecdf is not None
                    ):
                        g_pred = self._full_g_model.predict(S[mask].reshape(-1, 1))
                        T_ranked = self._full_ecdf(g_pred)
                        Y_hat[mask] = self._full_iso_model.predict(T_ranked)
                    elif self._full_monotone_model is not None:
                        Y_hat[mask] = self._full_monotone_model.predict(S[mask])
                    else:
                        # Ultimate fallback
                        Y_hat[mask] = np.mean(S[mask])
            return Y_hat

    def _select_best_mode(self, S: np.ndarray, Y: np.ndarray, folds: np.ndarray) -> Literal["monotone", "two_stage"]:
        """Select best mode based on OOF RMSE."""
        # Get OOF predictions for each mode
        pred_mono = self._predict_monotone(S, folds)
        pred_two_stage = self._predict_two_stage(S, folds)

        # Calculate RMSEs
        rmse_mono = np.sqrt(np.mean((Y - pred_mono) ** 2))
        rmse_two_stage = np.sqrt(np.mean((Y - pred_two_stage) ** 2))

        # Check for non-monotonicity by comparing performance in different regions
        # Sort by judge scores
        sort_idx = np.argsort(S)

        # Split into thirds and check local performance
        n_third = len(S) // 3
        low_mask = sort_idx[:n_third]
        mid_mask = sort_idx[n_third : 2 * n_third]
        high_mask = sort_idx[2 * n_third :]

        rmse_mono_low = np.sqrt(np.mean((Y[low_mask] - pred_mono[low_mask]) ** 2))
        rmse_flex_low = np.sqrt(np.mean((Y[low_mask] - pred_two_stage[low_mask]) ** 2))

        rmse_mono_mid = np.sqrt(np.mean((Y[mid_mask] - pred_mono[mid_mask]) ** 2))
        rmse_flex_mid = np.sqrt(np.mean((Y[mid_mask] - pred_two_stage[mid_mask]) ** 2))

        rmse_mono_high = np.sqrt(np.mean((Y[high_mask] - pred_mono[high_mask]) ** 2))
        rmse_flex_high = np.sqrt(
            np.mean((Y[high_mask] - pred_two_stage[high_mask]) ** 2)
        )

        # Count regions where two-stage is better
        better_count = 0
        if rmse_flex_low < rmse_mono_low:
            better_count += 1
        if rmse_flex_mid < rmse_mono_mid:
            better_count += 1
        if rmse_flex_high < rmse_mono_high:
            better_count += 1

        # Apply 1-SE rule: prefer simpler model unless complex is significantly better
        # Standard error of RMSE estimate (not of mean)
        residuals_mono = Y - pred_mono
        se_mono = np.std(residuals_mono**2) / np.sqrt(len(S))

        logger.info(f"Calibration mode selection:")
        logger.info(
            f"  Overall RMSE - Monotone: {rmse_mono:.4f}, Two-stage: {rmse_two_stage:.4f}"
        )
        logger.info(
            f"  Regional performance - Two-stage better in {better_count}/3 regions"
        )
        logger.debug(f"    Low S: Mono={rmse_mono_low:.4f}, Flex={rmse_flex_low:.4f}")
        logger.debug(f"    Mid S: Mono={rmse_mono_mid:.4f}, Flex={rmse_flex_mid:.4f}")
        logger.debug(
            f"    High S: Mono={rmse_mono_high:.4f}, Flex={rmse_flex_high:.4f}"
        )

        # Select two-stage if:
        # 1. It's significantly better overall (1-SE rule), OR
        # 2. It's better in at least 2/3 regions (indicates non-monotonicity)
        if rmse_two_stage < rmse_mono - se_mono or better_count >= 2:
            logger.info(f"  → Selected: two_stage (better in {better_count}/3 regions)")
            return "two_stage"
        else:
            logger.info(f"  → Selected: monotone (simpler model preferred)")
            return "monotone"

    def index(self, S: np.ndarray, folds: Optional[np.ndarray] = None) -> np.ndarray:
        """Return the index used for isotonic regression.

        For monotone mode: Returns S (or normalized S)
        For two-stage mode: Returns rank_transform(g(S))

        Args:
            S: Judge scores
            folds: Optional fold assignments for OOF transformation

        Returns:
            Index values for isotonic regression
        """
        mode = self.selected_mode or self.mode

        if mode == "monotone":
            # In monotone mode, the index is just S
            return S

        elif mode == "two_stage":
            if folds is None:
                # Use full model for inference
                if self._full_g_model is not None:
                    g_pred = self._full_g_model.predict(S.reshape(-1, 1))
                    if self._full_ecdf is not None:
                        return np.asarray(self._full_ecdf(g_pred))
                    else:
                        # Fallback to rank transform if ECDF not available
                        return self._rank_transform(g_pred)
                else:
                    # Fallback: ensemble average
                    g_ensemble = np.zeros_like(S)
                    count = 0
                    for g_model in self._g_models.values():
                        if g_model is not None:
                            g_ensemble += g_model.predict(S.reshape(-1, 1))
                            count += 1
                    if count > 0:
                        g_ensemble /= count
                        return self._rank_transform(g_ensemble)
                    else:
                        return S  # Ultimate fallback
            else:
                # OOF transformation for training
                T = np.zeros_like(S)
                for k in np.unique(folds):
                    mask = folds == k
                    if k in self._g_models and k in self._ecdf_models:
                        if self._g_models[k] is not None:
                            g_pred = self._g_models[k].predict(S[mask].reshape(-1, 1))
                            T[mask] = self._ecdf_models[k](g_pred)
                        else:
                            # Fallback to ECDF on raw scores
                            T[mask] = self._ecdf_models[k](S[mask])
                    else:
                        # Ultimate fallback
                        T[mask] = S[mask]
                return T
        else:
            # Fallback for unknown modes
            return S

    def _rank_transform(self, x: np.ndarray) -> np.ndarray:
        """Simple rank transformation to [0, 1]."""
        from scipy.stats import rankdata

        ranks = rankdata(x, method="average")
        return np.asarray((ranks - 0.5) / len(x))

    def get_diagnostics(self) -> Dict[str, Any]:
        """Get diagnostics about the fitted calibrator."""
        return {
            "mode": self.selected_mode or self.mode,
            "n_folds": len(self._monotone_models or self._iso_models),
            "has_two_stage": bool(self._g_models),
            "has_monotone": bool(self._monotone_models),
        }

    @property
    def iso_reg(self) -> Optional[Any]:
        """Get the isotonic regression model for compatibility."""
        mode = self.selected_mode or self.mode
        if mode == "monotone":
            return self._full_monotone_model
        else:
            # For two-stage, return the final isotonic model
            return self._full_iso_model


=== ./cje/calibration/iic.py ===

"""
Isotonic Influence Control (IIC) for variance reduction.

IIC residualizes influence functions against judge scores to reduce variance
without changing the target estimand. This is a pure variance reduction
technique that operates on the influence function layer.

Theory:
    For any asymptotically linear estimator with IF φ:
    - Fit isotonic regression: ĥ(s) ≈ E[φ|S=s]
    - Residualize: φ̃ = φ - ĥ(S)
    - Properties:
        * E[φ̃] = E[φ] = 0 (unbiased)
        * Var(φ̃) ≤ Var(φ) (variance reduction)
        * Same asymptotic distribution, tighter CIs

This is a key contribution from the CJE paper that provides "free" variance
reduction by exploiting the judge score structure.
"""

from dataclasses import dataclass
from typing import Optional, Dict, Any, Tuple
import numpy as np
from sklearn.isotonic import IsotonicRegression
import logging

logger = logging.getLogger(__name__)


@dataclass
class IICConfig:
    """Configuration for Isotonic Influence Control.

    Args:
        enable: Whether to apply IIC (default True - it's free variance reduction!)
        use_cross_fit: Whether to use fold-honest fitting (recommended)
        min_samples_for_iic: Minimum samples to attempt IIC
        compute_diagnostics: Whether to compute R² and other diagnostics
        store_components: Whether to store E[φ|S] for visualization
    """

    enable: bool = True  # On by default - no reason not to use it
    use_cross_fit: bool = True
    min_samples_for_iic: int = 50  # Need enough data for isotonic regression
    compute_diagnostics: bool = True
    store_components: bool = False  # For debugging/visualization


class IsotonicInfluenceControl:
    """Reduce influence function variance via isotonic residualization.

    This implements the IIC component from the CJE paper, providing
    variance reduction by exploiting the relationship between influence
    functions and judge scores.

    The key insight: influence functions often correlate with judge scores
    (since both relate to outcome quality). By removing the predictable
    component E[φ|S], we keep the same estimand but reduce variance.
    """

    def __init__(self, config: Optional[IICConfig] = None):
        """Initialize IIC.

        Args:
            config: Configuration (uses defaults if None)
        """
        self.config = config or IICConfig()
        self._diagnostics: Dict[str, Dict[str, Any]] = {}
        self._fitted_components: Dict[str, np.ndarray] = {}

    def residualize(
        self,
        influence: np.ndarray,
        judge_scores: np.ndarray,
        policy: str,
        fold_ids: Optional[np.ndarray] = None,
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Residualize influence function against judge scores.

        This is the main entry point for IIC. It fits E[φ|S] using isotonic
        regression and returns the residuals φ - Ê[φ|S].

        The direction (increasing/decreasing) is chosen automatically based on
        which gives better fit, measured by Spearman correlation.

        Args:
            influence: Raw influence function values (per-sample contributions)
            judge_scores: Judge scores S (ordering index)
            policy: Policy name (for diagnostics)
            fold_ids: Optional fold assignments for cross-fitting
                     (use same folds as reward calibration for consistency)

        Returns:
            residualized_if: φ̃ = φ - Ê[φ|S] (same mean, less variance)
            diagnostics: Dict with R², variance reduction, etc.
        """
        if not self.config.enable:
            return influence, {"applied": False, "reason": "disabled"}

        n = len(influence)
        if n < self.config.min_samples_for_iic:
            logger.debug(f"Too few samples ({n}) for IIC")
            return influence, {"applied": False, "reason": "insufficient_samples"}

        # Validate inputs
        if len(judge_scores) != n:
            logger.warning(
                f"Length mismatch: {n} influences, {len(judge_scores)} scores"
            )
            return influence, {"applied": False, "reason": "length_mismatch"}

        # Check for valid values
        if not np.all(np.isfinite(influence)):
            logger.warning(f"Non-finite influence values for {policy}")
            return influence, {"applied": False, "reason": "non_finite_influence"}

        valid_scores = np.isfinite(judge_scores)
        if not np.all(valid_scores):
            # Handle missing judge scores gracefully
            if valid_scores.sum() < self.config.min_samples_for_iic:
                return influence, {
                    "applied": False,
                    "reason": "insufficient_valid_scores",
                }
            # We'll only fit on valid scores

        # Fit E[φ|S] using isotonic regression
        if self.config.use_cross_fit and fold_ids is not None:
            fitted_values = self._fit_cross_fitted(
                influence, judge_scores, fold_ids, valid_scores
            )
        else:
            fitted_values = self._fit_global(influence, judge_scores, valid_scores)

        # Compute residuals: φ̃ = φ - Ê[φ|S]
        residuals = influence - fitted_values

        # Store fitted component if requested (for visualization)
        if self.config.store_components:
            self._fitted_components[policy] = fitted_values

        # Compute diagnostics
        diagnostics = {"applied": True}
        if self.config.compute_diagnostics:
            diagnostics.update(
                self._compute_diagnostics(influence, fitted_values, residuals, policy)
            )

        self._diagnostics[policy] = diagnostics

        logger.debug(
            f"IIC for {policy}: R²={diagnostics.get('r_squared', 0):.3f}, "
            f"variance reduction={diagnostics.get('var_reduction', 0):.1%}"
        )

        return residuals, diagnostics

    def _fit_global(
        self, influence: np.ndarray, judge_scores: np.ndarray, valid_mask: np.ndarray
    ) -> np.ndarray:
        """Global isotonic fit (simpler but may overfit).

        This fits a single isotonic regression on all data.
        Direction (increasing/decreasing) is chosen based on Spearman correlation.
        """
        fitted = np.zeros_like(influence)

        if valid_mask.sum() < 2:
            # Not enough valid data
            return fitted

        # Determine direction based on Spearman correlation
        from scipy.stats import spearmanr

        corr, _ = spearmanr(judge_scores[valid_mask], influence[valid_mask])

        # Choose direction: increasing if positive correlation, decreasing if negative
        increasing = corr >= 0

        # Fit isotonic regression on valid data with chosen direction
        iso = IsotonicRegression(increasing=increasing, out_of_bounds="clip")
        iso.fit(judge_scores[valid_mask], influence[valid_mask])

        # Predict for all data
        fitted[valid_mask] = iso.predict(judge_scores[valid_mask])

        # For invalid scores, use mean of influence (no reduction)
        fitted[~valid_mask] = influence[valid_mask].mean()

        # Store direction in diagnostics
        self._last_direction = "increasing" if increasing else "decreasing"
        self._last_correlation = float(corr)

        return fitted

    def _fit_cross_fitted(
        self,
        influence: np.ndarray,
        judge_scores: np.ndarray,
        fold_ids: np.ndarray,
        valid_mask: np.ndarray,
    ) -> np.ndarray:
        """Fold-honest isotonic fit (recommended).

        This prevents overfitting by using out-of-fold predictions.
        Each sample's E[φ|S] is predicted using a model trained on other folds.
        Direction is chosen per fold based on training data correlation.
        """
        fitted = np.zeros_like(influence)
        unique_folds = np.unique(fold_ids[fold_ids >= 0])

        if len(unique_folds) < 2:
            # Not enough folds for cross-fitting
            logger.debug("Insufficient folds for cross-fitting, using global fit")
            return self._fit_global(influence, judge_scores, valid_mask)

        # Track directions used across folds for diagnostics
        directions_used = []
        correlations = []

        from scipy.stats import spearmanr

        for fold in unique_folds:
            # Define train and test sets
            train_mask = (fold_ids >= 0) & (fold_ids != fold) & valid_mask
            test_mask = (fold_ids == fold) & valid_mask

            if train_mask.sum() < 10 or test_mask.sum() == 0:
                # Not enough data in this fold
                if test_mask.sum() > 0:
                    fitted[test_mask] = (
                        influence[train_mask].mean() if train_mask.sum() > 0 else 0
                    )
                continue

            # Determine direction based on training data correlation
            corr, _ = spearmanr(judge_scores[train_mask], influence[train_mask])
            increasing = corr >= 0
            directions_used.append("increasing" if increasing else "decreasing")
            correlations.append(float(corr))

            # Fit isotonic regression on training folds with chosen direction
            iso = IsotonicRegression(increasing=increasing, out_of_bounds="clip")
            iso.fit(judge_scores[train_mask], influence[train_mask])

            # Predict on test fold
            fitted[test_mask] = iso.predict(judge_scores[test_mask])

        # Handle samples not in any fold (shouldn't happen with our fold system)
        unfitted = (fold_ids < 0) | ~valid_mask
        if unfitted.sum() > 0:
            fitted[unfitted] = (
                influence[valid_mask].mean() if valid_mask.sum() > 0 else 0
            )

        # Store aggregated direction info for diagnostics
        if directions_used:
            # Most common direction across folds
            from collections import Counter

            direction_counts = Counter(directions_used)
            self._last_direction = direction_counts.most_common(1)[0][0]
            self._last_correlation = np.mean(correlations) if correlations else 0.0
            self._fold_directions = directions_used  # Store for detailed diagnostics

        return fitted

    def _compute_diagnostics(
        self,
        original: np.ndarray,
        fitted: np.ndarray,
        residual: np.ndarray,
        policy: str,
    ) -> Dict[str, Any]:
        """Compute IIC diagnostics for reporting and visualization.

        Key metrics:
        - R²: How much of the IF variance is explained by judge scores
        - Variance reduction: How much we reduced the IF variance
        - SE reduction: How much we reduced the standard error
        """
        var_original = np.var(original, ddof=1)
        var_residual = np.var(residual, ddof=1)
        var_fitted = np.var(fitted, ddof=1)

        # R² of the isotonic fit (how well S predicts φ)
        ss_tot = np.sum((original - original.mean()) ** 2)
        ss_res = np.sum((original - fitted) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

        # Variance reduction (main benefit)
        var_reduction = 1 - (var_residual / var_original) if var_original > 0 else 0

        # Standard error reduction (what users care about)
        se_reduction = (
            1 - np.sqrt(max(0, var_residual) / var_original) if var_original > 0 else 0
        )

        # Effective sample size gain (interpretable)
        # ESS ∝ 1/Var, so ESS gain = var_original/var_residual
        ess_gain = var_original / var_residual if var_residual > 0 else 1.0

        diagnostics = {
            "policy": policy,
            "r_squared": float(max(0, r_squared)),  # Can be negative if fit is terrible
            "var_original": float(var_original),
            "var_residual": float(var_residual),
            "var_fitted": float(var_fitted),
            "var_reduction": float(max(0, var_reduction)),
            "se_reduction": float(max(0, se_reduction)),
            "ess_gain": float(ess_gain),
            "n_samples": len(original),
            "mean_preserved": abs(residual.mean() - original.mean()) < 1e-10,
        }

        # Add direction information if available
        if hasattr(self, "_last_direction"):
            diagnostics["direction"] = self._last_direction
            diagnostics["correlation"] = self._last_correlation

        # Add per-fold directions if cross-fitting was used
        if hasattr(self, "_fold_directions"):
            from collections import Counter

            direction_counts = Counter(self._fold_directions)
            diagnostics["fold_directions"] = dict(direction_counts)

        return diagnostics

    def get_diagnostics(self, policy: Optional[str] = None) -> Dict[str, Any]:
        """Get IIC diagnostics for reporting.

        Args:
            policy: Specific policy (None for all)

        Returns:
            Dictionary of diagnostics
        """
        if policy is not None:
            return self._diagnostics.get(policy, {})
        return self._diagnostics.copy()

    def get_fitted_component(self, policy: str) -> Optional[np.ndarray]:
        """Get fitted E[φ|S] component for visualization.

        Only available if store_components=True in config.
        """
        return self._fitted_components.get(policy)

    def summary(self) -> str:
        """Generate summary of IIC performance across policies."""
        if not self._diagnostics:
            return "No IIC diagnostics available"

        lines = ["IIC Performance Summary:"]
        for policy, diag in self._diagnostics.items():
            if diag.get("applied", False):
                lines.append(
                    f"  {policy}: R²={diag.get('r_squared', 0):.3f}, "
                    f"SE reduction={diag.get('se_reduction', 0):.1%}, "
                    f"ESS gain={diag.get('ess_gain', 1):.2f}x"
                )
            else:
                lines.append(
                    f"  {policy}: Not applied ({diag.get('reason', 'unknown')})"
                )

        return "\n".join(lines)


=== ./cje/calibration/isotonic.py ===

"""Isotonic calibration for importance weights.

This module provides variance-controlled IPS weight calibration using:
- Single-pass mean-1 PAV (no cross-fitting needed)
- Closed-form variance-safe blending with feasibility handling
- Robust edge case handling for sparse/degenerate weights
"""

from __future__ import annotations
import numpy as np
import logging
from typing import Dict, Tuple, Optional, Union
from sklearn.isotonic import isotonic_regression

logger = logging.getLogger(__name__)

# Tolerances
EPS = 1e-12
MEAN_TOL = 1e-10
VAR_TOL = 1.001  # allow 0.1% wiggle room on the variance cap


def _pav_mean1_projection_sorted(
    w_sorted_mean1: np.ndarray, mode: str = "fast"
) -> np.ndarray:
    """
    Inputs:
      w_sorted_mean1: weights sorted ASCENDING, already normalized to mean 1.
      mode: "fast" (single-pass) or "exact" (bisection on Lagrange multiplier)

    Returns:
      v: monotone non-decreasing vector with EXACT mean 1

    Implementation:
      - fast: project cumulative excess with increasing PAV, then differentiate
      - exact: true Euclidean projection via bisection (30-40 PAV calls)
    """
    if mode == "exact":
        return _mean_one_isotonic_projection_exact(w_sorted_mean1)

    # Fast mode: single-pass with increasing=False for better ESS in heavy tails
    z = np.cumsum(w_sorted_mean1 - 1.0)  # cumulative excess over mean
    y = isotonic_regression(
        z, increasing=False
    )  # DECREASING gives convex shape, better for heavy tails
    v = np.diff(np.concatenate(([0.0], y))) + 1.0
    v = np.clip(v, 0.0, None)

    # Safety: ensure monotonicity (isotonic repair if needed)
    v = np.maximum.accumulate(v)

    # Enforce exact mean 1 without adding EPS (keeps math for blending exact)
    tot = float(v.sum())
    if tot <= 0.0:
        v = np.full_like(v, 1.0)  # degenerate fallback (should be rare)
    else:
        v *= v.size / tot

    # Safety assertions
    assert np.all(np.diff(v) >= -1e-12), "Monotonicity violated in PAV"
    assert abs(v.mean() - 1.0) < 1e-10, f"Mean not preserved: {v.mean()}"

    return v


def _mean_one_isotonic_projection_exact(
    w_sorted_mean1: np.ndarray, tol: float = 1e-10, max_iters: int = 40
) -> np.ndarray:
    """
    Exact Euclidean projection onto {v: v₁≤…≤vₙ, mean(v)=1} via bisection.

    This gives the mathematically correct projection with guaranteed monotonicity
    and exact mean=1, at the cost of ~30-40 PAV calls.
    """
    n = len(w_sorted_mean1)
    lo, hi = -float(w_sorted_mean1.max()), float(w_sorted_mean1.max())

    for _ in range(max_iters):
        mu = 0.5 * (lo + hi)
        v = isotonic_regression(w_sorted_mean1 - mu, increasing=True)
        m = float(v.mean())
        if abs(m - 1.0) <= tol:
            break
        if m > 1.0:
            lo = mu
        else:
            hi = mu

    # Ensure exact mean to numerical tolerance
    tot = float(v.sum())
    if tot > 0:
        v *= n / tot
    else:
        v = np.ones_like(w_sorted_mean1)  # Degenerate case

    return np.asarray(v)


def _variance_safe_blend_closed_form(
    raw_norm: np.ndarray,
    cal_norm: np.ndarray,
    max_variance_ratio: float = 1.0,
    tol: float = 1e-12,
) -> Tuple[np.ndarray, float, Dict]:
    """
    Blend raw and calibrated (both mean=1) to satisfy Var(out) <= max_ratio * Var(raw)
    when FEASIBLE; otherwise return the variance-minimizing blend.

    Returns:
      out_mean1, alpha, info_dict
    """
    r = np.asarray(raw_norm, dtype=float)
    c = np.asarray(cal_norm, dtype=float)
    Vr = float(r.var())
    Vc = float(c.var())
    target = Vr * max_variance_ratio

    # Quick accept: calibrated already within cap (allow tiny slack)
    if Vc <= target * VAR_TOL:
        return c, 1.0, dict(feasible=True, achieved_var=Vc, target_var=target)

    d = c - r
    # Var(d) with population convention
    Vd = float(d.var())
    # Cov(r,d) with mean(r)=1, mean(d)=0 ⇒ Cov(r,d) = E[(r-1)*d]
    C = float(np.mean((r - 1.0) * d))

    if Vd <= tol:
        # No direction to move; return raw
        return r, 0.0, dict(feasible=(Vr <= target), achieved_var=Vr, target_var=target)

    # Variance along the path: V(α) = Vr + 2αC + α^2 Vd
    # Minimum at α* = -C / Vd (clip to [0,1])
    alpha_star = float(np.clip(-C / Vd, 0.0, 1.0))
    Vmin = Vr + 2.0 * alpha_star * C + (alpha_star**2) * Vd

    # If target is unattainable, return best possible α*
    if Vmin > target * VAR_TOL:
        out = (1.0 - alpha_star) * r + alpha_star * c
        return (
            out,
            alpha_star,
            dict(
                feasible=False,
                achieved_var=Vmin,
                target_var=target,
                note="Target variance ratio unattainable; using variance-minimizing blend",
            ),
        )

    # Target is feasible: pick the LARGEST feasible α in [0,1]
    # Solve α^2 Vd + 2α C + (Vr - target) <= 0
    disc = C * C - Vd * (Vr - target)
    # Numerical guard
    if disc < 0:
        disc = 0.0
    sqrt_disc = float(np.sqrt(disc))
    alpha1 = (-C - sqrt_disc) / (Vd + tol)
    alpha2 = (-C + sqrt_disc) / (Vd + tol)

    # Feasible interval is [alpha1, alpha2]; choose the largest feasible in [0,1]
    candidate = float(np.clip(alpha2, 0.0, 1.0))
    out = (1.0 - candidate) * r + candidate * c
    Vach = float(out.var())

    # Final re-check with slack; if numerically over, fall back to alpha1 or α*
    if Vach > target * VAR_TOL:
        candidate_alt = float(np.clip(alpha1, 0.0, 1.0))
        out_alt = (1.0 - candidate_alt) * r + candidate_alt * c
        Vach_alt = float(out_alt.var())
        if Vach_alt <= target * VAR_TOL:
            return (
                out_alt,
                candidate_alt,
                dict(feasible=True, achieved_var=Vach_alt, target_var=target),
            )
        # Fall back to α* (will satisfy target up to slack because target is feasible modulo numerics)
        out_star = (1.0 - alpha_star) * r + alpha_star * c
        Vach_star = float(out_star.var())
        return (
            out_star,
            alpha_star,
            dict(
                feasible=True,
                achieved_var=Vach_star,
                target_var=target,
                note="Numerical instability; using variance-minimizing α*",
            ),
        )

    return out, candidate, dict(feasible=True, achieved_var=Vach, target_var=target)


def calibrate_to_target_mean(
    weights: np.ndarray,
    target_mean: float = 1.0,
    enforce_variance_nonincrease: bool = True,
    max_variance_ratio: float = 1.0,  # ≤1.0 ⇒ no increase; <1.0 ⇒ force reduction
    return_diagnostics: bool = False,
    projection_mode: str = "exact",  # Always use exact mode for consistency
    ordering_index: Optional[np.ndarray] = None,
) -> Union[np.ndarray, Tuple[np.ndarray, Dict]]:
    """
    Variance-controlled IPS weight calibration (Hájek mean-one IPS):

    1) Sort by ordering_index (or raw weights if not provided), run mean-1 PAV.
    2) Optionally apply CLOSED-FORM variance-safe blend toward raw (mean-1).
    3) Rescale to target_mean (exact).

    This method trades a small amount of bias for substantially reduced variance,
    improving stability and effective sample size. The bias-variance tradeoff is
    controlled by the max_variance_ratio parameter. All blending happens in
    mean-one space to ensure correct variance calculations.

    Guarantees:
      • output ≥ 0
      • sample mean == target_mean (within MEAN_TOL)
      • var(out)/var(raw) ≤ max_variance_ratio (when feasible) if enforced
      • weights are non-decreasing in the rank order of the ordering index

    Args:
        weights: Raw importance weights (should be non-negative)
        target_mean: Target mean for calibrated weights (default 1.0 for SNIPS)
        enforce_variance_nonincrease: Whether to cap variance at raw level
        max_variance_ratio: Maximum allowed variance ratio (≤1.0)
        return_diagnostics: If True, return (weights, diagnostics_dict)
        projection_mode: Always "exact" (bisection) for consistency
        ordering_index: Optional array to determine isotonic ordering (e.g., judge scores).
                       If None, sorts by raw weights (backward compatibility).

                       Note: When ordering_index is uncorrelated with weights (e.g., judge
                       scores uncorrelated with importance ratios), the isotonic projection
                       may produce nearly constant weights. This is expected behavior and
                       provides variance stabilization even without a monotonic relationship.

    Returns:
        Calibrated weights, or (weights, diagnostics) if return_diagnostics=True
    """
    w = np.asarray(weights, dtype=float)
    n = w.size
    if n == 0:
        if return_diagnostics:
            return w, dict(n_samples=0, alpha_blend=None, feasible=None)
        return w

    # Check for negative weights (importance weights should be non-negative)
    n_negative = int((w < -1e-15).sum())
    if n_negative > 0:
        logger.warning(f"Clipping {n_negative} negative weights (min={w.min():.3e})")
    w = np.maximum(w, 0.0)

    # Handle degenerate cases
    s = float(w.sum())
    if s <= 0:
        # Safer: constant weights at the target mean
        out = np.full_like(w, target_mean)
        if return_diagnostics:
            return out, dict(
                n_samples=n,
                alpha_blend=None,
                feasible=None,
                note="All-zero weights; returned constant weights",
            )
        return out

    # Normalize raw to mean 1 (baseline for SNIPS & variance comparison)
    mean_w = s / n  # s > 0 by construction
    raw_norm = w / mean_w  # exact mean-1 normalization (no EPS)
    raw_var = float(raw_norm.var())

    # If essentially constant, just return target_mean
    if float(np.max(raw_norm) - np.min(raw_norm)) < 1e-8 or raw_var < 1e-12:
        out = np.full_like(w, target_mean)
        if return_diagnostics:
            return out, dict(
                n_samples=n,
                alpha_blend=None,
                feasible=None,
                note="Constant weights detected",
            )
        return out

    # ---- Mean-1 PAV on the full vector (no cross-fitting) ----
    # Use ordering_index if provided (e.g., judge scores), otherwise fall back to raw weights
    if ordering_index is not None:
        if len(ordering_index) != n:
            raise ValueError(
                f"ordering_index length ({len(ordering_index)}) must match weights length ({n})"
            )
        order = np.argsort(ordering_index, kind="stable")  # Sort by the provided index

        # Handle ties in ordering_index by pooling weights within tied groups
        sorted_index = ordering_index[order]
        sorted_weights = raw_norm[order]

        # Find unique values and their positions
        unique_vals, inverse_indices = np.unique(sorted_index, return_inverse=True)

        # Pool weights within tied groups (average within each group)
        pooled_weights = np.zeros_like(sorted_weights)
        for i in range(len(unique_vals)):
            mask = inverse_indices == i
            pooled_weights[mask] = sorted_weights[mask].mean()

        # Now apply PAV to the pooled weights
        cal_sorted = _pav_mean1_projection_sorted(pooled_weights, mode=projection_mode)
    else:
        order = np.argsort(
            raw_norm, kind="stable"
        )  # Fall back to raw weights for backward compatibility
        cal_sorted = _pav_mean1_projection_sorted(raw_norm[order], mode=projection_mode)

    cal_norm = np.empty_like(raw_norm)
    cal_norm[order] = cal_sorted  # mean 1, monotone by construction

    # ---- Optional: variance-safe blend (closed form) ----
    if enforce_variance_nonincrease:
        blended_norm, alpha, blend_info = _variance_safe_blend_closed_form(
            raw_norm, cal_norm, max_variance_ratio=max_variance_ratio
        )
    else:
        blended_norm = cal_norm
        alpha = 1.0
        blend_info = dict(
            feasible=None,
            achieved_var=float(cal_norm.var()),
            target_var=raw_var * max_variance_ratio,
        )

    # ---- Final scale to target mean ----
    # Exact rescale to target mean; blended_norm.mean() > 0 by construction
    out = blended_norm * (target_mean / float(blended_norm.mean()))

    # ---- Checks ----
    if out.min() < -1e-12:
        raise AssertionError(f"Negative calibrated weight: {out.min():.3e}")

    mean_err = abs(out.mean() - target_mean)
    if mean_err >= MEAN_TOL:
        # Check for sparse case (many zeros)
        zero_frac = (w < 1e-10).mean()
        if zero_frac > 0.7:  # More than 70% zeros
            # Relax tolerance for sparse cases
            sparse_tol = max(MEAN_TOL, target_mean * 0.01)  # 1% relative tolerance
            if mean_err < sparse_tol:
                pass  # Accept with relaxed tolerance
            else:
                raise AssertionError(
                    f"Mean not preserved (sparse case {zero_frac:.1%} zeros): "
                    f"expected {target_mean:.12f}, got {out.mean():.12f} "
                    f"(error={mean_err:.2e})"
                )
        else:
            raise AssertionError(
                f"Mean not preserved: expected {target_mean:.12f}, got {out.mean():.12f} "
                f"(error={mean_err:.2e})"
            )

    # Monotonic in ordering_index (or raw_norm if no ordering_index provided)
    # CRITICAL: Must check in the same order used for isotonic regression
    if ordering_index is not None:
        idx = np.argsort(ordering_index, kind="stable")
        index_sorted = ordering_index[idx]
        out_sorted = out[idx]
        boundaries = np.flatnonzero(np.diff(index_sorted) > 1e-15)
        if boundaries.size and np.any(
            out_sorted[boundaries + 1] < out_sorted[boundaries] - 1e-12
        ):
            # Find first violation for error message
            violations = np.where(
                out_sorted[boundaries + 1] < out_sorted[boundaries] - 1e-12
            )[0]
            i = boundaries[violations[0]]
            raise AssertionError(
                f"Monotonicity violated: index {index_sorted[i]:.3e} -> weight {out_sorted[i]:.3e} "
                f"but index {index_sorted[i+1]:.3e} -> weight {out_sorted[i+1]:.3e}"
            )
    else:
        # Fall back to checking raw_norm order for backward compatibility
        idx = np.argsort(raw_norm, kind="stable")
        raw_sorted = raw_norm[idx]
        out_sorted = out[idx]
        boundaries = np.flatnonzero(np.diff(raw_sorted) > 1e-15)
        if boundaries.size and np.any(
            out_sorted[boundaries + 1] < out_sorted[boundaries] - 1e-12
        ):
            # Find first violation for error message
            violations = np.where(
                out_sorted[boundaries + 1] < out_sorted[boundaries] - 1e-12
            )[0]
            i = boundaries[violations[0]]
            raise AssertionError(
                f"Monotonicity violated: weight {raw_sorted[i]:.3e} -> {out_sorted[i]:.3e} "
                f"but weight {raw_sorted[i+1]:.3e} -> {out_sorted[i+1]:.3e}"
            )

    if return_diagnostics:
        diagnostics = {
            "n_samples": n,
            "n_negative_clipped": n_negative,
            "alpha_blend": alpha,
            "feasible": blend_info.get("feasible"),
            "achieved_var": blend_info.get("achieved_var"),
            "target_var": blend_info.get("target_var"),
            "achieved_var_ratio": (
                blend_info["achieved_var"] / raw_var if raw_var > 0 else np.nan
            ),
            "target_var_ratio": max_variance_ratio,
        }
        if "note" in blend_info:
            diagnostics["note"] = blend_info["note"]
        return out, diagnostics

    return out


=== ./cje/calibration/judge.py ===

"""Judge score calibration using isotonic regression.

Calibrates cheap LLM judge scores to actual business KPIs/oracle labels
using monotonic regression on a labeled subset.
"""

import numpy as np
from typing import Optional, Tuple, Dict, List, Literal, TYPE_CHECKING
from sklearn.isotonic import IsotonicRegression
from sklearn.model_selection import KFold
from dataclasses import dataclass
import logging
import hashlib

if TYPE_CHECKING:
    from .flexible_calibrator import FlexibleCalibrator

logger = logging.getLogger(__name__)


@dataclass
class CalibrationResult:
    """Result of judge calibration."""

    calibrated_scores: np.ndarray  # Calibrated scores for all data
    calibration_rmse: float  # RMSE on oracle subset
    coverage_at_01: float  # Fraction within ±0.1 of true label
    n_oracle: int  # Number of oracle samples used
    calibrator: Optional["JudgeCalibrator"] = None  # The fitted calibrator
    fold_ids: Optional[np.ndarray] = None  # CV fold assignment for each sample
    oof_rmse: Optional[float] = None  # Out-of-fold RMSE (if cross-fitted)
    oof_coverage_at_01: Optional[float] = None  # Out-of-fold coverage (if cross-fitted)

    def summary(self) -> str:
        """Format calibration results."""
        return (
            f"Calibration Summary:\n"
            f"  Oracle samples: {self.n_oracle}\n"
            f"  RMSE: {self.calibration_rmse:.3f}\n"
            f"  Coverage (±0.1): {self.coverage_at_01:.1%}"
        )


class JudgeCalibrator:
    """Calibrate judge scores to oracle labels using isotonic regression.

    Args:
        random_seed: Random seed for reproducibility
    """

    def __init__(
        self,
        random_seed: int = 42,
        balance_oracle_folds: bool = True,
        calibration_mode: Optional[Literal["monotone", "two_stage", "auto"]] = "auto",
    ):
        """Initialize judge calibrator.

        Args:
            random_seed: Random seed for reproducibility
            balance_oracle_folds: Whether to balance oracle samples across folds
            calibration_mode: Calibration method to use:
                - 'auto' (default): Automatically select based on cross-validation
                - 'monotone': Force standard isotonic regression
                - 'two_stage': Force flexible two-stage calibration
                - None: Use monotone (for backward compatibility)
        """
        self.random_seed = random_seed
        self.balance_oracle_folds = (
            balance_oracle_folds  # Whether to balance oracle samples across folds
        )
        # None defaults to 'monotone' for backward compatibility
        self.calibration_mode = (
            calibration_mode if calibration_mode is not None else "monotone"
        )
        self.selected_mode: Optional[str] = None  # For storing auto mode selection
        self._final_calibrator: Optional[IsotonicRegression] = None
        self._flexible_calibrator: Optional["FlexibleCalibrator"] = None  # Will hold FlexibleCalibrator if needed
        self._fold_models: Dict[int, IsotonicRegression] = {}
        self._fold_ids: Optional[np.ndarray] = None
        self._n_folds: int = 5
        self._prompt_ids: Optional[List[str]] = (
            None  # Store prompt_ids for fold assignment
        )

    def fit_transform(
        self,
        judge_scores: np.ndarray,
        oracle_labels: Optional[np.ndarray] = None,
        oracle_mask: Optional[np.ndarray] = None,
    ) -> CalibrationResult:
        """Calibrate judge scores using oracle labels.

        Args:
            judge_scores: Raw judge scores for all data
            oracle_labels: True labels for oracle subset (if oracle_mask provided)
            oracle_mask: Boolean mask indicating which samples have oracle labels

        Returns:
            CalibrationResult with calibrated scores and diagnostics

        Example:
            # With explicit mask
            calibrator = JudgeCalibrator()
            result = calibrator.fit_transform(
                judge_scores=all_scores,
                oracle_labels=oracle_values,
                oracle_mask=has_oracle_label
            )

            # Or with implicit mask (oracle_labels shorter than judge_scores)
            result = calibrator.fit_transform(
                judge_scores=all_scores,
                oracle_labels=oracle_subset_labels
            )
        """
        judge_scores = np.asarray(judge_scores)
        n_total = len(judge_scores)

        # Handle different input formats
        if oracle_mask is not None:
            # Explicit mask provided
            oracle_mask = np.asarray(oracle_mask, dtype=bool)
            if oracle_labels is None:
                raise ValueError("oracle_labels required when oracle_mask provided")
            oracle_labels = np.asarray(oracle_labels)

            # Extract oracle subset
            oracle_scores = judge_scores[oracle_mask]
            oracle_y = oracle_labels

        elif oracle_labels is not None and len(oracle_labels) < n_total:
            # Oracle labels provided for first n samples
            n_oracle = len(oracle_labels)
            oracle_scores = judge_scores[:n_oracle]
            oracle_y = np.asarray(oracle_labels)
            oracle_mask = np.zeros(n_total, dtype=bool)
            oracle_mask[:n_oracle] = True

        elif oracle_labels is not None:
            # All data has oracle labels (no holdout)
            oracle_labels = np.asarray(oracle_labels)
            if len(oracle_labels) != n_total:
                raise ValueError(
                    f"oracle_labels length ({len(oracle_labels)}) must match "
                    f"judge_scores length ({n_total}) or be shorter for partial labeling"
                )
            oracle_scores = judge_scores
            oracle_y = oracle_labels
            oracle_mask = np.ones(n_total, dtype=bool)
        else:
            # No oracle labels provided
            raise ValueError(
                "oracle_labels is required for calibration. "
                "Provide oracle labels for at least a subset of samples."
            )

        n_oracle = len(oracle_y)

        if n_oracle < 10:
            raise ValueError(f"Too few oracle samples ({n_oracle}). Need at least 10.")

        # Initialize calibrated scores
        calibrated_scores = np.copy(judge_scores)

        # Use appropriate calibration based on mode
        if self.calibration_mode != "monotone":
            # Use flexible calibrator for auto/two_stage modes
            from .flexible_calibrator import FlexibleCalibrator

            self._flexible_calibrator = FlexibleCalibrator(
                mode=self.calibration_mode, random_seed=self.random_seed
            )
            # Create simple fold split for flexible calibrator
            oracle_folds = np.arange(len(oracle_y)) % 5
            self._flexible_calibrator.fit(oracle_scores, oracle_y, oracle_folds)

            # Log selected mode if auto was used
            if self.calibration_mode == "auto":
                selected = self._flexible_calibrator.selected_mode
                self.selected_mode = selected  # Store for metadata
                logger.info(f"Auto-calibration selected: {selected}")

            # Transform all scores
            calibrated_scores = np.clip(
                self._flexible_calibrator.predict(judge_scores), 0.0, 1.0
            )

            # Store isotonic for compatibility (if available)
            if hasattr(self._flexible_calibrator, "iso_reg"):
                self._final_calibrator = self._flexible_calibrator.iso_reg
        else:
            # Standard monotone calibration
            self._final_calibrator = IsotonicRegression(out_of_bounds="clip")
            self._final_calibrator.fit(oracle_scores, oracle_y)

            # Apply calibration to all samples using full model
            # Clip to [0,1] to ensure rewards stay in valid range even if oracle labels exceed bounds
            calibrated_scores = np.clip(
                self._final_calibrator.predict(judge_scores), 0.0, 1.0
            )

        # Compute diagnostics on oracle subset
        oracle_calibrated = calibrated_scores[oracle_mask]
        rmse = np.sqrt(np.mean((oracle_calibrated - oracle_y) ** 2))
        coverage_01 = np.mean(np.abs(oracle_calibrated - oracle_y) <= 0.1)

        # Log summary
        logger.info(
            f"Calibration complete: {n_oracle} oracle samples, "
            f"RMSE={rmse:.3f}, coverage@0.1={coverage_01:.1%}"
        )

        return CalibrationResult(
            calibrated_scores=calibrated_scores,
            calibration_rmse=float(rmse),
            coverage_at_01=float(coverage_01),
            n_oracle=n_oracle,
            calibrator=self,
            fold_ids=self._fold_ids,
        )

    def predict(self, judge_scores: np.ndarray) -> np.ndarray:
        """Apply calibration to new judge scores.

        Args:
            judge_scores: Judge scores to calibrate

        Returns:
            Calibrated scores
        """
        if self._flexible_calibrator is not None:
            # Use flexible calibrator for predictions
            return np.clip(
                self._flexible_calibrator.predict(judge_scores, folds=None), 0.0, 1.0
            )

        if self._final_calibrator is None:
            raise RuntimeError("Calibrator must be fitted before prediction")

        # Predict and clip to [0,1] to ensure rewards stay in valid range
        result = self._final_calibrator.predict(np.asarray(judge_scores))
        return np.clip(np.asarray(result), 0.0, 1.0)

    def fit_cv(
        self,
        judge_scores: np.ndarray,
        oracle_labels: Optional[np.ndarray] = None,
        oracle_mask: Optional[np.ndarray] = None,
        n_folds: int = 5,
        prompt_ids: Optional[List[str]] = None,
    ) -> CalibrationResult:
        """Fit both global and cross-fitted calibration models.

        This method:
        1. Fits a global model f_all on all oracle data (for stable rewards)
        2. Fits per-fold models f^(-k) for cross-fitted predictions (for DR)
        3. Assigns fold IDs to all samples (labeled by CV, unlabeled by hash)

        Args:
            judge_scores: Raw judge scores for all data
            oracle_labels: True labels for oracle subset
            oracle_mask: Boolean mask indicating which samples have oracle labels
            n_folds: Number of CV folds

        Returns:
            CalibrationResult with both global and CV calibration
        """
        judge_scores = np.asarray(judge_scores)
        n_total = len(judge_scores)
        self._n_folds = n_folds

        # Handle different input formats (same as fit_transform)
        if oracle_mask is not None:
            oracle_mask = np.asarray(oracle_mask, dtype=bool)
            if oracle_labels is None:
                raise ValueError("oracle_labels required when oracle_mask provided")
            oracle_labels = np.asarray(oracle_labels)
            oracle_scores = judge_scores[oracle_mask]
            oracle_y = oracle_labels
        elif oracle_labels is not None and len(oracle_labels) < n_total:
            n_oracle = len(oracle_labels)
            oracle_scores = judge_scores[:n_oracle]
            oracle_y = np.asarray(oracle_labels)
            oracle_mask = np.zeros(n_total, dtype=bool)
            oracle_mask[:n_oracle] = True
        elif oracle_labels is not None:
            oracle_labels = np.asarray(oracle_labels)
            if len(oracle_labels) != n_total:
                raise ValueError(
                    f"oracle_labels length ({len(oracle_labels)}) must match "
                    f"judge_scores length ({n_total}) or be shorter for partial labeling"
                )
            oracle_scores = judge_scores
            oracle_y = oracle_labels
            oracle_mask = np.ones(n_total, dtype=bool)
        else:
            raise ValueError(
                "oracle_labels is required for calibration. "
                "Provide oracle labels for at least a subset of samples."
            )

        n_oracle = len(oracle_y)
        if n_oracle < n_folds * 2:
            raise ValueError(
                f"Too few oracle samples ({n_oracle}) for {n_folds}-fold CV. "
                f"Need at least {n_folds * 2}."
            )

        # Step 1: Assign fold IDs to all samples first (unified approach)
        if prompt_ids is not None:
            # Use the unified fold system when prompt_ids are available
            from ..data.folds import (
                get_folds_for_prompts,
                get_folds_with_oracle_balance,
            )

            self._prompt_ids = prompt_ids
            if self.balance_oracle_folds:
                # Ensure oracle samples are evenly distributed across folds
                self._fold_ids = get_folds_with_oracle_balance(
                    prompt_ids, oracle_mask, n_folds, self.random_seed
                )
            else:
                # Simple hash-based for all samples
                self._fold_ids = get_folds_for_prompts(
                    prompt_ids, n_folds, self.random_seed
                )
        else:
            # Fallback to old system for backward compatibility
            self._fold_ids = np.zeros(n_total, dtype=int)

            # Labeled samples: assign by KFold
            oracle_indices = np.where(oracle_mask)[0]
            kf = KFold(n_splits=n_folds, shuffle=True, random_state=self.random_seed)
            for fold_id, (_, test_idx) in enumerate(kf.split(oracle_indices)):
                fold_samples = oracle_indices[test_idx]
                self._fold_ids[fold_samples] = fold_id

            # Unlabeled samples: assign deterministically by stable hash
            unlabeled_mask = ~oracle_mask
            unlabeled_indices = np.where(unlabeled_mask)[0]
            if len(unlabeled_indices) > 0:

                def _fold_for_idx(i: int, seed: int, n_folds: int) -> int:
                    """Stable hash-based fold assignment."""
                    h = hashlib.blake2b(f"{i}-{seed}".encode(), digest_size=2)
                    return int.from_bytes(h.digest(), "big") % n_folds

                for idx in unlabeled_indices:
                    self._fold_ids[idx] = _fold_for_idx(
                        int(idx), self.random_seed, n_folds
                    )

        # Extract oracle fold IDs for use in flexible calibrator
        oracle_fold_ids = self._fold_ids[oracle_mask]

        # Step 2: Fit global model
        if self.calibration_mode != "monotone":
            logger.info(f"Calibration mode: {self.calibration_mode}")

            # Use flexible calibration
            from .flexible_calibrator import FlexibleCalibrator

            # Fit flexible calibrator
            logger.info(f"Fitting FlexibleCalibrator with {n_oracle} oracle samples")
            self._flexible_calibrator = FlexibleCalibrator(
                mode=self.calibration_mode, random_seed=self.random_seed
            )
            self._flexible_calibrator.fit(oracle_scores, oracle_y, oracle_fold_ids)

            # Log selected mode if auto was used
            if self.calibration_mode == "auto":
                selected = self._flexible_calibrator.selected_mode
                self.selected_mode = selected  # Store for metadata
                logger.info(f"Auto-calibration selected: {selected}")
                if selected == "two_stage":
                    logger.info(
                        "  → Non-monotone relationship detected, using flexible calibration"
                    )
                else:
                    logger.info(
                        "  → Monotone relationship confirmed, using standard calibration"
                    )

            # Get calibrated scores using the full model (no folds for inference)
            calibrated_scores = np.clip(
                self._flexible_calibrator.predict(judge_scores, folds=None), 0.0, 1.0
            )
        else:
            logger.info("Calibration mode: monotone (standard isotonic regression)")
            # Use standard monotone calibration
            self._final_calibrator = IsotonicRegression(out_of_bounds="clip")
            self._final_calibrator.fit(oracle_scores, oracle_y)
            # Clip to [0,1] to ensure rewards stay in valid range even if oracle labels exceed bounds
            calibrated_scores = np.clip(
                self._final_calibrator.predict(judge_scores), 0.0, 1.0
            )

        # Step 3: Fit per-fold models
        if self._flexible_calibrator is not None:
            # Flexible calibrator already has per-fold models fitted
            self._fold_models = {}  # Will use flexible calibrator for predictions
        else:
            # Standard isotonic per-fold models
            self._fold_models = {}
            for fold_id in range(n_folds):
                # Get training indices (all oracle samples NOT in this fold)
                train_mask = oracle_mask & (self._fold_ids != fold_id)
                train_scores = judge_scores[train_mask]
                # Get corresponding oracle labels for training samples
                oracle_fold_mask = self._fold_ids[oracle_mask] != fold_id
                train_labels = oracle_y[oracle_fold_mask]

                if len(train_scores) > 0:
                    fold_model = IsotonicRegression(out_of_bounds="clip")
                    fold_model.fit(train_scores, train_labels)
                    self._fold_models[fold_id] = fold_model
                else:
                    # Fallback to global model if not enough data
                    self._fold_models[fold_id] = self._final_calibrator

        # Compute diagnostics with both global and OOF predictions
        oracle_calibrated = calibrated_scores[oracle_mask]
        rmse = np.sqrt(np.mean((oracle_calibrated - oracle_y) ** 2))
        coverage_01 = np.mean(np.abs(oracle_calibrated - oracle_y) <= 0.1)

        # Compute OOF diagnostics for oracle points
        oracle_oof = np.empty_like(oracle_y)
        if self._flexible_calibrator is not None:
            # Get OOF predictions from flexible calibrator
            oracle_fold_ids = self._fold_ids[oracle_mask]
            oracle_oof = np.clip(
                self._flexible_calibrator.predict(oracle_scores, oracle_fold_ids),
                0.0,
                1.0,
            )
        else:
            # Standard isotonic OOF predictions
            for fold_id, model in self._fold_models.items():
                mask = self._fold_ids[oracle_mask] == fold_id
                if np.any(mask):
                    # Clip predictions to [0,1]
                    oracle_oof[mask] = np.clip(
                        model.predict(oracle_scores[mask]), 0.0, 1.0
                    )

        rmse_oof = float(np.sqrt(np.mean((oracle_oof - oracle_y) ** 2)))
        coverage_01_oof = float(np.mean(np.abs(oracle_oof - oracle_y) <= 0.1))

        # Add information about calibration mode to log message
        mode_str = ""
        if self._flexible_calibrator is not None:
            if self.calibration_mode == "auto":
                mode_str = f" [{self._flexible_calibrator.selected_mode} via auto]"
            else:
                mode_str = f" [{self.calibration_mode}]"
        else:
            mode_str = " [monotone]"

        logger.info(
            f"CV Calibration complete{mode_str}: {n_oracle} oracle samples, {n_folds} folds, "
            f"RMSE={rmse:.3f} (OOF: {rmse_oof:.3f}), "
            f"coverage@0.1={coverage_01:.1%} (OOF: {coverage_01_oof:.1%})"
        )

        return CalibrationResult(
            calibrated_scores=calibrated_scores,
            calibration_rmse=float(rmse),
            coverage_at_01=float(coverage_01),
            n_oracle=n_oracle,
            calibrator=self,
            fold_ids=self._fold_ids,
            oof_rmse=rmse_oof,
            oof_coverage_at_01=coverage_01_oof,
        )

    def predict_all(self, judge_scores: np.ndarray) -> np.ndarray:
        """Predict using global model f_all (stable for rewards).

        Args:
            judge_scores: Judge scores to calibrate

        Returns:
            Globally calibrated scores
        """
        return self.predict(judge_scores)

    def index(
        self, judge_scores: np.ndarray, folds: Optional[np.ndarray] = None
    ) -> np.ndarray:
        """Get the index used for isotonic regression.

        This returns the appropriate index for outcome modeling:
        - For monotone calibration: returns judge_scores unchanged
        - For two-stage calibration: returns the transformed index

        Args:
            judge_scores: Judge scores to transform
            folds: Optional fold assignments for OOF transformation

        Returns:
            Index values suitable for isotonic regression
        """
        if self._flexible_calibrator is not None:
            return self._flexible_calibrator.index(judge_scores, folds)
        else:
            # Standard monotone calibration uses judge scores directly
            return judge_scores

    def predict_oof(self, judge_scores: np.ndarray, fold_ids: np.ndarray) -> np.ndarray:
        """Out-of-fold predictions using cross-fitted models.

        Args:
            judge_scores: Judge scores to calibrate
            fold_ids: Fold assignment for each score

        Returns:
            Cross-fitted calibrated scores
        """
        if self._flexible_calibrator is not None:
            # Use flexible calibrator for OOF predictions
            return np.clip(
                self._flexible_calibrator.predict(judge_scores, fold_ids), 0.0, 1.0
            )

        if not self._fold_models:
            raise RuntimeError("Must call fit_cv before predict_oof")

        judge_scores = np.asarray(judge_scores)
        fold_ids = np.asarray(fold_ids)

        predictions = np.zeros_like(judge_scores)
        for fold_id, model in self._fold_models.items():
            fold_mask = fold_ids == fold_id
            if np.any(fold_mask):
                # Clip predictions to [0,1] to ensure valid rewards
                predictions[fold_mask] = np.clip(
                    model.predict(judge_scores[fold_mask]), 0.0, 1.0
                )

        return np.clip(predictions, 0.0, 1.0)  # Extra safety clip


def calibrate_judge_scores(
    judge_scores: np.ndarray,
    oracle_labels: np.ndarray,
    oracle_mask: Optional[np.ndarray] = None,
) -> Tuple[np.ndarray, Dict[str, float]]:
    """Convenience function for judge calibration.

    Args:
        judge_scores: Raw judge scores for all data
        oracle_labels: True labels for oracle subset
        oracle_mask: Optional boolean mask for oracle samples

    Returns:
        Tuple of (calibrated_scores, diagnostics_dict)

    Example:
        # Calibrate judge scores with 25% oracle labels
        cal_scores, stats = calibrate_judge_scores(
            judge_scores=all_judge_scores,
            oracle_labels=oracle_subset_labels[:1000]  # First 1000 have labels
        )

        print(f"Calibration RMSE: {stats['rmse']:.3f}")
        print(f"Coverage: {stats['coverage']:.1%}")
    """
    calibrator = JudgeCalibrator()
    result = calibrator.fit_transform(judge_scores, oracle_labels, oracle_mask)

    diagnostics = {
        "rmse": result.calibration_rmse,
        "coverage": result.coverage_at_01,
        "n_oracle": result.n_oracle,
    }

    return result.calibrated_scores, diagnostics


=== ./cje/calibration/oracle_slice.py ===

"""Oracle slice augmentation for honest confidence intervals.

This module implements the augmentation term that accounts for the uncertainty
in learning the judge→oracle calibration map f̂(S) from a finite oracle slice.
The augmentation restores first-order unbiasedness and provides honest CIs
that properly widen when the oracle slice is small.

Theory:
    The base estimator uses proxy outcome f̂(S) everywhere. The true target is E[W*Y].
    The gap is E[W*(Y - f̂(S))]. On the oracle slice, we can estimate this gap using:

    AUG = (L/p) * m(S) * (Y - f̂(S))

    where:
    - L ∈ {0,1} indicates oracle label presence
    - p = E[L] is the labeling probability (MCAR assumption)
    - m(S) = E[W|S] is the conditional mean of weights given score
    - Y is the true oracle label
    - f̂(S) is the calibrated proxy from judge scores

    This augmentation is unbiased for the gap and has controlled variance.
"""

from dataclasses import dataclass
from typing import Optional, Tuple, Dict, Any, List
import numpy as np
from sklearn.isotonic import IsotonicRegression
import logging

logger = logging.getLogger(__name__)


@dataclass
class OracleSliceConfig:
    """Configuration for oracle slice augmentation.

    Args:
        enable_augmentation: Whether to add the augmentation term
        enable_cross_fit: Whether to cross-fit m̂(S) and π̂(S)
        min_pi: Minimum value for π̂(S) to avoid division issues
        use_mar: Whether to model MAR labeling (vs assuming MCAR)
    """

    enable_augmentation: bool = True
    enable_cross_fit: bool = True
    min_pi: float = 0.01
    use_mar: bool = False  # Start with MCAR, add MAR in phase 2


class OracleSliceAugmentation:
    """Computes augmentation terms for oracle slice uncertainty.

    This handles:
    1. Estimating m̂(S) = E[W|S] via isotonic regression
    2. Computing augmentation: (L/p) * m̂(S) * (Y - f̂(S))
    3. Tracking diagnostics about the augmentation

    The augmentation corrects for the bias from using f̂(S) instead of Y,
    while the influence function properly accounts for the added variance.
    """

    def __init__(self, config: Optional[OracleSliceConfig] = None):
        """Initialize oracle slice augmentation.

        Args:
            config: Configuration object (uses defaults if None)
        """
        self.config = config or OracleSliceConfig()
        self._m_hat_cache: Dict[str, np.ndarray] = {}
        self._m_hat_models: Dict[str, Any] = {}
        self._diagnostics: Dict[str, Dict] = {}

    def fit_m_hat(
        self,
        weights: np.ndarray,
        judge_scores: np.ndarray,
        policy: str,
        cv_folds: Optional[np.ndarray] = None,
    ) -> np.ndarray:
        """Estimate m̂(S) = E[W|S] via isotonic regression.

        Args:
            weights: The actual calibrated weights used by the estimator
            judge_scores: Judge scores S
            policy: Target policy name
            cv_folds: Optional fold assignments for cross-fitting

        Returns:
            m_hat: Estimated E[W|S], normalized to mean 1
        """
        if not self.config.enable_augmentation:
            return np.ones_like(weights)

        # Handle edge cases
        if len(weights) == 0 or len(judge_scores) == 0:
            logger.warning(f"Empty weights or scores for policy {policy}")
            return np.ones_like(weights)

        # Cross-fitted version if requested and folds available
        if self.config.enable_cross_fit and cv_folds is not None:
            m_hat = self._fit_m_hat_cross_fitted(weights, judge_scores, cv_folds)

            # Also fit global model for unlabeled rows
            iso_global = IsotonicRegression(out_of_bounds="clip")
            iso_global.fit(judge_scores, weights)
            self._m_hat_models[policy] = iso_global

            # Fill any missing values (unlabeled rows) with global fit
            unlabeled = (cv_folds < 0) | np.isnan(cv_folds)
            if np.any(unlabeled):
                m_hat[unlabeled] = iso_global.predict(judge_scores[unlabeled])

        else:
            # Global fit only (simpler, used for IPS)
            iso = IsotonicRegression(out_of_bounds="clip")
            iso.fit(judge_scores, weights)
            m_hat = iso.predict(judge_scores)
            self._m_hat_models[policy] = iso

        # Normalize to mean 1 (preserves unbiasedness)
        if m_hat.mean() > 0:
            m_hat = m_hat / m_hat.mean()
        else:
            logger.warning(f"m_hat has zero mean for policy {policy}")
            m_hat = np.ones_like(m_hat)

        # Cache for later use
        self._m_hat_cache[policy] = m_hat

        logger.debug(
            f"Fitted m̂(S) for policy {policy}: "
            f"mean={m_hat.mean():.3f}, std={m_hat.std():.3f}, "
            f"range=[{m_hat.min():.3f}, {m_hat.max():.3f}]"
        )

        return m_hat

    def _fit_m_hat_cross_fitted(
        self, weights: np.ndarray, judge_scores: np.ndarray, cv_folds: np.ndarray
    ) -> np.ndarray:
        """Fit m̂(S) using cross-fitting to avoid overfitting.

        Args:
            weights: Calibrated weights
            judge_scores: Judge scores
            cv_folds: Fold assignments (-1 for unlabeled)

        Returns:
            m_hat: Out-of-fold predictions
        """
        m_hat = np.zeros_like(weights)
        unique_folds = np.unique(cv_folds[cv_folds >= 0])

        if len(unique_folds) < 2:
            # Not enough folds for cross-fitting, use global
            iso = IsotonicRegression(out_of_bounds="clip")
            iso.fit(judge_scores, weights)
            return iso.predict(judge_scores)

        for fold in unique_folds:
            train_mask = (cv_folds >= 0) & (cv_folds != fold)
            test_mask = cv_folds == fold

            if train_mask.sum() > 0 and test_mask.sum() > 0:
                iso = IsotonicRegression(out_of_bounds="clip")
                iso.fit(judge_scores[train_mask], weights[train_mask])
                m_hat[test_mask] = iso.predict(judge_scores[test_mask])

        return m_hat

    def compute_augmentation(
        self,
        policy: str,
        rewards: np.ndarray,  # f̂(S) - calibrated proxy outcomes
        data: List[Dict[str, Any]],
        dataset_samples: Optional[List[Any]] = None,
        oracle_field: str = "oracle_label",
    ) -> Tuple[np.ndarray, Dict[str, float]]:
        """Compute augmentation vector and diagnostics.

        Args:
            policy: Target policy name
            rewards: Calibrated rewards f̂(S) for each sample
            data: List of data dictionaries with prompt_id
            dataset_samples: Original dataset samples with metadata
            oracle_field: Field name for oracle labels in metadata

        Returns:
            aug_vector: Per-sample augmentation values
            diagnostics: Dictionary with p_oracle, n_oracle, variance info
        """
        if not self.config.enable_augmentation:
            return np.zeros_like(rewards), {}

        if policy not in self._m_hat_cache:
            logger.warning(f"No m̂(S) fitted for policy {policy}, skipping augmentation")
            return np.zeros_like(rewards), {}

        m_hat = self._m_hat_cache[policy]

        # Extract oracle labels and mask
        oracle_labels, oracle_mask = self._extract_oracle_info(
            data, dataset_samples, oracle_field
        )

        # Check if we have any oracle labels
        p = oracle_mask.mean() if len(oracle_mask) > 0 else 0.0
        n_oracle = int(oracle_mask.sum())

        if p <= 0 or n_oracle == 0:
            logger.debug(f"No oracle labels for policy {policy}")
            return np.zeros_like(rewards), {
                "p_oracle": 0.0,
                "n_oracle": 0,
                "aug_mean": 0.0,
                "aug_var": 0.0,
                "slice_variance_share": 0.0,
            }

        # Compute augmentation: (L/p) * m̂(S) * (Y - f̂(S))
        # Note: For MCAR, π(S) = p (constant)
        residuals = np.where(oracle_mask, oracle_labels - rewards, 0.0)
        aug = (oracle_mask / p) * m_hat * residuals

        # Compute diagnostics
        aug_mean = float(aug.mean())
        aug_var = float(np.var(aug, ddof=1)) if len(aug) > 1 else 0.0

        # Store diagnostics
        diagnostics = {
            "p_oracle": float(p),
            "n_oracle": n_oracle,
            "aug_mean": aug_mean,
            "aug_var": aug_var,
            "slice_variance_share": 0.0,  # Will be computed by estimator
        }

        self._diagnostics[policy] = diagnostics

        logger.debug(
            f"Augmentation for {policy}: mean={aug_mean:.4f}, "
            f"var={aug_var:.4f}, p={p:.3f}, n_oracle={n_oracle}"
        )

        return aug, diagnostics

    def _extract_oracle_info(
        self,
        data: List[Dict[str, Any]],
        dataset_samples: Optional[List[Any]],
        oracle_field: str,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Extract oracle labels and mask from dataset.

        Args:
            data: List of data dictionaries with prompt_id
            dataset_samples: Original dataset samples
            oracle_field: Field name for oracle labels

        Returns:
            oracle_labels: Array of oracle values (0 where missing)
            oracle_mask: Boolean array indicating oracle presence
        """
        n = len(data)
        oracle_labels = np.zeros(n)
        oracle_mask = np.zeros(n, dtype=bool)

        if dataset_samples is None:
            # Try to extract from data directly
            for i, d in enumerate(data):
                if "metadata" in d and oracle_field in d["metadata"]:
                    val = d["metadata"][oracle_field]
                    if val is not None:
                        oracle_labels[i] = float(val)
                        oracle_mask[i] = True
        else:
            # Build mapping from prompt_id to metadata
            metadata_map = {}
            for sample in dataset_samples:
                if hasattr(sample, "prompt_id") and hasattr(sample, "metadata"):
                    pid = str(sample.prompt_id)
                    metadata_map[pid] = sample.metadata

            # Extract oracle labels for each data point
            for i, d in enumerate(data):
                prompt_id = str(d.get("prompt_id", ""))
                if prompt_id in metadata_map:
                    metadata = metadata_map[prompt_id]
                    if oracle_field in metadata and metadata[oracle_field] is not None:
                        oracle_labels[i] = float(metadata[oracle_field])
                        oracle_mask[i] = True

        return oracle_labels, oracle_mask

    def get_diagnostics(self, policy: Optional[str] = None) -> Dict[str, Any]:
        """Get augmentation diagnostics.

        Args:
            policy: Specific policy to get diagnostics for (None = all)

        Returns:
            Dictionary of diagnostics
        """
        if policy is not None:
            return self._diagnostics.get(policy, {})
        return self._diagnostics.copy()


=== ./cje/calibration/simcal.py ===

"""Stacked Score-Indexed Monotone Calibration (SIMCal) for importance weights.

This module implements stacked SIMCal, which combines {increasing, decreasing}
isotonic candidates via convex optimization to minimize out-of-fold (OOF)
influence function variance.

The stacking approach:
1. Builds candidate weight vectors (isotonic increasing/decreasing, optionally baseline)
2. Computes OOF influence functions for each candidate
3. Solves a quadratic program on the simplex to find optimal mixture
4. Applies uniform blending to satisfy ESS/variance constraints
"""

from dataclasses import dataclass
from typing import Tuple, Dict, Any, Optional, List
import numpy as np
from sklearn.isotonic import IsotonicRegression
from sklearn.model_selection import KFold
import warnings


@dataclass
class SimcalConfig:
    """Configuration for stacked SIMCal calibration.

    Stacked SIMCal combines multiple candidate weight vectors (baseline,
    increasing, decreasing) to minimize OOF influence function variance,
    then applies uniform blending to meet ESS/variance constraints.

    Args:
        ess_floor: Minimum ESS as fraction of n (e.g., 0.2 => ESS >= 0.2 * n)
        var_cap: Maximum allowed variance of calibrated weights
        epsilon: Small constant for numerical stability
        include_baseline: Whether to include raw weights in the stack (default True)
        ridge_lambda: Ridge regularization for covariance matrix (default 1e-8)
        n_folds: Number of folds for OOF if fold_ids not provided (default 5)
        baseline_shrink: Shrinkage toward baseline for stability (default 0.05)
    """

    ess_floor: Optional[float] = 0.2
    var_cap: Optional[float] = None
    epsilon: float = 1e-9
    include_baseline: bool = False  # Default OFF - isotonic usually sufficient
    ridge_lambda: float = 1e-8
    n_folds: int = 5
    baseline_shrink: float = 0.0

    def __post_init__(self) -> None:
        if self.ess_floor is not None and not (0 < self.ess_floor <= 1):
            raise ValueError(f"ess_floor must be in (0, 1], got {self.ess_floor}")
        if self.var_cap is not None and self.var_cap <= 0:
            raise ValueError(f"var_cap must be positive, got {self.var_cap}")
        if self.baseline_shrink < 0 or self.baseline_shrink > 1:
            raise ValueError(
                f"baseline_shrink must be in [0, 1], got {self.baseline_shrink}"
            )

        # Validate consistency between ess_floor and var_cap
        if self.ess_floor is not None and self.var_cap is not None:
            # ESS = n/(1 + Var) implies Var <= 1/ess_floor - 1
            implied_var_cap = (1.0 / self.ess_floor) - 1.0
            if self.var_cap > implied_var_cap:
                warnings.warn(
                    f"var_cap={self.var_cap:.3f} is looser than ESS-implied cap "
                    f"{implied_var_cap:.3f} from ess_floor={self.ess_floor}. "
                    f"The ESS constraint will dominate.",
                    UserWarning,
                )


class SIMCalibrator:
    """Stacked Score-Indexed Monotone Calibrator.

    Combines {increasing, decreasing} candidates (optionally baseline) to minimize
    OOF influence function variance, then applies uniform blending to
    meet ESS/variance constraints.
    """

    def __init__(self, config: SimcalConfig):
        """Initialize SIMCalibrator with configuration.

        Args:
            config: SimcalConfig with calibration parameters
        """
        self.cfg = config

    @staticmethod
    def implied_var_cap(ess_floor: float) -> float:
        """Compute the implied variance cap from an ESS floor constraint.

        Since ESS = n/(1 + Var), requiring ESS >= ess_floor * n
        implies Var <= 1/ess_floor - 1.

        Args:
            ess_floor: Minimum ESS as fraction of n (must be in (0, 1])

        Returns:
            Maximum allowed variance to satisfy the ESS constraint
        """
        if not (0 < ess_floor <= 1):
            raise ValueError(f"ess_floor must be in (0, 1], got {ess_floor}")
        return (1.0 / ess_floor) - 1.0

    def transform(
        self,
        w: np.ndarray,
        s: np.ndarray,
        *,
        rewards: Optional[np.ndarray] = None,
        residuals: Optional[np.ndarray] = None,
        fold_ids: Optional[np.ndarray] = None,
    ) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Calibrate weights using stacked score-indexed monotone projection.

        Algorithm:
        1. Build candidate weight vectors: {increasing, decreasing, baseline?}
        2. Compute OOF influence functions for each candidate
        3. Solve quadratic program to find optimal mixture on simplex
        4. Apply single γ-blend toward uniform for constraints
        5. Optional: Apply baseline shrinkage for stability

        Args:
            w: Raw importance weights (must be positive, will be normalized to mean 1)
            s: Score index (e.g., judge scores) for ordering
            rewards: Rewards for IPS influence functions (optional, uses weights only if None)
            residuals: DR residuals (R - g_oof(S)) for DR influence functions
            fold_ids: Pre-assigned fold IDs for OOF computation (optional)

        Returns:
            Tuple of (calibrated_weights, info_dict) where info_dict contains:
                - mixture_weights: Optimal convex combination weights
                - candidates: Names of candidate weight vectors
                - gamma: Uniform blending parameter for constraints
                - var_before: Variance of input weights
                - var_after: Final variance after all adjustments
                - ess_before: ESS of input weights
                - ess_after: Final ESS after all adjustments
                - oof_variance_reduction: Ratio of stacked to best single candidate

        Raises:
            ValueError: If weights contain non-positive, NaN, or infinite values
        """
        # Input validation
        w = np.asarray(w, dtype=float)
        s = np.asarray(s, dtype=float)

        if len(w) != len(s):
            raise ValueError(f"Length mismatch: weights={len(w)}, scores={len(s)}")

        if not np.all(np.isfinite(w)) or not np.all(np.isfinite(s)):
            raise ValueError("SIMCal: NaNs or infinities in inputs")

        if np.any(w <= 0):
            raise ValueError("SIMCal: weights must be positive")

        # Ensure mean-one normalization
        w = w / w.mean()
        n = len(w)

        # Build candidate weight vectors
        candidates = []
        candidate_names = []

        # 1. Baseline (raw weights)
        if self.cfg.include_baseline:
            candidates.append(w.copy())
            candidate_names.append("baseline")

        # 2. Isotonic increasing
        iso_inc = IsotonicRegression(increasing=True, out_of_bounds="clip")
        w_inc = iso_inc.fit(s, w).predict(s)
        w_inc = np.maximum(w_inc, self.cfg.epsilon)
        w_inc = w_inc / w_inc.mean()
        candidates.append(w_inc)
        candidate_names.append("increasing")

        # 3. Isotonic decreasing
        iso_dec = IsotonicRegression(increasing=False, out_of_bounds="clip")
        w_dec = iso_dec.fit(s, w).predict(s)
        w_dec = np.maximum(w_dec, self.cfg.epsilon)
        w_dec = w_dec / w_dec.mean()
        candidates.append(w_dec)
        candidate_names.append("decreasing")

        K = len(candidates)

        # Determine what to use for influence functions
        if residuals is not None:
            # DR influence functions: w * (R - g_oof(S))
            if_targets = residuals
            if_type = "dr"
        elif rewards is not None:
            # IPS influence functions: w * R
            if_targets = rewards
            if_type = "ips"
        else:
            # Weight-only influence functions: w itself
            if_targets = np.ones(n)  # Makes IF = w - 1
            if_type = "weight"

        # Compute OOF influence functions
        if fold_ids is not None:
            # Use provided fold assignments
            unique_folds = np.unique(fold_ids)
            n_folds = len(unique_folds)
        else:
            # Generate fold assignments
            kf = KFold(n_splits=self.cfg.n_folds, shuffle=True, random_state=42)
            fold_ids = np.zeros(n, dtype=int)
            for fold_idx, (_, test_idx) in enumerate(kf.split(np.arange(n))):
                fold_ids[test_idx] = fold_idx
            n_folds = self.cfg.n_folds

        # Compute OOF influence matrix (n x K)
        IF_matrix = np.zeros((n, K))

        for k, w_cand in enumerate(candidates):
            # For each fold, compute influence relative to training mean
            for fold_id in range(n_folds):
                test_mask = fold_ids == fold_id
                train_mask = ~test_mask

                if np.sum(train_mask) == 0:
                    continue

                # Compute mean on training folds
                train_mean = np.mean(w_cand[train_mask] * if_targets[train_mask])

                # OOF influence for test fold
                IF_matrix[test_mask, k] = (
                    w_cand[test_mask] * if_targets[test_mask] - train_mean
                )

        # Compute empirical covariance matrix
        Sigma = np.cov(IF_matrix.T)  # K x K

        # Add ridge regularization for stability
        if self.cfg.ridge_lambda > 0:
            reg_amount = self.cfg.ridge_lambda * np.trace(Sigma) / K
            Sigma = Sigma + reg_amount * np.eye(K)

        # Solve quadratic program on simplex: min_π π^T Σ π s.t. π ≥ 0, 1^T π = 1
        mixture_weights = self._solve_simplex_qp(Sigma)

        # Compute stacked weights
        w_stacked = np.zeros(n)
        for k, pi_k in enumerate(mixture_weights):
            w_stacked += pi_k * candidates[k]

        # Apply constraints via uniform blending
        w_final, gamma = self._apply_constraints(w_stacked)

        # Optional: Apply baseline shrinkage for stability
        if self.cfg.baseline_shrink > 0:
            w_final = (
                1 - self.cfg.baseline_shrink
            ) * w_final + self.cfg.baseline_shrink * w
            w_final = w_final / w_final.mean()

        # Compute diagnostics
        var_before = float(np.var(w))
        var_after = float(np.var(w_final))
        ess_before = n / (1 + var_before)
        ess_after = n / (1 + var_after)

        # Compute variance reduction vs best single candidate
        single_variances = [np.var(IF_matrix[:, k]) for k in range(K)]
        best_single_var = min(single_variances)
        stacked_var = np.var(IF_matrix @ mixture_weights)
        variance_reduction = (
            stacked_var / best_single_var if best_single_var > 0 else 1.0
        )

        info = {
            "mixture_weights": mixture_weights.tolist(),
            "candidates": candidate_names,
            "gamma": gamma,
            "var_before": var_before,
            "var_after": var_after,
            "ess_before": ess_before,
            "ess_after": ess_after,
            "oof_variance_reduction": float(variance_reduction),
            "if_type": if_type,
            "n_folds": n_folds,
            "baseline_shrink": self.cfg.baseline_shrink,
        }

        return w_final, info

    def _solve_simplex_qp(self, Sigma: np.ndarray) -> np.ndarray:
        """Solve quadratic program on simplex using active set method.

        Minimize π^T Σ π subject to π ≥ 0, 1^T π = 1

        Args:
            Sigma: K x K positive semi-definite covariance matrix

        Returns:
            Optimal mixture weights on simplex
        """
        K = Sigma.shape[0]

        # Start with uniform weights
        active_set = set(range(K))

        max_iterations = 20
        for _ in range(max_iterations):
            # Solve on current active set
            if len(active_set) == 0:
                # Degenerate case - return uniform
                return np.ones(K) / K

            # Build reduced system
            active_idx = sorted(active_set)
            Sigma_active = Sigma[np.ix_(active_idx, active_idx)]

            # Solve equality-constrained QP: min π^T Σ π s.t. 1^T π = 1
            # Solution: π = Σ^{-1} 1 / (1^T Σ^{-1} 1)
            try:
                ones = np.ones(len(active_idx))
                Sigma_inv_ones = np.linalg.solve(Sigma_active, ones)
                denom = np.dot(ones, Sigma_inv_ones)

                if abs(denom) < 1e-10:
                    # Near-singular - use uniform on active set
                    pi_active = ones / len(active_idx)
                else:
                    pi_active = Sigma_inv_ones / denom
            except np.linalg.LinAlgError:
                # Singular - use uniform on active set
                pi_active = np.ones(len(active_idx)) / len(active_idx)

            # Check for negative weights
            if np.all(pi_active >= -1e-10):
                # Feasible - construct full solution
                pi_full = np.zeros(K)
                for i, idx in enumerate(active_idx):
                    pi_full[idx] = max(0, pi_active[i])

                # Renormalize to ensure exact sum to 1
                pi_full = pi_full / pi_full.sum()
                return pi_full

            # Remove most negative from active set
            min_idx = np.argmin(pi_active)
            active_set.remove(active_idx[min_idx])

        # Fallback to uniform if no convergence
        return np.ones(K) / K

    def _apply_constraints(self, w: np.ndarray) -> Tuple[np.ndarray, float]:
        """Apply ESS/variance constraints via uniform blending.

        Args:
            w: Mean-one weight vector

        Returns:
            Tuple of (constrained_weights, gamma) where gamma is the blending parameter
        """
        n = len(w)
        var_w = np.var(w)
        gamma = 0.0

        if var_w > 0:
            # Check ESS constraint
            if self.cfg.ess_floor is not None:
                var_max_ess = (1.0 / self.cfg.ess_floor) - 1.0
                if var_w > var_max_ess:
                    gamma = max(gamma, 1.0 - np.sqrt(var_max_ess / var_w))

            # Check variance cap
            if self.cfg.var_cap is not None and var_w > self.cfg.var_cap:
                gamma = max(gamma, 1.0 - np.sqrt(self.cfg.var_cap / var_w))

        gamma = float(np.clip(gamma, 0.0, 1.0))

        # Apply blending: w ← 1 + (1-γ)(w-1)
        w_constrained = 1.0 + (1.0 - gamma) * (w - 1.0)
        w_constrained = np.maximum(w_constrained, self.cfg.epsilon)
        w_constrained = w_constrained / w_constrained.mean()

        return w_constrained, gamma


=== ./cje/cfbits/__init__.py ===

"""CF-bits: Information accounting for causal inference.

CF-bits provides an information-accounting layer for CJE that decomposes
uncertainty into identification width (structural limits) and sampling width
(statistical noise).
"""

from .core import CFBits, compute_cfbits, GatesDecision, apply_gates
from .sampling import (
    EfficiencyStats,
    SamplingVariance,
    compute_ifr_aess,
    compute_sampling_width,
    compute_estimator_eif,
)
from .overlap import OverlapFloors, estimate_overlap_floors
from .identification import compute_identification_width
from .playbooks import cfbits_report_fresh_draws, cfbits_report_logging_only

__all__ = [
    # Core
    "CFBits",
    "compute_cfbits",
    "GatesDecision",
    "apply_gates",
    # Sampling
    "EfficiencyStats",
    "SamplingVariance",
    "compute_ifr_aess",
    "compute_sampling_width",
    "compute_estimator_eif",
    # Overlap
    "OverlapFloors",
    "estimate_overlap_floors",
    # Identification
    "compute_identification_width",
    # Playbooks
    "cfbits_report_fresh_draws",
    "cfbits_report_logging_only",
]


=== ./cje/cfbits/core.py ===

"""Core CF-bits computation and gating logic.

CF-bits measure information gain as the log-ratio of baseline to actual width:
bits = log₂(W₀ / W)

Each halving of width adds 1 bit of information.
"""

from dataclasses import dataclass
from typing import Optional, List, Dict, Any, Literal
import numpy as np
import logging

logger = logging.getLogger(__name__)


@dataclass
class CFBits:
    """CF-bits metrics and width decomposition.

    Attributes:
        bits_tot: Total bits of information
        bits_id: Bits from identification (structural)
        bits_var: Bits from sampling efficiency
        w0: Baseline width
        w_id: Identification width
        w_var: Sampling width
        w_tot: Total width (w_id + w_var)
        w_max: Maximum of w_id and w_var
    """

    bits_tot: float
    bits_id: Optional[float]
    bits_var: Optional[float]
    w0: float
    w_id: float
    w_var: float
    w_tot: float
    w_max: float

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "bits_tot": self.bits_tot,
            "bits_id": self.bits_id,
            "bits_var": self.bits_var,
            "w0": self.w0,
            "w_id": self.w_id,
            "w_var": self.w_var,
            "w_tot": self.w_tot,
            "w_max": self.w_max,
            "dominant": "identification" if self.w_id > self.w_var else "sampling",
        }

    def summary(self) -> str:
        """Human-readable summary."""
        dominant = "identification" if self.w_id > self.w_var else "sampling"
        return (
            f"CF-bits: {self.bits_tot:.2f} bits total "
            f"(W: {self.w_tot:.3f} from {self.w0:.1f} baseline). "
            f"Decomposition: Wid={self.w_id:.3f}, Wvar={self.w_var:.3f}. "
            f"Dominant: {dominant}."
        )


@dataclass
class GatesDecision:
    """Reliability gating decision based on diagnostic thresholds.

    Attributes:
        state: Overall reliability state
        reasons: List of specific issues or confirmations
        suggestions: Recommended actions to improve reliability
    """

    state: Literal["GOOD", "WARNING", "CRITICAL", "REFUSE"]
    reasons: List[str]
    suggestions: Dict[str, Any]

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "state": self.state,
            "reasons": self.reasons,
            "suggestions": self.suggestions,
        }

    def summary(self) -> str:
        """Human-readable summary."""
        emoji = {"GOOD": "✅", "WARNING": "⚠️", "CRITICAL": "❌", "REFUSE": "🚫"}
        return (
            f"{emoji.get(self.state, '')} {self.state}: {', '.join(self.reasons[:2])}"
        )


def bits_from_width(w0: float, w: float, epsilon: float = 1e-10) -> float:
    """Compute bits of information from width reduction.

    bits = log₂(w0 / w)

    Args:
        w0: Baseline width
        w: Actual width
        epsilon: Small constant for numerical stability

    Returns:
        Bits of information (can be negative if w > w0)
    """
    if w0 <= epsilon:
        logger.warning(f"Baseline width {w0} too small")
        return 0.0

    if w <= epsilon:
        logger.warning(f"Width {w} too small, capping bits")
        return 10.0  # Cap at 10 bits (1024x reduction)

    return float(np.log2(w0 / w))


def compute_cfbits(
    w0: float,
    wid: float,
    wvar: float,
    ifr_main: Optional[float] = None,
    ifr_oua: Optional[float] = None,
) -> CFBits:
    """Compute CF-bits metrics from width components.

    Args:
        w0: Baseline width (typically 1.0 for KPI ∈ [0,1])
        wid: Identification width (structural uncertainty)
        wvar: Sampling width (statistical uncertainty)
        ifr_main: Information Fraction Ratio without OUA
        ifr_oua: Information Fraction Ratio with OUA (preferred)

    Returns:
        CFBits object with all metrics
    """
    # Total width (interval Minkowski sum under independence)
    w_tot = wid + wvar

    # Maximum width (identifies bottleneck)
    w_max = max(wid, wvar)

    # Total bits
    bits_tot = bits_from_width(w0, w_tot)

    # Identification bits (if meaningful)
    bits_id = bits_from_width(w0, wid) if wid > 0 else None

    # Variance bits (from IFR_OUA if available, else IFR_main)
    # bits_var = 0.5 * log₂(IFR) since width scales as √variance
    bits_var = None
    if ifr_oua is not None and ifr_oua > 0:
        # Prefer IFR_OUA as it accounts for oracle uncertainty
        bits_var = 0.5 * float(np.log2(ifr_oua))
    elif ifr_main is not None and ifr_main > 0:
        # Fall back to IFR_main if OUA not available
        bits_var = 0.5 * float(np.log2(ifr_main))

    return CFBits(
        bits_tot=bits_tot,
        bits_id=bits_id,
        bits_var=bits_var,
        w0=w0,
        w_id=wid,
        w_var=wvar,
        w_tot=w_tot,
        w_max=w_max,
    )


def apply_gates(
    aessf: Optional[float] = None,
    aessf_lcb: Optional[float] = None,
    ifr: Optional[float] = None,
    ifr_lcb: Optional[float] = None,
    tail_index: Optional[float] = None,
    var_oracle_ratio: Optional[float] = None,
    thresholds: Optional[Dict[str, float]] = None,
) -> GatesDecision:
    """Apply reliability gates based on diagnostic metrics.

    Uses lower confidence bounds (LCBs) when available for conservative gating.

    Default thresholds (can be overridden):
    - A-ESSF < 0.05: REFUSE (catastrophic overlap)
    - A-ESSF < 0.20: CRITICAL (poor overlap, need DR)
    - IFR < 0.2: CRITICAL (very inefficient)
    - IFR < 0.5: WARNING (inefficient)
    - Tail index < 2.0: CRITICAL (infinite variance risk)
    - Tail index < 2.5: WARNING (heavy tails)
    - Oracle variance ratio > 1.0: WARNING (OUA dominates)

    Args:
        aessf: Adjusted ESS Fraction on σ(S) (point estimate)
        aessf_lcb: Lower confidence bound for A-ESSF (preferred)
        ifr: Information Fraction Ratio (point estimate)
        ifr_lcb: Lower confidence bound for IFR (preferred)
        tail_index: Hill tail index
        var_oracle_ratio: Ratio of oracle variance to main variance
        thresholds: Optional custom thresholds

    Returns:
        GatesDecision with state, reasons, and suggestions
    """
    # Default thresholds
    default_thresholds = {
        "aessf_refuse": 0.05,
        "aessf_critical": 0.20,
        "ifr_critical": 0.2,
        "ifr_warning": 0.5,
        "tail_critical": 2.0,
        "tail_warning": 2.5,
        "oracle_warning": 1.0,
    }

    if thresholds:
        default_thresholds.update(thresholds)
    thresholds = default_thresholds

    # Collect issues
    reasons = []
    suggestions = {}
    state: Literal["GOOD", "WARNING", "CRITICAL", "REFUSE"] = "GOOD"

    # Check overlap (A-ESSF) - use LCB if available
    aessf_check = aessf_lcb if aessf_lcb is not None else aessf
    if aessf_check is not None:
        lcb_note = " (LCB)" if aessf_lcb is not None else ""
        if aessf_check < thresholds["aessf_refuse"]:
            state = "REFUSE"
            reasons.append(f"Catastrophic overlap (A-ESSF{lcb_note}={aessf_check:.1%})")
            suggestions["change_policy"] = "Use policies with better overlap"
        elif aessf_check < thresholds["aessf_critical"]:
            if state != "REFUSE":
                state = "CRITICAL"
            reasons.append(f"Poor overlap (A-ESSF{lcb_note}={aessf_check:.1%})")
            suggestions["use_dr"] = "Use DR methods with fresh draws"
            suggestions["fresh_draws"] = "100"  # Suggested number as string

    # Check efficiency (IFR) - use LCB if available
    ifr_check = ifr_lcb if ifr_lcb is not None else ifr
    if ifr_check is not None:
        lcb_note = " (LCB)" if ifr_lcb is not None else ""
        if ifr_check < thresholds["ifr_critical"]:
            if state not in ["REFUSE", "CRITICAL"]:
                state = "CRITICAL"
            reasons.append(f"Very inefficient (IFR{lcb_note}={ifr_check:.1%})")
            suggestions["improve_estimator"] = "Use more efficient estimator"
        elif ifr_check < thresholds["ifr_warning"]:
            if state == "GOOD":
                state = "WARNING"
            reasons.append(f"Inefficient (IFR{lcb_note}={ifr_check:.1%})")
            suggestions["consider_dr"] = "Consider DR methods"

    # Check tail safety
    if tail_index is not None:
        if tail_index < thresholds["tail_critical"]:
            if state not in ["REFUSE", "CRITICAL"]:
                state = "CRITICAL"
            reasons.append(f"Infinite variance risk (tail={tail_index:.1f})")
            suggestions["robust_estimator"] = "Use robust estimator or trim weights"
        elif tail_index < thresholds["tail_warning"]:
            if state == "GOOD":
                state = "WARNING"
            reasons.append(f"Heavy tails (tail={tail_index:.1f})")
            suggestions["monitor_tails"] = "Monitor tail behavior"

    # Check oracle dominance
    if var_oracle_ratio is not None:
        if var_oracle_ratio > thresholds["oracle_warning"]:
            if state == "GOOD":
                state = "WARNING"
            reasons.append(
                f"Oracle uncertainty dominates (ratio={var_oracle_ratio:.1f})"
            )
            suggestions["add_labels"] = "Add more oracle labels"

    # If no issues found
    if not reasons:
        reasons.append("All diagnostics within acceptable ranges")

    return GatesDecision(
        state=state,
        reasons=reasons,
        suggestions=suggestions,
    )


=== ./cje/cfbits/identification.py ===

"""Identification width computation for CF-bits.

This module computes the identification width (Wid) which represents
structural uncertainty from limited overlap and calibration.

Phase 2 implementation - placeholder for now.
"""

from typing import Optional, Tuple, Dict, Any, TYPE_CHECKING
import numpy as np
import logging

if TYPE_CHECKING:
    from ..estimators.base_estimator import BaseCJEEstimator

logger = logging.getLogger(__name__)


def compute_identification_width(
    estimator: "BaseCJEEstimator",
    policy: str,
    alpha: float = 0.05,
    method: str = "isotonic_bands",
) -> Tuple[float, Dict[str, Any]]:
    """Compute identification width (Wid) for uncertainty.

    Identification width captures structural uncertainty from:
    - Limited overlap (propensity score bounds)
    - Calibration uncertainty (isotonic regression bands)

    Phase 2 implementation - returns placeholder for now.

    Args:
        estimator: Fitted CJE estimator
        policy: Target policy name
        alpha: Significance level (default 0.05 for 95% CI)
        method: Method for computing Wid (future options)

    Returns:
        Tuple of (Wid, diagnostics dict)
    """
    # Placeholder implementation
    logger.debug(f"Identification width computation not yet implemented for {policy}")

    # Return small placeholder value
    wid = 0.1  # Placeholder

    diagnostics = {
        "method": method,
        "alpha": alpha,
        "implemented": False,
        "note": "Phase 2 feature - isotonic bands and overlap bounds",
    }

    return wid, diagnostics


def compute_isotonic_bands(
    X: np.ndarray,
    Y: np.ndarray,
    alpha: float = 0.05,
    n_boot: int = 500,
) -> Tuple[np.ndarray, np.ndarray]:
    """Compute confidence bands for isotonic regression.

    Phase 2 feature - not yet implemented.

    Args:
        X: Covariate values (e.g., judge scores)
        Y: Response values
        alpha: Significance level
        n_boot: Number of bootstrap samples

    Returns:
        Tuple of (lower_band, upper_band)
    """
    # Placeholder
    n = len(X)
    fitted = np.mean(Y) * np.ones(n)
    margin = 0.1

    return fitted - margin, fitted + margin


def compute_overlap_bounds(
    weights: np.ndarray,
    alpha: float = 0.05,
) -> Tuple[float, float]:
    """Compute bounds on overlap contribution to Wid.

    Phase 2 feature - not yet implemented.

    Args:
        weights: Importance weights
        alpha: Significance level

    Returns:
        Tuple of (lower_bound, upper_bound)
    """
    # Placeholder
    return 0.0, 0.2


=== ./cje/cfbits/overlap.py ===

"""Overlap metrics on σ(S) for structural information bounds.

These metrics measure the structural overlap on the judge score marginal,
providing ceilings on what any S-indexed calibration can achieve.
"""

from dataclasses import dataclass
from typing import Optional, Tuple, Dict, Any
import numpy as np
import logging
from sklearn.isotonic import IsotonicRegression

logger = logging.getLogger(__name__)


@dataclass
class OverlapFloors:
    """Overlap metrics that bound achievable efficiency.

    Attributes:
        aessf: Adjusted ESS Fraction on σ(S) ∈ (0,1]
        bc: Bhattacharyya coefficient on σ(S) ∈ (0,1]
        chi2_s: χ² divergence on judge marginal
        ci_aessf: Confidence interval for A-ESSF
        ci_bc: Confidence interval for BC
        omega_profile: Optional profile of ω(s) = E[W|S=s]
    """

    aessf: float
    bc: float
    chi2_s: float
    ci_aessf: Tuple[float, float]
    ci_bc: Tuple[float, float]
    omega_profile: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        result = {
            "aessf": self.aessf,
            "bc": self.bc,
            "chi2_s": self.chi2_s,
            "ci_aessf": list(self.ci_aessf),
            "ci_bc": list(self.ci_bc),
        }
        if self.omega_profile:
            result["omega_profile"] = self.omega_profile
        return result

    def summary(self) -> str:
        """Human-readable summary."""
        return (
            f"Structural overlap: A-ESSF={self.aessf:.1%} "
            f"[{self.ci_aessf[0]:.1%}, {self.ci_aessf[1]:.1%}], "
            f"BC={self.bc:.1%} [Theory: A-ESSF ≤ BC²={self.bc**2:.1%}]"
        )


def estimate_omega_conservative(
    S: np.ndarray,
    W: np.ndarray,
    method: str = "bins",
    n_bins: int = 20,
    use_cross_fit: bool = True,
    n_folds: int = 5,
    random_state: Optional[int] = None,
) -> np.ndarray:
    """Conservative estimate of ω(s) = E[W|S=s].

    Uses binning or kernel methods to estimate E[W|S] without assuming monotonicity.
    For A-ESSF computation, we want a conservative (high variance) estimate.

    Args:
        S: Judge scores
        W: Raw importance weights
        method: "bins" for histogram, "isotonic_both" for max of ↑↓
        n_bins: Number of bins for histogram method
        use_cross_fit: Whether to use cross-fitting
        n_folds: Number of folds for cross-fitting
        random_state: Random seed for reproducibility

    Returns:
        Conservative ω(S) estimates
    """
    n = len(S)

    # Set up random number generator
    rng = np.random.default_rng(random_state)

    if method == "bins":
        # Histogram-based estimation
        # Create bins based on quantiles
        bin_edges = np.percentile(S, np.linspace(0, 100, n_bins + 1))
        bin_edges[0] -= 1e-10
        bin_edges[-1] += 1e-10

        omega_hat = np.zeros(n)

        if use_cross_fit and n_folds > 1 and n >= 50:
            # Cross-fitted version
            fold_indices = np.arange(n) % n_folds
            rng.shuffle(fold_indices)

            for fold in range(n_folds):
                train_mask = fold_indices != fold
                test_mask = fold_indices == fold

                if np.sum(test_mask) == 0 or np.sum(train_mask) < 20:
                    continue

                # Compute bin means on training
                bin_means = {}
                for i in range(n_bins):
                    bin_mask = (S[train_mask] >= bin_edges[i]) & (
                        S[train_mask] < bin_edges[i + 1]
                    )
                    if np.sum(bin_mask) > 0:
                        bin_means[i] = np.mean(W[train_mask][bin_mask])
                    else:
                        bin_means[i] = np.mean(W[train_mask])  # Fallback

                # Apply to test
                for i in range(n_bins):
                    test_bin_mask = (
                        test_mask & (S >= bin_edges[i]) & (S < bin_edges[i + 1])
                    )
                    omega_hat[test_bin_mask] = bin_means[i]

            # Fill any zeros with simple estimate
            if np.any(omega_hat == 0):
                for i in range(n_bins):
                    bin_mask = (S >= bin_edges[i]) & (S < bin_edges[i + 1])
                    if np.sum(bin_mask) > 0:
                        omega_hat[bin_mask & (omega_hat == 0)] = np.mean(W[bin_mask])
        else:
            # Simple binning
            for i in range(n_bins):
                bin_mask = (S >= bin_edges[i]) & (S < bin_edges[i + 1])
                if np.sum(bin_mask) > 0:
                    omega_hat[bin_mask] = np.mean(W[bin_mask])
                else:
                    omega_hat[bin_mask] = np.mean(W)

    elif method == "isotonic_both":
        # Try both increasing and decreasing, take max variance
        from sklearn.isotonic import IsotonicRegression

        # Increasing fit
        iso_inc = IsotonicRegression(y_min=0, increasing=True)
        omega_inc = iso_inc.fit_transform(S, W)

        # Decreasing fit
        iso_dec = IsotonicRegression(y_min=0, increasing=False)
        omega_dec = iso_dec.fit_transform(S, W)

        # Choose the one with higher variance (more conservative for A-ESSF)
        var_inc = np.var(omega_inc)
        var_dec = np.var(omega_dec)

        if var_inc >= var_dec:
            omega_hat = omega_inc
            logger.debug(f"Using increasing fit (var={var_inc:.3f} vs {var_dec:.3f})")
        else:
            omega_hat = omega_dec
            logger.debug(f"Using decreasing fit (var={var_dec:.3f} vs {var_inc:.3f})")
    else:
        raise ValueError(f"Unknown method: {method}")

    # Ensure mean-1 normalization
    if np.mean(omega_hat) > 0:
        omega_hat = omega_hat * (np.mean(W) / np.mean(omega_hat))
    else:
        omega_hat = np.ones_like(W)

    return omega_hat


def compute_chi2_divergence(omega: np.ndarray) -> float:
    """Compute χ² divergence from fitted ω(S).

    χ²_S = E[ω(S)²] - 1

    Args:
        omega: Fitted ω(S) values (should be mean-1)

    Returns:
        χ² divergence
    """
    return float(np.mean(omega**2) - 1)


def compute_aessf(chi2_s: float) -> float:
    """Compute Adjusted ESS Fraction from χ² divergence.

    A-ESSF = 1 / (1 + χ²_S) = exp(-D₂(P'_S || P_S))

    Args:
        chi2_s: χ² divergence on judge marginal

    Returns:
        A-ESSF ∈ (0, 1]
    """
    return 1.0 / (1.0 + chi2_s)


def compute_bhattacharyya_coefficient(
    S: np.ndarray,
    W: np.ndarray,
    n_bins: int = 20,
) -> float:
    """Compute Bhattacharyya coefficient on σ(S).

    BC = ∫ √(dP'_S × dP_S)

    Uses adaptive binning for numerical stability.

    Args:
        S: Judge scores
        W: Importance weights
        n_bins: Number of bins for discretization

    Returns:
        BC ∈ (0, 1]
    """
    # Create bins based on quantiles of S
    bin_edges = np.percentile(S, np.linspace(0, 100, n_bins + 1))
    bin_edges[0] -= 1e-10  # Ensure all points included
    bin_edges[-1] += 1e-10

    # Assign to bins
    bin_indices = np.digitize(S, bin_edges) - 1

    # Compute probabilities under P and P'
    p0 = np.zeros(n_bins)
    p1 = np.zeros(n_bins)

    for i in range(n_bins):
        mask = bin_indices == i
        if np.sum(mask) > 0:
            p0[i] = np.mean(mask)  # Empirical probability under P
            p1[i] = np.mean(W[mask]) * p0[i]  # Probability under P'

    # Normalize
    p0 = p0 / np.sum(p0)
    p1 = p1 / np.sum(p1)

    # Compute BC
    bc = float(np.sum(np.sqrt(p0 * p1)))

    return min(bc, 1.0)  # Ensure ≤ 1


def bootstrap_overlap_metrics(
    S: np.ndarray,
    W: np.ndarray,
    n_boot: int = 500,
    alpha: float = 0.05,
    seed: Optional[int] = None,
) -> Dict[str, Tuple[float, float]]:
    """Bootstrap confidence intervals for overlap metrics.

    Uses paired bootstrap, resampling (S, W) pairs together.

    Args:
        S: Judge scores
        W: Importance weights
        n_boot: Number of bootstrap samples
        alpha: Significance level
        seed: Random seed for reproducibility

    Returns:
        Dict with CIs for 'aessf' and 'bc'
    """
    if seed is not None:
        np.random.seed(seed)

    n = len(S)
    aessf_boot = []
    bc_boot = []

    for _ in range(n_boot):
        # Resample with replacement
        idx = np.random.choice(n, n, replace=True)
        S_boot = S[idx]
        W_boot = W[idx]

        # Compute metrics using conservative estimation
        # Use a derived seed for reproducibility within bootstrap
        boot_seed = None if seed is None else seed + _ + 1
        omega_boot = estimate_omega_conservative(
            S_boot, W_boot, method="bins", use_cross_fit=False, random_state=boot_seed
        )
        chi2_boot = compute_chi2_divergence(omega_boot)
        aessf_boot.append(compute_aessf(chi2_boot))

        bc_boot.append(compute_bhattacharyya_coefficient(S_boot, W_boot))

    # Compute percentile CIs
    aessf_ci = (
        np.percentile(aessf_boot, 100 * alpha / 2),
        np.percentile(aessf_boot, 100 * (1 - alpha / 2)),
    )

    bc_ci = (
        np.percentile(bc_boot, 100 * alpha / 2),
        np.percentile(bc_boot, 100 * (1 - alpha / 2)),
    )

    return {
        "aessf": aessf_ci,
        "bc": bc_ci,
    }


def estimate_overlap_floors(
    S: np.ndarray,
    W: np.ndarray,
    method: str = "conservative",
    n_boot: int = 500,
    alpha: float = 0.05,
    random_state: Optional[int] = None,
    return_omega_profile: bool = False,
) -> OverlapFloors:
    """Estimate structural overlap floors on σ(S).

    These metrics provide upper bounds on achievable efficiency for any
    S-indexed calibration method.

    Args:
        S: Judge scores
        W: Raw importance weights (not necessarily calibrated)
        method: Method for ω(S) estimation ("conservative", "bins", "isotonic_both")
        n_boot: Number of bootstrap samples for CIs
        alpha: Significance level for CIs
        random_state: Random seed for reproducibility
        return_omega_profile: Whether to include ω(S) profile in output

    Returns:
        OverlapFloors with A-ESSF, BC, and confidence intervals
    """
    # Validate inputs
    if len(S) != len(W):
        raise ValueError(f"S and W must have same length, got {len(S)} and {len(W)}")

    if len(S) < 10:
        raise ValueError(f"Need at least 10 samples, got {len(S)}")

    # Remove any invalid weights (including tiny negatives from numerical noise)
    valid_mask = np.isfinite(W) & (W >= -1e-10)
    if not np.all(valid_mask):
        logger.warning(f"Removing {np.sum(~valid_mask)} invalid weights")
        S = S[valid_mask]
        W = W[valid_mask]

    # Clip any tiny negative values to 0
    W = np.maximum(W, 0.0)

    # Normalize weights to mean 1
    if np.mean(W) > 0:
        W = W / np.mean(W)
    else:
        raise ValueError("All weights are zero or negative")

    # Estimate ω(S) = E[W|S] conservatively
    if method == "conservative":
        # Default conservative method: bins
        omega = estimate_omega_conservative(
            S, W, method="bins", use_cross_fit=True, random_state=random_state
        )
    elif method in ["bins", "isotonic_both"]:
        omega = estimate_omega_conservative(
            S, W, method=method, use_cross_fit=True, random_state=random_state
        )
    else:
        raise ValueError(f"Unknown method: {method}")

    # Compute χ² divergence and A-ESSF
    chi2_s = compute_chi2_divergence(omega)
    aessf = compute_aessf(chi2_s)

    # Compute Bhattacharyya coefficient
    bc = compute_bhattacharyya_coefficient(S, W)

    # Bootstrap confidence intervals
    if n_boot > 0:
        cis = bootstrap_overlap_metrics(S, W, n_boot, alpha, random_state)
        ci_aessf = cis["aessf"]
        ci_bc = cis["bc"]
    else:
        ci_aessf = (aessf, aessf)
        ci_bc = (bc, bc)

    # Prepare omega profile if requested
    omega_profile = None
    if return_omega_profile:
        # Create grid of S values
        s_grid = np.percentile(S, np.linspace(0, 100, 100))

        # For profile, we can show the actual omega values
        # Interpolate conservatively
        from scipy.interpolate import interp1d

        # Sort by S for interpolation
        sort_idx = np.argsort(S)
        S_sorted = S[sort_idx]
        omega_sorted = omega[sort_idx]

        # Create interpolator with nearest neighbor for extrapolation
        interp = interp1d(
            S_sorted,
            omega_sorted,
            kind="nearest",
            bounds_error=False,
            fill_value=(omega_sorted[0], omega_sorted[-1]),
        )
        omega_grid = interp(s_grid)

        omega_profile = {
            "s_grid": s_grid.tolist(),
            "omega_grid": omega_grid.tolist(),
            "mean": float(np.mean(omega)),
            "std": float(np.std(omega)),
        }

    # Verify theoretical constraint: A-ESSF ≤ BC²
    if aessf > bc**2 + 0.01:  # Allow small numerical tolerance
        logger.warning(
            f"Theoretical violation: A-ESSF={aessf:.3f} > BC²={bc**2:.3f}. "
            "This suggests numerical issues."
        )

    return OverlapFloors(
        aessf=aessf,
        bc=bc,
        chi2_s=chi2_s,
        ci_aessf=ci_aessf,
        ci_bc=ci_bc,
        omega_profile=omega_profile,
    )


=== ./cje/cfbits/playbooks.py ===

"""Practical usage playbooks for CF-bits.

This module provides ready-to-use functions for common CF-bits workflows,
handling the two main scenarios: (A) fresh draws with DR, and (B) logging-only IPS.
"""

from typing import Optional, Dict, Any, TYPE_CHECKING
import numpy as np
import logging

from .sampling import compute_ifr_aess, compute_sampling_width, compute_estimator_eif
from .overlap import estimate_overlap_floors
from .core import compute_cfbits, apply_gates
from .identification import compute_identification_width

if TYPE_CHECKING:
    from ..estimators.base_estimator import BaseCJEEstimator

logger = logging.getLogger(__name__)


def cfbits_report_fresh_draws(
    estimator: "BaseCJEEstimator",
    policy: str,
    n_boot: int = 800,
    alpha: float = 0.05,
    random_state: Optional[int] = None,
    compute_tail_index: bool = False,
) -> Dict[str, Any]:
    """CF-bits report for fresh draws scenario (DR/TMLE).

    Use when you have:
    - Fresh draws from target policy with judge scores
    - DR/MRDR/TMLE estimator with cross-fitted nuisance models
    - Oracle labels for calibration

    Args:
        estimator: Fitted DR/TMLE estimator
        policy: Target policy name
        n_boot: Bootstrap samples for overlap CIs
        alpha: Significance level (0.05 for 95% CI)
        random_state: Random seed for reproducibility
        compute_tail_index: Whether to compute Hill tail index

    Returns:
        Complete CF-bits report dictionary
    """
    report = {"policy": policy, "scenario": "fresh_draws"}

    # 1. Sampling width with OUA
    logger.info(f"Computing sampling width for {policy}")
    wvar, var_components = compute_sampling_width(
        estimator,
        policy,
        alpha=alpha,
        use_iic=True,  # Use IIC for variance reduction
        compute_oua=True,
    )
    report["sampling_width"] = {
        "wvar": float(wvar),
        "var_main": float(var_components.var_main),
        "var_oracle": float(var_components.var_oracle),
        "var_total": float(var_components.var_total),
    }

    # 2. Efficient IF and IFR
    logger.info("Computing efficiency metrics")
    phi = estimator.get_influence_functions(policy)
    eif = compute_estimator_eif(estimator, policy)

    if phi is not None:
        efficiency = compute_ifr_aess(
            phi, eif=eif, n=len(phi), var_oracle=var_components.var_oracle
        )
        report["efficiency"] = {
            "ifr_main": float(efficiency.ifr_main),
            "ifr_oua": float(efficiency.ifr_oua),
            "aess_main": float(efficiency.aess_main),
            "aess_oua": float(efficiency.aess_oua),
            "var_phi": float(efficiency.var_phi),
            "var_eif": float(efficiency.var_eif),
        }
    else:
        logger.warning("No influence functions available")
        efficiency = None
        report["efficiency"] = None

    # 3. Structural floors on logging data
    logger.info("Computing structural overlap floors")
    try:
        # Get logging pool judge scores and RAW importance weights
        # Important: Use raw weights, not calibrated, to measure structural overlap
        S_log = estimator.sampler.get_judge_scores()
        W_log = estimator.sampler.compute_importance_weights(policy, mode="hajek")

        if S_log is not None and W_log is not None:
            floors = estimate_overlap_floors(
                S_log,
                W_log,
                method="conservative",
                n_boot=n_boot,
                alpha=alpha,
                random_state=random_state,
            )
            report["overlap"] = {
                "aessf": float(floors.aessf),
                "aessf_lcb": float(floors.ci_aessf[0]),
                "aessf_ucb": float(floors.ci_aessf[1]),
                "bc": float(floors.bc),
                "chi2_s": float(floors.chi2_s),
            }
        else:
            logger.warning("Cannot compute overlap: missing judge scores or weights")
            floors = None
            report["overlap"] = None
    except Exception as e:
        logger.warning(f"Failed to compute overlap: {e}")
        floors = None
        report["overlap"] = None

    # 4. Identification width (placeholder for now)
    wid, wid_diag = compute_identification_width(estimator, policy, alpha=alpha)
    report["identification"] = {
        "wid": float(wid),
        "diagnostics": wid_diag,
    }

    # 5. CF-bits computation
    logger.info("Computing CF-bits")
    cfbits = compute_cfbits(
        w0=1.0,
        wid=wid,
        wvar=wvar,
        ifr_main=efficiency.ifr_main if efficiency else None,
        ifr_oua=efficiency.ifr_oua if efficiency else None,
    )
    report["cfbits"] = {
        "bits_tot": float(cfbits.bits_tot),
        "bits_id": float(cfbits.bits_id) if cfbits.bits_id else None,
        "bits_var": float(cfbits.bits_var) if cfbits.bits_var else None,
        "w_tot": float(cfbits.w_tot),
        "w_max": float(cfbits.w_max),
        "dominant": "identification" if cfbits.w_id > cfbits.w_var else "sampling",
    }

    # 6. Reliability gates
    logger.info("Applying reliability gates")

    # Compute variance ratio
    var_oracle_ratio = None
    if phi is not None and len(phi) > 0:
        var_oracle_ratio = var_components.var_oracle / max(
            var_components.var_main / len(phi), 1e-12
        )

    # Compute tail index if requested
    tail_index = None
    if compute_tail_index and W_log is not None:
        try:
            from ..diagnostics.tail_diagnostics import estimate_tail_index

            tail_index = estimate_tail_index(W_log)
        except:
            logger.debug("Could not compute tail index")

    decision = apply_gates(
        aessf=floors.aessf if floors else None,
        aessf_lcb=floors.ci_aessf[0] if floors else None,
        ifr=efficiency.ifr_oua if efficiency else None,
        tail_index=tail_index,
        var_oracle_ratio=var_oracle_ratio,
    )
    report["gates"] = {
        "state": decision.state,
        "reasons": decision.reasons,
        "suggestions": decision.suggestions,
    }

    # 7. Summary interpretation
    report["summary"] = _generate_summary(report)

    return report


def cfbits_report_logging_only(
    estimator: "BaseCJEEstimator",
    policy: str,
    n_boot: int = 800,
    alpha: float = 0.05,
    random_state: Optional[int] = None,
    compute_tail_index: bool = True,
) -> Dict[str, Any]:
    """CF-bits report for logging-only scenario (IPS/Cal-IPS).

    Use when you have:
    - Only logged data (no fresh draws)
    - RawIPS or CalibratedIPS estimator
    - Limited ability to estimate EIF

    Args:
        estimator: Fitted IPS/Cal-IPS estimator
        policy: Target policy name
        n_boot: Bootstrap samples for overlap CIs
        alpha: Significance level (0.05 for 95% CI)
        random_state: Random seed for reproducibility
        compute_tail_index: Whether to compute Hill tail index

    Returns:
        Complete CF-bits report dictionary
    """
    report = {"policy": policy, "scenario": "logging_only"}

    # 1. Sampling width (with IIC if available)
    logger.info(f"Computing sampling width for {policy}")
    wvar, var_components = compute_sampling_width(
        estimator, policy, alpha=alpha, use_iic=True, compute_oua=True
    )
    report["sampling_width"] = {
        "wvar": float(wvar),
        "var_main": float(var_components.var_main),
        "var_oracle": float(var_components.var_oracle),
        "var_total": float(var_components.var_total),
    }

    # 2. IFR (limited for Cal-IPS)
    logger.info("Computing efficiency metrics")
    phi = estimator.get_influence_functions(policy)
    estimator_type = estimator.__class__.__name__

    if phi is not None:
        if estimator_type == "RawIPS":
            # For RawIPS, IF = EIF
            efficiency = compute_ifr_aess(
                phi, eif=phi, n=len(phi), var_oracle=var_components.var_oracle
            )
            report["efficiency"] = {
                "ifr_main": 1.0,  # IF = EIF
                "ifr_oua": float(efficiency.ifr_oua),
                "aess_main": float(len(phi)),
                "aess_oua": float(efficiency.aess_oua),
                "var_phi": float(efficiency.var_phi),
            }
        else:
            # For Cal-IPS, we don't have true EIF
            var_phi = np.var(phi, ddof=1)
            n = len(phi)

            # Can only compute OUA share
            oua_share = (n * var_components.var_oracle) / (
                var_phi + n * var_components.var_oracle
            )
            report["efficiency"] = {
                "ifr_main": None,  # Unknown without EIF
                "ifr_oua": None,
                "oua_share": float(oua_share),
                "var_phi": float(var_phi),
                "note": "EIF unavailable for CalibratedIPS",
            }
            efficiency = None
    else:
        report["efficiency"] = None
        efficiency = None

    # 3. Structural floors (critical for IPS)
    logger.info("Computing structural overlap floors")
    try:
        # Use RAW weights for structural overlap, not calibrated weights
        S_log = estimator.sampler.get_judge_scores()
        W_log = estimator.sampler.compute_importance_weights(policy, mode="hajek")

        if S_log is not None and W_log is not None:
            floors = estimate_overlap_floors(
                S_log,
                W_log,
                method="conservative",
                n_boot=n_boot,
                alpha=alpha,
                random_state=random_state,
            )
            report["overlap"] = {
                "aessf": float(floors.aessf),
                "aessf_lcb": float(floors.ci_aessf[0]),
                "aessf_ucb": float(floors.ci_aessf[1]),
                "bc": float(floors.bc),
                "chi2_s": float(floors.chi2_s),
            }
        else:
            floors = None
            report["overlap"] = None
    except Exception as e:
        logger.warning(f"Failed to compute overlap: {e}")
        floors = None
        report["overlap"] = None

    # 4. Identification width (placeholder)
    wid = 0.1  # Conservative placeholder for logging-only
    report["identification"] = {
        "wid": float(wid),
        "note": "Conservative placeholder for logging-only scenario",
    }

    # 5. CF-bits (limited without EIF)
    logger.info("Computing CF-bits")
    cfbits = compute_cfbits(
        w0=1.0,
        wid=wid,
        wvar=wvar,
        ifr_main=efficiency.ifr_main if efficiency else None,
        ifr_oua=efficiency.ifr_oua if efficiency else None,
    )
    report["cfbits"] = {
        "bits_tot": float(cfbits.bits_tot),
        "bits_var": None,  # Often unavailable without EIF
        "w_tot": float(cfbits.w_tot),
        "w_max": float(cfbits.w_max),
        "dominant": "sampling",  # Usually sampling-limited
    }

    # 6. Reliability gates (stricter for IPS)
    logger.info("Applying reliability gates")

    # Compute tail index (important for IPS)
    tail_index = None
    if compute_tail_index and W_log is not None:
        try:
            # Simple Hill estimator
            W_sorted = np.sort(W_log)[::-1]
            k = max(10, int(np.sqrt(len(W_sorted))))
            if k < len(W_sorted):
                tail_index = k / np.sum(np.log(W_sorted[:k] / W_sorted[k]))
        except:
            logger.debug("Could not compute tail index")

    # Variance ratio
    var_oracle_ratio = None
    if phi is not None and len(phi) > 0:
        var_oracle_ratio = var_components.var_oracle / max(
            var_components.var_main / len(phi), 1e-12
        )

    decision = apply_gates(
        aessf=floors.aessf if floors else None,
        aessf_lcb=floors.ci_aessf[0] if floors else None,
        ifr=efficiency.ifr_oua if efficiency and efficiency.ifr_oua else None,
        tail_index=tail_index,
        var_oracle_ratio=var_oracle_ratio,
    )
    report["gates"] = {
        "state": decision.state,
        "reasons": decision.reasons,
        "suggestions": decision.suggestions,
    }

    # 7. IPS-specific warnings
    if floors and floors.ci_aessf[0] < 0.05:
        report["warning"] = "CRITICAL: A-ESSF < 5% - consider refusing this evaluation"
    elif floors and floors.ci_aessf[0] < 0.20:
        report["warning"] = "Poor overlap - strongly recommend DR with fresh draws"

    report["summary"] = _generate_summary(report)

    return report


def _generate_summary(report: Dict[str, Any]) -> str:
    """Generate human-readable summary from CF-bits report."""
    parts = []

    # Scenario
    scenario = report.get("scenario", "unknown")
    parts.append(f"Scenario: {scenario}")

    # Gate state
    gate_state = report.get("gates", {}).get("state", "UNKNOWN")
    parts.append(f"Reliability: {gate_state}")

    # Key metrics
    if report.get("overlap"):
        aessf_lcb = report["overlap"]["aessf_lcb"]
        parts.append(f"A-ESSF LCB: {aessf_lcb:.1%}")

    if report.get("efficiency"):
        eff = report["efficiency"]
        if eff.get("ifr_oua") is not None:
            parts.append(f"IFR (OUA): {eff['ifr_oua']:.1%}")
        elif eff.get("oua_share") is not None:
            parts.append(f"OUA share: {eff['oua_share']:.1%}")

    # CF-bits
    if report.get("cfbits"):
        cf = report["cfbits"]
        parts.append(f"Total bits: {cf['bits_tot']:.2f}")
        parts.append(f"Width: {cf['w_tot']:.3f}")

    # Main recommendation
    if gate_state == "REFUSE":
        parts.append("→ Do not use this estimate")
    elif gate_state == "CRITICAL":
        parts.append("→ Use with extreme caution")
    elif gate_state == "WARNING":
        parts.append("→ Consider improvements")
    else:
        parts.append("→ Estimate appears reliable")

    return " | ".join(parts)


=== ./cje/cfbits/sampling.py ===

"""Sampling width and efficiency metrics for CF-bits.

This module computes:
- IFR (Information Fraction): ratio of efficient IF variance to actual IF variance
- aESS (adjusted Effective Sample Size): n × IFR
- Sampling width (Wvar): statistical uncertainty from finite samples
"""

from dataclasses import dataclass
from typing import Optional, Tuple, Dict, Any, TYPE_CHECKING
import numpy as np
import logging
from scipy import stats

if TYPE_CHECKING:
    from ..estimators.base_estimator import BaseCJEEstimator

logger = logging.getLogger(__name__)


@dataclass
class EfficiencyStats:
    """Efficiency metrics for an estimator.

    Attributes:
        ifr_main: Information Fraction (no OUA) = Var(EIF)/Var(IF)
        ifr_oua: Information Fraction incl. oracle = Var(EIF)/(Var(IF)+n*Var_oracle)
        aess_main: n × ifr_main
        aess_oua: n × ifr_oua
        var_phi: Variance of actual influence function
        var_eif: Variance of efficient influence function
        var_oracle: Oracle jackknife variance (per-sample scale)
    """

    ifr_main: float
    ifr_oua: float
    aess_main: float
    aess_oua: float
    var_phi: float
    var_eif: float
    var_oracle: float

    def to_dict(self) -> Dict[str, float]:
        """Convert to dictionary for serialization."""
        return {
            "ifr_main": self.ifr_main,
            "ifr_oua": self.ifr_oua,
            "aess_main": self.aess_main,
            "aess_oua": self.aess_oua,
            "var_phi": self.var_phi,
            "var_eif": self.var_eif,
            "var_oracle": self.var_oracle,
        }


@dataclass
class SamplingVariance:
    """Variance components for sampling width.

    Attributes:
        var_main: Main influence function variance
        var_oracle: Oracle uncertainty augmentation variance
        var_total: Total variance (var_main/n + var_oracle)
    """

    var_main: float
    var_oracle: float
    var_total: float

    def to_dict(self) -> Dict[str, float]:
        """Convert to dictionary for serialization."""
        return {
            "var_main": self.var_main,
            "var_oracle": self.var_oracle,
            "var_total": self.var_total,
        }


def compute_ifr_aess(
    phi: np.ndarray,
    eif: Optional[np.ndarray] = None,
    n: Optional[int] = None,
    var_oracle: float = 0.0,
) -> EfficiencyStats:
    """Compute Information Fraction Ratio and adjusted ESS.

    IFR measures how close the estimator is to the efficiency bound:
    - IFR_main = Var(EIF) / Var(IF) ∈ (0, 1]
    - IFR_OUA = Var(EIF) / (Var(IF) + n*Var_oracle) ∈ (0, 1]

    aESS is the equivalent sample size if we had an efficient estimator:
    - aESS_main = n × IFR_main
    - aESS_OUA = n × IFR_OUA

    Args:
        phi: Actual influence function values (per-sample contributions)
        eif: Efficient influence function values (if available)
        n: Sample size (defaults to len(phi))
        var_oracle: Oracle uncertainty variance (from jackknife)

    Returns:
        EfficiencyStats with both IFR versions and variance components
    """
    if n is None:
        n = len(phi)

    # Ensure phi is centered (should already be, but be safe)
    phi_centered = phi - np.mean(phi)
    var_phi = float(np.var(phi_centered, ddof=1))

    if eif is None:
        # For IPS estimators without nuisance parameters, IF = EIF
        # This is a reasonable default for now
        logger.debug("No EIF provided, assuming IF = EIF (IPS-like estimator)")
        var_eif = var_phi
    else:
        # Ensure eif is centered
        eif_centered = eif - np.mean(eif)
        var_eif = float(np.var(eif_centered, ddof=1))

    # Compute IFR_main (without OUA)
    if var_phi > 0:
        ifr_main = min(max(var_eif / var_phi, 0.0), 1.0)
    else:
        logger.warning("IF variance is 0, setting IFR to 1")
        ifr_main = 1.0

    # Compute IFR_OUA (including oracle uncertainty)
    # IFR_OUA = Var(EIF) / (Var(phi) + n*Var_oracle)
    denom = var_phi + n * max(var_oracle, 0.0)
    if denom > 0:
        ifr_oua = min(max(var_eif / denom, 0.0), 1.0)
    else:
        ifr_oua = 1.0

    # Compute aESS versions
    aess_main = n * ifr_main
    aess_oua = n * ifr_oua

    return EfficiencyStats(
        ifr_main=ifr_main,
        ifr_oua=ifr_oua,
        aess_main=aess_main,
        aess_oua=aess_oua,
        var_phi=var_phi,
        var_eif=var_eif,
        var_oracle=var_oracle,
    )


def compute_eif_plugin_dr(
    weights: np.ndarray,
    rewards: np.ndarray,
    predictions: np.ndarray,
    psi: Optional[float] = None,
) -> np.ndarray:
    """Compute plug-in efficient influence function for DR estimators.

    For DR estimators, the efficient influence function under the working
    model (correct outcome model and propensity scores) is:

    EIF = W * (R - g(X)) + g(X) - ψ

    where:
    - W: importance weights
    - R: rewards (outcomes)
    - g(X): outcome predictions
    - ψ: the target parameter (estimate)

    Args:
        weights: Importance weights (should be mean-1)
        rewards: Rewards/outcomes used in estimation
        predictions: Outcome model predictions g(X)
        psi: Target parameter estimate (if None, uses mean of base)

    Returns:
        Efficient influence function values
    """
    # Normalize weights to mean 1 if not already
    weights_normalized = weights / np.mean(weights)

    # DR efficient influence function base
    base = weights_normalized * (rewards - predictions) + predictions

    # Subtract estimate to get influence function
    if psi is None:
        psi = float(np.mean(base))

    eif = base - psi

    return eif


def compute_eif_plugin_ips(
    weights: np.ndarray,
    rewards: np.ndarray,
) -> np.ndarray:
    """Compute plug-in efficient influence function for IPS estimators.

    For IPS estimators without nuisance parameters, the influence function
    IS the efficient influence function:

    EIF = W * R - ψ

    Args:
        weights: Importance weights (should be mean-1)
        rewards: Rewards/outcomes

    Returns:
        Efficient influence function values
    """
    # Normalize weights to mean 1 if not already
    weights_normalized = weights / np.mean(weights)

    # IPS influence function
    eif = weights_normalized * rewards

    # Center
    eif_centered = eif - np.mean(eif)

    return eif_centered


def compute_sampling_width(
    estimator: "BaseCJEEstimator",
    policy: str,
    alpha: float = 0.05,
    use_iic: bool = True,
    iic_kwargs: Optional[Dict[str, Any]] = None,
    compute_oua: bool = True,  # Enable by default now
) -> Tuple[float, SamplingVariance]:
    """Compute sampling width (Wvar) for uncertainty.

    Sampling width captures the statistical uncertainty from finite samples:
    Wvar = 2 × z_{1-α/2} × √(Var(φ)/n + Var_oracle)

    Args:
        estimator: Fitted CJE estimator
        policy: Target policy name
        alpha: Significance level (default 0.05 for 95% CI)
        use_iic: Whether to apply IIC variance reduction
        iic_kwargs: Optional IIC configuration
        compute_oua: Whether to compute oracle uncertainty

    Returns:
        Tuple of (Wvar, SamplingVariance with components)
    """
    # Get influence functions
    if_values = estimator.get_influence_functions(policy)
    if if_values is None:
        raise ValueError(f"No influence functions available for policy '{policy}'")

    # Make a copy to avoid modifying original
    phi = if_values.copy()

    # Apply IIC if requested
    if use_iic:
        try:
            # Check if IIC is available
            from ..calibration.iic import IsotonicInfluenceControl

            # Get judge scores from sampler
            judge_scores = estimator.sampler.get_judge_scores()
            if judge_scores is not None and len(judge_scores) == len(phi):
                iic = IsotonicInfluenceControl()
                phi_residualized, iic_diagnostics = iic.residualize(
                    influence=phi,
                    judge_scores=judge_scores,
                    policy=policy,
                    fold_ids=None,  # Could use folds if available
                )

                if iic_diagnostics.get("applied", False):
                    var_reduction = iic_diagnostics.get("var_reduction_pct", 0)
                    logger.info(
                        f"IIC reduced variance by {var_reduction:.1f}% for {policy}"
                    )
                    phi = phi_residualized
            else:
                logger.debug("Judge scores not available for IIC")
        except ImportError:
            logger.warning("IIC not available, skipping variance reduction")

    # Compute main variance
    var_main = float(np.var(phi, ddof=1))
    n = len(phi)

    # Compute OUA variance via oracle jackknife
    var_oracle = 0.0
    if compute_oua:
        # Optional hook on estimator: get leave-one-oracle-fold re-estimates
        jackknife_vals = None
        if hasattr(estimator, "get_oracle_jackknife"):
            try:
                jackknife_vals = estimator.get_oracle_jackknife(policy)
            except Exception as e:
                logger.debug(f"get_oracle_jackknife failed: {e}")

        if jackknife_vals is not None:
            jackknife_vals = np.asarray(jackknife_vals, dtype=float)
            K = len(jackknife_vals)
            if K >= 2:
                psi_bar = float(np.mean(jackknife_vals))
                var_oracle = (
                    (K - 1) / K * float(np.mean((jackknife_vals - psi_bar) ** 2))
                )
                logger.debug(
                    f"Oracle jackknife variance: {var_oracle:.6f} from {K} folds"
                )
            else:
                logger.debug(f"Not enough oracle folds for jackknife: {K}")

    # Total variance
    var_total = var_main / n + var_oracle

    # Compute width
    z_score = stats.norm.ppf(1 - alpha / 2)
    wvar = 2 * z_score * np.sqrt(var_total)

    return wvar, SamplingVariance(
        var_main=var_main,
        var_oracle=var_oracle,
        var_total=var_total,
    )


def compute_estimator_eif(
    estimator: "BaseCJEEstimator",
    policy: str,
) -> Optional[np.ndarray]:
    """Compute or retrieve efficient influence function for an estimator.

    This function attempts to get the EIF through various methods:
    1. Check if estimator has get_eif() method
    2. Use plug-in formulas based on estimator type
    3. Default to IF = EIF for simple estimators

    Args:
        estimator: Fitted estimator
        policy: Target policy

    Returns:
        EIF values or None if unavailable
    """
    # First check if estimator provides EIF directly
    if hasattr(estimator, "get_eif"):
        eif = estimator.get_eif(policy)
        if eif is not None:
            return eif

    # Try plug-in based on estimator type
    estimator_type = estimator.__class__.__name__

    if estimator_type in ["RawIPS", "SNIPS"]:
        # For pure IPS variants without calibration learning, IF = EIF
        return estimator.get_influence_functions(policy)

    elif estimator_type == "CalibratedIPS":
        # CalibratedIPS has first-stage learning (calibration), so IF ≠ EIF in general
        # Would need to implement get_eif() properly accounting for calibration
        logger.debug("CalibratedIPS EIF not implemented - returning None")
        return None

    elif estimator_type in ["DRCPOEstimator", "MRDREstimator", "TMLEEstimator"]:
        # For DR estimators, use plug-in formula if we have the components
        try:
            weights = estimator.get_weights(policy)

            # Try to get outcome predictions
            if hasattr(estimator, "_outcome_predictions"):
                predictions = estimator._outcome_predictions.get(policy)
            else:
                predictions = None

            # Get rewards
            rewards = estimator.sampler.get_rewards()

            if weights is not None and predictions is not None and rewards is not None:
                # Try to get the estimate
                psi_hat = None
                if hasattr(estimator, "get_estimate"):
                    try:
                        psi_hat = estimator.get_estimate(policy)
                    except:
                        pass
                return compute_eif_plugin_dr(weights, rewards, predictions, psi=psi_hat)
            else:
                logger.debug(
                    f"Missing components for EIF computation in {estimator_type}"
                )
        except Exception as e:
            logger.debug(f"Failed to compute plug-in EIF: {e}")

    # Default: assume IF = EIF
    logger.debug(f"Using IF as EIF for {estimator_type}")
    return estimator.get_influence_functions(policy)


=== ./cje/data/__init__.py ===

"""Data loading and preparation utilities.

This module contains:
- Data models: Pydantic models for type safety
- PrecomputedSampler: Load data with log probs and rewards
- DatasetFactory: SOLID-compliant data loading with optional calibration
- DatasetLoader: Pure data loading functionality
- Reward Utils: Utility functions for calibrated rewards
"""

from .precomputed_sampler import PrecomputedSampler
from .reward_utils import (
    add_rewards_to_existing_data,
)
from .models import (
    Sample,
    Dataset,
    EstimationResult,
    LogProbStatus,
    LogProbResult,
)
from .factory import DatasetFactory, default_factory
from .loaders import DatasetLoader, JsonlDataSource, InMemoryDataSource
from .validation import (
    validate_cje_data,
    validate_for_precomputed_sampler,
)
from .folds import (
    get_fold,
    get_folds_for_prompts,
    get_folds_for_dataset,
    get_folds_with_oracle_balance,
)

from typing import Optional, List


# Convenience function
def load_dataset_from_jsonl(
    file_path: str, target_policies: Optional[List[str]] = None
) -> Dataset:
    """Load Dataset from JSONL file.

    Convenience function using the default factory.
    """
    return default_factory.create_from_jsonl(file_path, target_policies)


__all__ = [
    # Data loading
    "PrecomputedSampler",
    "DatasetFactory",
    "DatasetLoader",
    "default_factory",
    "JsonlDataSource",
    "InMemoryDataSource",
    # Data models
    "Sample",
    "Dataset",
    "EstimationResult",
    "LogProbStatus",
    "LogProbResult",
    # Utilities
    "add_rewards_to_existing_data",
    # Validation
    "validate_cje_data",
    "validate_for_precomputed_sampler",
    # Fold management
    "get_fold",
    "get_folds_for_prompts",
    "get_folds_for_dataset",
    "get_folds_with_oracle_balance",
    # Convenience function
    "load_dataset_from_jsonl",
]


=== ./cje/data/factory.py ===

"""Dataset factory for loading datasets.

This module follows SOLID principles by using dependency injection
and separating concerns into focused classes.
"""

from typing import List, Dict, Any, Optional, Tuple
import numpy as np

from .models import Dataset
from .loaders import DatasetLoader, DataSource, JsonlDataSource, InMemoryDataSource


class DatasetFactory:
    """Factory for creating Datasets from various sources.

    Follows SOLID principles:
    - Single Responsibility: Coordinates data loading
    - Open/Closed: Easy to extend with new loaders
    - Dependency Injection: Takes loader as dependency
    """

    def __init__(
        self,
        loader: Optional[DatasetLoader] = None,
    ):
        """Initialize factory with optional custom loader.

        Args:
            loader: DatasetLoader instance. If None, uses default.
        """
        self.loader = loader or DatasetLoader()

    def create_from_jsonl(
        self, file_path: str, target_policies: Optional[List[str]] = None
    ) -> Dataset:
        """Create Dataset from JSONL file.

        Args:
            file_path: Path to JSONL file
            target_policies: Optional list of target policy names

        Returns:
            Dataset instance
        """
        source = JsonlDataSource(file_path)
        return self.loader.load_from_source(source, target_policies)

    def create_from_data(
        self, data: List[Dict[str, Any]], target_policies: Optional[List[str]] = None
    ) -> Dataset:
        """Create Dataset from in-memory data.

        Args:
            data: List of dictionaries with data
            target_policies: Optional list of target policy names

        Returns:
            Dataset instance
        """
        source = InMemoryDataSource(data)
        return self.loader.load_from_source(source, target_policies)


# Convenience factory instance with default configuration
default_factory = DatasetFactory()


=== ./cje/data/folds.py ===

"""Unified fold assignment for cross-validation.

Core principle: Use prompt_id hashing for stable fold assignment
that survives filtering and works across all components.

All cross-validation in CJE MUST use these functions.
"""

import hashlib
import numpy as np
from typing import List, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from .models import Dataset


def get_fold(prompt_id: str, n_folds: int = 5, seed: int = 42) -> int:
    """Get fold assignment for a single prompt_id.

    This is THE authoritative way to assign folds in CJE.
    Uses stable hashing that:
    - Survives sample filtering
    - Works with fresh draws (same prompt_id → same fold)
    - Ensures consistency across all components

    Args:
        prompt_id: Unique identifier for the prompt
        n_folds: Number of folds for cross-validation
        seed: Random seed for reproducibility

    Returns:
        Fold index in [0, n_folds)

    Example:
        >>> get_fold("prompt_123")  # Always returns same fold
        2
        >>> get_fold("prompt_123", n_folds=10)  # Different for different n_folds
        7
    """
    if not prompt_id:
        raise ValueError("prompt_id cannot be empty")
    if n_folds < 2:
        raise ValueError(f"n_folds must be at least 2, got {n_folds}")

    hash_input = f"{prompt_id}-{seed}-{n_folds}".encode()
    hash_bytes = hashlib.blake2b(hash_input, digest_size=8).digest()
    return int.from_bytes(hash_bytes, "big") % n_folds


def get_folds_for_prompts(
    prompt_ids: List[str], n_folds: int = 5, seed: int = 42
) -> np.ndarray:
    """Get fold assignments for multiple prompt_ids.

    Args:
        prompt_ids: List of prompt identifiers
        n_folds: Number of folds
        seed: Random seed

    Returns:
        Array of fold indices, shape (len(prompt_ids),)
    """
    if not prompt_ids:
        return np.array([], dtype=int)

    return np.array([get_fold(pid, n_folds, seed) for pid in prompt_ids])


def get_folds_for_dataset(
    dataset: "Dataset", n_folds: int = 5, seed: int = 42
) -> np.ndarray:
    """Get fold assignments for all samples in a dataset.

    Args:
        dataset: Dataset with samples containing prompt_ids
        n_folds: Number of folds
        seed: Random seed

    Returns:
        Array of fold indices aligned with dataset.samples
    """
    prompt_ids = [s.prompt_id for s in dataset.samples]
    return get_folds_for_prompts(prompt_ids, n_folds, seed)


def get_folds_with_oracle_balance(
    prompt_ids: List[str], oracle_mask: np.ndarray, n_folds: int = 5, seed: int = 42
) -> np.ndarray:
    """Get folds with balanced oracle sample distribution.

    Ensures oracle samples are evenly distributed across folds
    (important for small oracle subsets). Unlabeled samples
    use standard hash-based assignment.

    Args:
        prompt_ids: All prompt identifiers
        oracle_mask: Boolean mask indicating oracle samples
        n_folds: Number of folds
        seed: Random seed

    Returns:
        Array of fold indices with balanced oracle distribution

    Note:
        This is primarily for JudgeCalibrator backward compatibility.
        New code should use get_folds_for_prompts() directly.
    """
    n = len(prompt_ids)
    if n == 0:
        return np.array([], dtype=int)

    if len(oracle_mask) != n:
        raise ValueError(
            f"oracle_mask length ({len(oracle_mask)}) must match "
            f"prompt_ids length ({n})"
        )

    folds = np.zeros(n, dtype=int)

    # Oracle samples: round-robin for perfect balance
    oracle_indices = np.where(oracle_mask)[0]
    if len(oracle_indices) > 0:
        # Shuffle oracle indices for randomization
        rng = np.random.RandomState(seed)
        oracle_indices = oracle_indices.copy()
        rng.shuffle(oracle_indices)

        for i, idx in enumerate(oracle_indices):
            folds[idx] = i % n_folds

    # Unlabeled samples: standard hash-based
    unlabeled = ~oracle_mask
    if np.any(unlabeled):
        unlabeled_ids = [prompt_ids[i] for i in range(n) if unlabeled[i]]
        unlabeled_folds = get_folds_for_prompts(unlabeled_ids, n_folds, seed)
        folds[unlabeled] = unlabeled_folds

    return folds


=== ./cje/data/fresh_draw_utils.py ===

"""Utilities for computing fresh draw statistics for Monte Carlo uncertainty."""

from typing import Dict, Any, TYPE_CHECKING
import numpy as np

if TYPE_CHECKING:
    from .fresh_draws import FreshDrawDataset


def compute_fresh_draw_prompt_stats(
    fresh_dataset: "FreshDrawDataset",
) -> Dict[str, Dict[str, Any]]:
    """Compute per-prompt statistics for fresh draws.

    Args:
        fresh_dataset: FreshDrawDataset with samples containing judge scores

    Returns:
        Dictionary mapping prompt_id to stats dict with keys:
        - 'mean': Sample mean of judge scores
        - 'var': Unbiased sample variance (ddof=1) if M_i > 1, else 0.0
        - 'n': Number of fresh draws (M_i)
    """
    scores_by_prompt: Dict[str, list] = {}

    # Group scores by prompt_id
    for sample in fresh_dataset.samples:
        pid = str(sample.prompt_id)
        # Get judge score from metadata
        if hasattr(sample, "judge_score"):
            score = float(sample.judge_score)
        elif "judge_score" in sample.metadata:
            score = float(sample.metadata["judge_score"])
        else:
            # Try reward as fallback
            score = float(sample.reward) if sample.reward is not None else 0.0

        scores_by_prompt.setdefault(pid, []).append(score)

    # Compute statistics per prompt
    stats: Dict[str, Dict[str, Any]] = {}
    for pid, scores_list in scores_by_prompt.items():
        x = np.array(scores_list, dtype=np.float64)
        m = x.size

        # Compute mean and variance
        mean = float(x.mean()) if m > 0 else 0.0
        var = float(x.var(ddof=1)) if m > 1 else 0.0  # Unbiased variance

        stats[pid] = {"mean": mean, "var": var, "n": m}

    return stats


=== ./cje/data/fresh_draws.py ===

"""Data models and utilities for fresh draws used in DR estimation."""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, TYPE_CHECKING
import numpy as np
from pydantic import BaseModel, Field, field_validator

logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from .models import Dataset


class FreshDrawSample(BaseModel):
    """A single fresh draw sample for DR estimation.

    Represents a fresh response sampled from a target policy,
    evaluated by the judge.
    """

    prompt_id: str = Field(..., description="ID to align with logged data")
    target_policy: str = Field(..., description="Policy that generated this response")
    judge_score: float = Field(..., ge=0, le=1, description="Judge evaluation score")
    response: Optional[str] = Field(None, description="Generated response (optional)")
    draw_idx: int = Field(
        ..., ge=0, description="Draw index for this prompt (0, 1, 2...)"
    )
    fold_id: Optional[int] = Field(
        None, description="CV fold assignment (should match logged data)"
    )

    @field_validator("judge_score")
    def validate_judge_score(cls, v: float) -> float:
        if not 0 <= v <= 1:
            raise ValueError(f"Judge score must be in [0, 1], got {v}")
        return v


class FreshDrawDataset(BaseModel):
    """Collection of fresh draws for a target policy.

    Contains pre-generated fresh samples from a target policy,
    evaluated by a judge, for use in DR estimation.
    """

    target_policy: str = Field(..., description="Target policy name")
    draws_per_prompt: int = Field(..., ge=1, description="Number of draws per prompt")
    samples: List[FreshDrawSample] = Field(..., min_length=1)

    @field_validator("samples")
    def validate_samples(
        cls, v: List[FreshDrawSample], info: Any
    ) -> List[FreshDrawSample]:
        """Ensure samples are consistent."""
        if "target_policy" in info.data:
            policy = info.data["target_policy"]
            for sample in v:
                if sample.target_policy != policy:
                    raise ValueError(
                        f"Sample has policy '{sample.target_policy}' "
                        f"but dataset is for '{policy}'"
                    )
        return v

    @property
    def n_samples(self) -> int:
        """Total number of fresh draw samples."""
        return len(self.samples)

    def get_prompt_ids(self) -> List[str]:
        """Get unique prompt IDs in dataset."""
        return sorted(set(s.prompt_id for s in self.samples))

    def get_scores_for_prompt_id(self, prompt_id: str) -> np.ndarray:
        """Get judge scores for a specific prompt.

        Args:
            prompt_id: The prompt ID to get scores for

        Returns:
            Array of judge scores for this prompt, sorted by draw_idx
        """
        # Sort by draw_idx for reproducibility
        matching_samples = sorted(
            [s for s in self.samples if s.prompt_id == prompt_id],
            key=lambda s: s.draw_idx,
        )

        if not matching_samples:
            raise ValueError(f"No samples found for prompt_id '{prompt_id}'")

        # Allow variable draws per prompt (don't enforce exact count)
        # Just log if different from expected
        if self.draws_per_prompt and len(matching_samples) != self.draws_per_prompt:
            logger.debug(
                f"Prompt '{prompt_id}' has {len(matching_samples)} draws "
                f"(dataset average: {self.draws_per_prompt})"
            )

        return np.array([s.judge_score for s in matching_samples])

    def get_samples_for_prompt_id(self, prompt_id: str) -> List[FreshDrawSample]:
        """Get all samples for a specific prompt.

        Args:
            prompt_id: The prompt ID to get samples for

        Returns:
            List of samples for this prompt
        """
        samples = [s for s in self.samples if s.prompt_id == prompt_id]

        if not samples:
            raise ValueError(f"No samples found for prompt_id '{prompt_id}'")

        # Sort by draw_idx to ensure consistent ordering
        return sorted(samples, key=lambda s: s.draw_idx)

    def to_arrays(self) -> Dict[str, np.ndarray]:
        """Convert dataset to arrays for efficient computation.

        Returns:
            Dict with 'prompt_ids' and 'judge_scores' arrays
        """
        # Sort samples by (prompt_id, draw_idx) for consistent ordering
        sorted_samples = sorted(self.samples, key=lambda s: (s.prompt_id, s.draw_idx))

        prompt_ids = []
        judge_scores = []

        for sample in sorted_samples:
            prompt_ids.append(sample.prompt_id)
            judge_scores.append(sample.judge_score)

        return {
            "prompt_ids": np.array(prompt_ids),
            "judge_scores": np.array(judge_scores),
        }

    def summary(self) -> Dict[str, Any]:
        """Get summary statistics for the dataset."""
        scores = np.array([s.judge_score for s in self.samples])
        unique_prompts = self.get_prompt_ids()

        return {
            "target_policy": self.target_policy,
            "n_samples": len(self.samples),
            "n_prompts": len(unique_prompts),
            "draws_per_prompt": self.draws_per_prompt,
            "judge_score_mean": float(scores.mean()),
            "judge_score_std": float(scores.std()),
            "judge_score_min": float(scores.min()),
            "judge_score_max": float(scores.max()),
        }


# ============================================================================
# Utility functions for fresh draws
# ============================================================================


def load_fresh_draws_from_jsonl(path: str) -> Dict[str, FreshDrawDataset]:
    """Load fresh draws from JSONL file, grouped by policy.

    This function delegates to FreshDrawLoader in the loaders module
    for consistency with other data loading operations.

    Expected JSONL format:
    {"prompt_id": "0", "target_policy": "premium", "judge_score": 0.85, "draw_idx": 0}
    {"prompt_id": "0", "target_policy": "premium", "judge_score": 0.82, "draw_idx": 1}
    {"prompt_id": "1", "target_policy": "premium", "judge_score": 0.90, "draw_idx": 0}

    Args:
        path: Path to JSONL file containing fresh draws

    Returns:
        Dict mapping policy names to FreshDrawDataset objects
    """
    from .loaders import FreshDrawLoader

    return FreshDrawLoader.load_from_jsonl(path)


def validate_fresh_draws(
    fresh_draws: FreshDrawDataset,
    logged_dataset: "Dataset",
    policy: str,
) -> None:
    """Validate fresh draws have complete coverage for a policy.

    Args:
        fresh_draws: Fresh draw dataset to validate
        logged_dataset: Logged dataset to check coverage against
        policy: Target policy name

    Raises:
        ValueError: If fresh draws don't have complete coverage
    """
    # Get valid samples for this policy from logged data
    valid_samples = [
        s for s in logged_dataset.samples if s.get_importance_weight(policy) is not None
    ]

    if not valid_samples:
        raise ValueError(f"No valid logged samples for policy '{policy}'")

    # Get prompt IDs
    logged_ids = {s.prompt_id for s in valid_samples}
    fresh_ids = set(fresh_draws.get_prompt_ids())

    # Check coverage
    missing = logged_ids - fresh_ids
    extra = fresh_ids - logged_ids

    if missing:
        raise ValueError(
            f"Fresh draws missing for {len(missing)} prompts:\n"
            f"  First 5 missing: {list(missing)[:5]}\n"
            f"DR requires fresh draws for ALL samples with valid importance weights."
        )

    if extra:
        logger.warning(
            f"Fresh draws contain {len(extra)} extra prompts not in logged data. "
            f"These will be ignored."
        )

    # Check draws per prompt (allow variable M_i)
    draw_counts = []
    for prompt_id in logged_ids:
        try:
            prompt_id_str = str(prompt_id) if prompt_id is not None else ""
            scores = fresh_draws.get_scores_for_prompt_id(prompt_id_str)
            draw_counts.append(len(scores))

            # Warn if significantly different from expected
            if (
                fresh_draws.draws_per_prompt
                and len(scores) != fresh_draws.draws_per_prompt
            ):
                logger.debug(
                    f"Prompt '{prompt_id}' has {len(scores)} draws "
                    f"(dataset average: {fresh_draws.draws_per_prompt})"
                )
        except ValueError as e:
            raise ValueError(f"Validation failed: {e}")

    # Compute statistics
    min_draws = min(draw_counts) if draw_counts else 0
    max_draws = max(draw_counts) if draw_counts else 0
    avg_draws = sum(draw_counts) / len(draw_counts) if draw_counts else 0

    logger.info(
        f"Fresh draws validated: {len(fresh_ids)} prompts, "
        f"draws/prompt: min={min_draws}, avg={avg_draws:.1f}, max={max_draws}"
    )


def create_synthetic_fresh_draws(
    logged_dataset: "Dataset",
    target_policy: str,
    draws_per_prompt: int = 5,
    score_correlation: float = 0.8,
    seed: Optional[int] = None,
) -> FreshDrawDataset:
    """Create synthetic fresh draws for testing.

    Generates correlated judge scores based on logged data,
    useful for testing DR without actual API calls.

    Args:
        logged_dataset: Logged dataset to base fresh draws on
        target_policy: Target policy name
        draws_per_prompt: Number of draws per prompt
        score_correlation: Correlation with logged judge scores (0-1)
        seed: Random seed for reproducibility

    Returns:
        Synthetic FreshDrawDataset
    """
    if seed is not None:
        np.random.seed(seed)

    # Get valid samples for this policy
    valid_samples = [
        s
        for s in logged_dataset.samples
        if s.get_importance_weight(target_policy) is not None
    ]

    if not valid_samples:
        raise ValueError(f"No valid samples for policy '{target_policy}'")

    samples: List[FreshDrawSample] = []
    for sample in valid_samples:
        prompt_id = sample.prompt_id
        base_score = sample.metadata.get("judge_score", 0.5)

        for draw_idx in range(draws_per_prompt):
            # Generate correlated score
            noise = np.random.normal(0, 0.1 * (1 - score_correlation))
            score = np.clip(base_score + noise, 0, 1)

            fresh_sample = FreshDrawSample(
                prompt_id=prompt_id,
                target_policy=target_policy,
                judge_score=float(score),
                response=f"Synthetic response for {prompt_id} draw {draw_idx}",
                draw_idx=draw_idx,
                fold_id=None,
            )
            samples.append(fresh_sample)

    dataset = FreshDrawDataset(
        target_policy=target_policy,
        draws_per_prompt=draws_per_prompt,
        samples=samples,
    )

    logger.info(
        f"Created synthetic fresh draws: {len(samples)} samples, "
        f"{len(valid_samples)} prompts, {draws_per_prompt} draws/prompt"
    )

    return dataset


def load_fresh_draws_auto(
    data_dir: Path,
    policy: str,
    verbose: bool = False,
) -> FreshDrawDataset:
    """
    Load fresh draws from files.

    This function tries to load fresh draws from standard locations:
    1. {data_dir}/{policy}_responses.jsonl
    2. {data_dir}/responses/{policy}_responses.jsonl
    3. {data_dir}/{policy}_fresh.jsonl
    4. {data_dir}/fresh_draws/{policy}.jsonl

    Args:
        data_dir: Directory to search for fresh draw files
        policy: Target policy name
        verbose: Whether to log detailed information

    Returns:
        FreshDrawDataset for the specified policy

    Raises:
        FileNotFoundError: If no fresh draw file found
    """
    # Standard file patterns to check
    possible_files = [
        data_dir / f"{policy}_responses.jsonl",
        data_dir / "responses" / f"{policy}_responses.jsonl",
        data_dir / f"{policy}_fresh.jsonl",
        data_dir / "fresh_draws" / f"{policy}.jsonl",
    ]

    # Try to load from each possible location
    for file_path in possible_files:
        if file_path.exists():
            if verbose:
                logger.info(f"Loading fresh draws from {file_path}")

            try:
                # Load the file
                fresh_samples = []
                with open(file_path, "r") as f:
                    for line in f:
                        data = json.loads(line)

                        # Handle different formats
                        # Check for judge_score properly - don't use 'or' for numeric fields
                        if "judge_score" in data and data["judge_score"] is not None:
                            judge_score = data["judge_score"]
                        elif (
                            "metadata" in data
                            and "judge_score" in data["metadata"]
                            and data["metadata"]["judge_score"] is not None
                        ):
                            judge_score = data["metadata"]["judge_score"]
                        else:
                            # Never fabricate missing data - fail clearly
                            raise ValueError(
                                f"Missing judge_score for prompt_id={data.get('prompt_id')} "
                                f"in {file_path}. Fresh draws require judge scores."
                            )

                        fresh_sample = FreshDrawSample(
                            prompt_id=str(data.get("prompt_id")),
                            target_policy=policy,
                            response=data.get("response", ""),
                            judge_score=judge_score,
                            draw_idx=data.get("draw_idx", 0),
                            fold_id=data.get("fold_id"),
                        )
                        fresh_samples.append(fresh_sample)

                # Create dataset
                fresh_dataset = FreshDrawDataset(
                    target_policy=policy,
                    draws_per_prompt=1,  # Will be updated based on actual data
                    samples=fresh_samples,
                )

                # Update draws_per_prompt based on actual data
                prompt_counts: Dict[str, int] = {}
                for sample in fresh_samples:
                    prompt_counts[sample.prompt_id] = (
                        prompt_counts.get(sample.prompt_id, 0) + 1
                    )
                if prompt_counts:
                    fresh_dataset.draws_per_prompt = max(prompt_counts.values())

                if verbose:
                    logger.info(
                        f"Loaded {len(fresh_samples)} fresh draws for {policy} "
                        f"({len(prompt_counts)} unique prompts)"
                    )

                return fresh_dataset

            except Exception as e:
                logger.warning(f"Failed to load {file_path}: {e}")
                continue

    # No file found - raise error with helpful message
    searched_paths = "\n  ".join(str(p) for p in possible_files)
    raise FileNotFoundError(
        f"No fresh draw file found for policy '{policy}'. Searched:\n  {searched_paths}\n"
        f"Fresh draws must be generated from real teacher forcing responses."
    )


def save_fresh_draws_to_jsonl(
    datasets: Dict[str, FreshDrawDataset],
    path: str,
) -> None:
    """Save fresh draw datasets to JSONL file.

    Args:
        datasets: Dict mapping policy names to FreshDrawDataset objects
        path: Output path for JSONL file
    """
    path_obj = Path(path)
    path_obj.parent.mkdir(parents=True, exist_ok=True)

    with open(path_obj, "w") as f:
        for policy, dataset in datasets.items():
            for sample in dataset.samples:
                record = {
                    "prompt_id": sample.prompt_id,
                    "target_policy": sample.target_policy,
                    "judge_score": sample.judge_score,
                    "draw_idx": sample.draw_idx,
                }
                if sample.response is not None:
                    record["response"] = sample.response

                f.write(json.dumps(record) + "\n")

    total_samples = sum(len(d.samples) for d in datasets.values())
    logger.info(f"Saved {total_samples} fresh draws to {path_obj}")


=== ./cje/data/loaders.py ===

"""Data loading utilities following SOLID principles.

This module separates data loading concerns from the Dataset model,
following the Single Responsibility Principle.
"""

import json
import logging
from typing import List, Dict, Any, Optional, Protocol
from abc import ABC, abstractmethod
from collections import defaultdict
from pathlib import Path

from .models import Dataset, Sample
from .fresh_draws import FreshDrawSample, FreshDrawDataset

logger = logging.getLogger(__name__)


class DataSource(Protocol):
    """Protocol for data sources."""

    def load(self) -> List[Dict[str, Any]]:
        """Load raw data as list of dictionaries."""
        ...


class JsonlDataSource:
    """Load data from JSONL files."""

    def __init__(self, file_path: str):
        self.file_path = file_path

    def load(self) -> List[Dict[str, Any]]:
        """Load data from JSONL file."""
        data = []
        with open(self.file_path, "r") as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
        return data


class InMemoryDataSource:
    """Load data from in-memory list."""

    def __init__(self, data: List[Dict[str, Any]]):
        self.data = data

    def load(self) -> List[Dict[str, Any]]:
        """Return the in-memory data."""
        return self.data


class DatasetLoader:
    """Loads and converts raw data into typed Dataset objects.

    Follows Single Responsibility Principle - only handles data loading and conversion.
    """

    def __init__(
        self,
        base_policy_field: str = "base_policy_logprob",
        target_policy_logprobs_field: str = "target_policy_logprobs",
        prompt_field: str = "prompt",
        response_field: str = "response",
        reward_field: str = "reward",
    ):
        self.base_policy_field = base_policy_field
        self.target_policy_logprobs_field = target_policy_logprobs_field
        self.prompt_field = prompt_field
        self.response_field = response_field
        self.reward_field = reward_field

    def load_from_source(
        self, source: DataSource, target_policies: Optional[List[str]] = None
    ) -> Dataset:
        """Load Dataset from a data source.

        Args:
            source: Data source to load from
            target_policies: List of target policy names. If None, auto-detected.

        Returns:
            Dataset instance
        """
        data = source.load()
        return self._convert_raw_data(data, target_policies)

    def _convert_raw_data(
        self, data: List[Dict[str, Any]], target_policies: Optional[List[str]] = None
    ) -> Dataset:
        """Convert raw data to Dataset."""
        # Auto-detect target policies if needed
        if target_policies is None:
            target_policies = self._detect_target_policies(data)

        # Convert raw data to samples
        samples = []
        for idx, record in enumerate(data):
            try:
                sample = self._convert_record_to_sample(record, idx)
                samples.append(sample)
            except (KeyError, ValueError) as e:
                # Skip invalid records
                print(f"Skipping invalid record: {e}")
                continue

        if not samples:
            raise ValueError("No valid samples could be created from data")

        return Dataset(
            samples=samples,
            target_policies=target_policies,
            metadata={
                "source": "loader",
                "base_policy_field": self.base_policy_field,
                "target_policy_logprobs_field": self.target_policy_logprobs_field,
            },
        )

    def _detect_target_policies(self, data: List[Dict[str, Any]]) -> List[str]:
        """Auto-detect target policies from data."""
        policies = set()
        for record in data:
            if self.target_policy_logprobs_field in record:
                policies.update(record[self.target_policy_logprobs_field].keys())
        return sorted(list(policies))

    def _convert_record_to_sample(self, record: Dict[str, Any], idx: int = 0) -> Sample:
        """Convert a single record to a Sample.

        Args:
            record: Raw data record
            idx: Index in dataset (used as fallback if prompt is also missing)
        """
        # Get prompt_id - check top-level first, then metadata, then auto-generate
        prompt_id = record.get("prompt_id") or record.get("metadata", {}).get(
            "prompt_id"
        )
        if prompt_id is None:
            # Auto-generate from prompt hash for consistency across datasets
            # This ensures fresh draws will map to the same prompt_id
            prompt = record.get(self.prompt_field, "")
            if prompt:
                import hashlib

                # Use first 12 chars of SHA256 for readable but unique ID
                prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()[:12]
                prompt_id = f"prompt_{prompt_hash}"
            else:
                # Fallback to index if no prompt either
                prompt_id = f"sample_{idx:06d}"

        # Extract reward if present (handle nested format)
        reward = None
        if self.reward_field in record:
            reward = record[self.reward_field]
            if isinstance(reward, dict):
                reward = reward.get("mean", reward.get("value"))
            if reward is not None:
                reward = float(reward)

        # Get base log prob
        base_logprob = record.get(self.base_policy_field)

        # Get target log probs
        target_logprobs = record.get(self.target_policy_logprobs_field, {})

        # Collect all other fields into metadata
        metadata = record.get("metadata", {})

        # Add any fields that aren't core fields to metadata
        core_fields = {
            "prompt_id",
            self.prompt_field,
            self.response_field,
            self.reward_field,
            self.base_policy_field,
            self.target_policy_logprobs_field,
            "metadata",
        }

        for key, value in record.items():
            if key not in core_fields:
                metadata[key] = value

        # Create Sample object
        return Sample(
            prompt_id=prompt_id,
            prompt=record[self.prompt_field],
            response=record[self.response_field],
            reward=reward,
            base_policy_logprob=base_logprob,
            target_policy_logprobs=target_logprobs,
            metadata=metadata,
        )


class FreshDrawLoader:
    """Loader for fresh draw samples used in DR estimation."""

    @staticmethod
    def load_from_jsonl(path: str) -> Dict[str, FreshDrawDataset]:
        """Load fresh draws from JSONL file, grouped by policy.

        Expected JSONL format:
        {"prompt_id": "0", "target_policy": "premium", "judge_score": 0.85, "draw_idx": 0}
        {"prompt_id": "0", "target_policy": "premium", "judge_score": 0.82, "draw_idx": 1}
        {"prompt_id": "1", "target_policy": "premium", "judge_score": 0.90, "draw_idx": 0}

        Args:
            path: Path to JSONL file containing fresh draws

        Returns:
            Dict mapping policy names to FreshDrawDataset objects
        """
        path_obj = Path(path)
        if not path_obj.exists():
            raise FileNotFoundError(f"Fresh draws file not found: {path_obj}")

        # Group samples by policy
        samples_by_policy: Dict[str, List[FreshDrawSample]] = defaultdict(list)

        with open(path_obj, "r") as f:
            for line_num, line in enumerate(f, 1):
                try:
                    data = json.loads(line)

                    # Create FreshDrawSample
                    sample = FreshDrawSample(
                        prompt_id=data["prompt_id"],
                        target_policy=data["target_policy"],
                        judge_score=data["judge_score"],
                        response=data.get("response"),  # Optional
                        draw_idx=data.get(
                            "draw_idx", 0
                        ),  # Default to 0 if not provided
                        fold_id=data.get("fold_id"),  # Optional
                    )

                    samples_by_policy[sample.target_policy].append(sample)

                except (json.JSONDecodeError, KeyError, ValueError) as e:
                    logger.warning(f"Skipping invalid line {line_num}: {e}")

        # Create FreshDrawDataset for each policy
        datasets = {}
        for policy, samples in samples_by_policy.items():
            # Determine draws_per_prompt
            prompt_counts: Dict[str, int] = defaultdict(int)
            for sample in samples:
                prompt_counts[sample.prompt_id] += 1

            # Check consistency
            draws_counts = list(prompt_counts.values())
            if draws_counts and len(set(draws_counts)) > 1:
                logger.warning(
                    f"Inconsistent draws per prompt for {policy}: {set(draws_counts)}"
                )

            draws_per_prompt = max(draws_counts) if draws_counts else 1

            datasets[policy] = FreshDrawDataset(
                samples=samples,
                target_policy=policy,
                draws_per_prompt=draws_per_prompt,
            )

        return datasets


=== ./cje/data/models.py ===

"""Data models for CJE using Pydantic."""

from typing import Dict, List, Optional, Any, Tuple, TYPE_CHECKING, ForwardRef, Union
from enum import Enum
from pydantic import BaseModel, Field, field_validator
import numpy as np


class LogProbStatus(Enum):
    """Status of log probability computation."""

    SUCCESS = "success"
    API_ERROR = "api_error"
    TOKEN_BOUNDARY_ERROR = "token_boundary_error"
    TOKEN_LIMIT_EXCEEDED = "token_limit_exceeded"
    EMPTY_RESPONSE = "empty_response"


class LogProbResult(BaseModel):
    """Result of log probability computation with explicit error handling."""

    value: Optional[float] = Field(
        None, description="Log probability value if successful"
    )
    status: LogProbStatus = Field(
        LogProbStatus.API_ERROR, description="Computation status"
    )
    error: Optional[str] = Field(None, description="Error message if failed")
    metadata: Dict[str, Any] = Field(
        default_factory=dict, description="Additional metadata"
    )

    @property
    def is_valid(self) -> bool:
        """Check if computation succeeded."""
        return self.status == LogProbStatus.SUCCESS and self.value is not None


class Sample(BaseModel):
    """A single sample for CJE analysis."""

    prompt_id: str = Field(..., description="Unique identifier for the prompt")
    prompt: str = Field(..., description="Input prompt/context")
    response: str = Field(..., description="Generated response")
    reward: Optional[float] = Field(
        None, ge=0, le=1, description="Calibrated reward [0,1]"
    )
    base_policy_logprob: Optional[float] = Field(
        None, description="Log prob under base policy"
    )
    target_policy_logprobs: Dict[str, Optional[float]] = Field(
        ..., description="Log probs under target policies (None for failures)"
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Optional metadata (judge_score, oracle_label, etc.)",
    )

    @field_validator("base_policy_logprob")
    def validate_base_policy_logprob(cls, v: Optional[float]) -> Optional[float]:
        if v is not None and v > 0:
            raise ValueError(f"Log probability must be <= 0, got {v}")
        return v

    @field_validator("target_policy_logprobs")
    def validate_target_policy_logprobs(
        cls, v: Dict[str, Optional[float]]
    ) -> Dict[str, Optional[float]]:
        for policy, logprob in v.items():
            if logprob is not None and logprob > 0:
                raise ValueError(
                    f"Log probability for {policy} must be <= 0, got {logprob}"
                )
        return v

    def get_importance_weight(self, target_policy: str) -> Optional[float]:
        """Compute importance weight for a target policy."""
        if self.base_policy_logprob is None:
            return None
        target_lp = self.target_policy_logprobs.get(target_policy)
        if target_lp is None:
            return None
        return float(np.exp(target_lp - self.base_policy_logprob))


class Dataset(BaseModel):
    """A dataset for CJE analysis.

    This is a pure data container following the Single Responsibility Principle.
    For loading data, use DatasetFactory or DatasetLoader.
    """

    samples: List[Sample] = Field(..., min_length=1)
    target_policies: List[str] = Field(..., min_length=1)
    metadata: Dict[str, Any] = Field(default_factory=dict)

    @field_validator("target_policies")
    def validate_policies_exist(cls, v: List[str], info: Any) -> List[str]:
        """Ensure target policies exist in samples."""
        if "samples" in info.data:
            all_policies = set()
            for sample in info.data["samples"]:
                all_policies.update(sample.target_policy_logprobs.keys())

            missing = set(v) - all_policies
            if missing:
                raise ValueError(f"Target policies not found in data: {missing}")
        return v

    def filter_valid_samples(self, target_policy: str) -> List[Sample]:
        """Get samples with valid data for a specific target policy."""
        valid_samples = []
        for sample in self.samples:
            if (
                sample.base_policy_logprob is not None
                and sample.target_policy_logprobs.get(target_policy) is not None
            ):
                valid_samples.append(sample)
        return valid_samples

    @property
    def n_samples(self) -> int:
        return len(self.samples)

    def summary(self) -> Dict[str, Any]:
        """Get dataset summary statistics."""
        rewards = [s.reward for s in self.samples if s.reward is not None]
        valid_counts = {policy: 0 for policy in self.target_policies}

        for sample in self.samples:
            for policy in self.target_policies:
                if sample.get_importance_weight(policy) is not None:
                    valid_counts[policy] += 1

        return {
            "n_samples": self.n_samples,
            "target_policies": self.target_policies,
            "reward_mean": np.mean(rewards) if rewards else None,
            "reward_std": np.std(rewards) if rewards else None,
            "valid_samples_per_policy": valid_counts,
        }


class EstimationResult(BaseModel):
    """Result from a CJE estimator.

    Influence functions are first-class outputs for statistical inference.
    Diagnostics contain quality metrics and health indicators.
    Metadata contains configuration and context.
    """

    # Core results
    estimates: np.ndarray = Field(..., description="Point estimates for each policy")
    standard_errors: np.ndarray = Field(..., description="Standard errors")
    n_samples_used: Dict[str, int] = Field(..., description="Valid samples per policy")
    method: str = Field(..., description="Estimation method used")

    # First-class statistical artifact
    influence_functions: Optional[Dict[str, np.ndarray]] = Field(
        None,
        description="Influence functions for each policy (when store_influence=True)",
    )

    # Quality metrics
    diagnostics: Optional[Union["IPSDiagnostics", "DRDiagnostics"]] = Field(
        None, description="Diagnostic information (IPSDiagnostics or DRDiagnostics)"
    )

    # Robust inference (Phase 3)
    robust_standard_errors: Optional[np.ndarray] = Field(
        None, description="Robust standard errors (bootstrap/cluster)"
    )
    robust_confidence_intervals: Optional[List[Tuple[float, float]]] = Field(
        None, description="Robust confidence intervals"
    )

    # Configuration and context
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="Configuration parameters and context (dataset path, timestamp, etc.)",
    )

    model_config = {"arbitrary_types_allowed": True}

    def confidence_interval(self, alpha: float = 0.05) -> Tuple[np.ndarray, np.ndarray]:
        """Compute confidence intervals."""
        from scipy import stats

        z = stats.norm.ppf(1 - alpha / 2)
        lower = self.estimates - z * self.standard_errors
        upper = self.estimates + z * self.standard_errors
        return lower, upper

    def best_policy(self) -> int:
        """Get index of best policy by point estimate."""
        return int(np.argmax(self.estimates))

    def compare_policies(
        self, idx1: int, idx2: int, alpha: float = 0.05
    ) -> Dict[str, Any]:
        """Compare two policies using influence functions when available."""
        diff = self.estimates[idx1] - self.estimates[idx2]

        # Use influence functions for proper variance estimation
        if self.influence_functions and "target_policies" in self.metadata:
            policies = self.metadata["target_policies"]
            if idx1 < len(policies) and idx2 < len(policies):
                p1 = policies[idx1]
                p2 = policies[idx2]

                if p1 in self.influence_functions and p2 in self.influence_functions:
                    # Compute variance of difference using influence functions
                    if1 = self.influence_functions[p1]
                    if2 = self.influence_functions[p2]

                    # Ensure same length (should be aligned)
                    if len(if1) == len(if2):
                        diff_if = if1 - if2
                        se_diff = float(np.std(diff_if, ddof=1) / np.sqrt(len(diff_if)))
                    else:
                        # Fall back to conservative estimate if lengths mismatch
                        se_diff = np.sqrt(
                            self.standard_errors[idx1] ** 2
                            + self.standard_errors[idx2] ** 2
                        )
                else:
                    # Fall back if policies not found
                    se_diff = np.sqrt(
                        self.standard_errors[idx1] ** 2
                        + self.standard_errors[idx2] ** 2
                    )
            else:
                # Fall back if indices out of range
                se_diff = np.sqrt(
                    self.standard_errors[idx1] ** 2 + self.standard_errors[idx2] ** 2
                )
        else:
            # Conservative estimate ignoring covariance
            se_diff = np.sqrt(
                self.standard_errors[idx1] ** 2 + self.standard_errors[idx2] ** 2
            )

        z_score = diff / se_diff if se_diff > 0 else 0

        from scipy import stats

        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))

        return {
            "difference": diff,
            "se_difference": se_diff,
            "z_score": z_score,
            "p_value": p_value,
            "significant": p_value < alpha,
            "used_influence": self.influence_functions is not None
            and se_diff
            != np.sqrt(
                self.standard_errors[idx1] ** 2 + self.standard_errors[idx2] ** 2
            ),
        }

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        ci_lower, ci_upper = self.confidence_interval()

        result = {
            "method": self.method,
            "estimates": self.estimates.tolist(),
            "standard_errors": self.standard_errors.tolist(),
            "n_samples_used": self.n_samples_used,
            "confidence_intervals": {
                "alpha": 0.05,
                "lower": ci_lower.tolist(),
                "upper": ci_upper.tolist(),
            },
        }

        # Add influence functions if present (convert to lists for JSON)
        if self.influence_functions:
            result["influence_functions"] = {
                policy: ifs.tolist() for policy, ifs in self.influence_functions.items()
            }

        # Add diagnostics if present
        if self.diagnostics:
            result["diagnostics"] = self.diagnostics.to_dict()

        # Add metadata if non-empty
        if self.metadata:
            result["metadata"] = self.metadata

        # Add per-policy results if policies are specified
        if "target_policies" in self.metadata:
            policies = self.metadata["target_policies"]
            result["per_policy_results"] = {}
            for i, policy in enumerate(policies):
                result["per_policy_results"][policy] = {
                    "estimate": float(self.estimates[i]),
                    "standard_error": float(self.standard_errors[i]),
                    "ci_lower": float(ci_lower[i]),
                    "ci_upper": float(ci_upper[i]),
                    "n_samples": self.n_samples_used.get(policy, 0),
                }

        # Include diagnostics if available
        if "diagnostics" in self.metadata:
            result["diagnostics"] = self.metadata["diagnostics"]

        return result


# Import at the end to resolve forward references
from ..diagnostics import IPSDiagnostics, DRDiagnostics

# Update forward references - compatible with both Pydantic v1 and v2
if hasattr(EstimationResult, "model_rebuild"):
    # Pydantic v2
    EstimationResult.model_rebuild()
elif hasattr(EstimationResult, "update_forward_refs"):
    # Pydantic v1
    EstimationResult.update_forward_refs()


=== ./cje/data/precomputed_sampler.py ===

"""Precomputed sampler for CJE estimation.

This module provides the PrecomputedSampler class which manages data access
for CJE estimators. It maintains multiple data representations for different
purposes:

1. Original samples (dataset.samples): Complete Sample objects with metadata
2. Formatted data (formatted_data): Filtered samples for weight computation
3. Policy data (get_data_for_policy): Transformed dicts with flattened metadata

Example Data Flow:
-----------------
    # Original Sample object
    sample.metadata["judge_score"] = 0.8
    sample.reward = 0.75

    # After get_data_for_policy("gpt-4")
    data["judge_score"] = 0.8  # Flattened from metadata
    data["reward"] = 0.75      # Direct from sample
    data["cv_fold"] = 2        # Computed from prompt_id

Key Differences:
---------------
    # WRONG - looking in metadata after get_data_for_policy
    data = sampler.get_data_for_policy("gpt-4")
    score = data[0]["metadata"]["judge_score"]  # ❌ KeyError!

    # RIGHT - judge_score is flattened to top level
    data = sampler.get_data_for_policy("gpt-4")
    score = data[0]["judge_score"]  # ✓ Correct

    # ALSO RIGHT - accessing original samples directly
    score = sampler.dataset.samples[0].metadata["judge_score"]  # ✓ Correct

See PolicyDataDict for the complete structure returned by get_data_for_policy().
"""

from typing import Dict, List, Optional, Any, Union, TypedDict
import numpy as np
import logging

from .models import Dataset
from .factory import DatasetFactory
from .folds import get_folds_for_prompts, get_fold

logger = logging.getLogger(__name__)


class PolicyDataDict(TypedDict, total=False):
    """Structure returned by get_data_for_policy().

    This is a flattened representation that combines data from:
    - Sample fields (reward, prompt, response, etc.)
    - Sample.metadata (judge_score, oracle_label)
    - Computed fields (cv_fold)

    Note: judge_score is moved from metadata to top-level for convenience.
    """

    # Required fields
    reward: float
    base_policy_logprob: float
    policy_logprob: float
    prompt: str
    response: str
    prompt_id: str

    # Optional fields (from metadata or computed)
    judge_score: Optional[float]
    oracle_label: Optional[float]
    cv_fold: int


class PrecomputedSampler:
    """Wrapper around Dataset that provides CJE-specific operations.

    This class takes a Dataset and adds methods needed for importance sampling
    estimation like weight computation, filtering, and diagnostic checks.

    Data Representations:
    --------------------
    The sampler maintains three data representations:

    1. self.dataset.samples: Original Sample objects with full metadata
       - Access: sampler.dataset.samples
       - Use for: Accessing complete metadata, oracle labels

    2. self.formatted_data: Filtered/validated samples for weight computation
       - Access: Internal only (use get_data_for_policy instead)
       - Use for: Weight computation (pre-filtered for efficiency)

    3. get_data_for_policy(): Transformed dicts with flattened structure
       - Access: sampler.get_data_for_policy(policy)
       - Use for: Main data access in estimators
       - Structure: See PolicyDataDict

    Important: get_data_for_policy() transforms the data by:
    - Filtering to samples with valid logprobs for the policy
    - Flattening metadata.judge_score to top-level
    - Adding computed cv_fold field
    """

    def __init__(
        self,
        data_or_dataset: Union[Dataset, List[Dict[str, Any]]],
        target_policies: Optional[List[str]] = None,
        **kwargs: Any,
    ):
        """Initialize with either a Dataset or raw data.

        Args:
            data_or_dataset: Either a Dataset instance or raw data list
            target_policies: Target policy names (only used if data_or_dataset is a list)
            **kwargs: Additional arguments passed to DatasetFactory

        Raises:
            ValueError: If any samples are missing rewards
        """
        if isinstance(data_or_dataset, Dataset):
            self.dataset = data_or_dataset
        else:
            # Create Dataset from raw data using factory
            factory = DatasetFactory()
            self.dataset = factory.create_from_data(
                data_or_dataset, target_policies=target_policies
            )

        # Validate that all samples have rewards
        samples_without_rewards = [
            i for i, sample in enumerate(self.dataset.samples) if sample.reward is None
        ]
        if samples_without_rewards:
            raise ValueError(
                f"PrecomputedSampler requires all samples to have rewards. "
                f"Found {len(samples_without_rewards)} samples without rewards. "
                f"Please calibrate your dataset first using calibrate_dataset()."
            )

        self.target_policies = self.dataset.target_policies

        # Prepare formatted data
        self.formatted_data = self._format_for_estimators()

    @classmethod
    def from_jsonl(
        cls, file_path: str, target_policies: Optional[List[str]] = None, **kwargs: Any
    ) -> "PrecomputedSampler":
        """Create sampler from JSONL file.

        Args:
            file_path: Path to JSONL file
            target_policies: Optional list of target policy names
            **kwargs: Additional arguments passed to DatasetFactory

        Returns:
            PrecomputedSampler instance
        """
        factory = DatasetFactory()
        dataset = factory.create_from_jsonl(file_path, target_policies)
        return cls(dataset)

    def _format_for_estimators(self) -> List[Dict[str, Any]]:
        """Format data for CJE estimators.

        Returns list of dicts with:
        - context: prompt text
        - response: generated text
        - base_policy_logprob: base policy log prob
        - reward: calibrated reward
        - target_policy_logprobs: dict of target log probs
        """
        formatted = []
        self._formatted_to_dataset_idx = []  # Track mapping for O(1) lookup
        n_missing_base = 0
        n_missing_target = {policy: 0 for policy in self.target_policies}

        for i, sample in enumerate(self.dataset.samples):
            # Skip samples without valid base log prob
            if sample.base_policy_logprob is None:
                n_missing_base += 1
                continue

            # Check all required target policies have valid log probs
            valid_targets = {}
            skip_record = False
            for policy in self.target_policies:
                logp = sample.target_policy_logprobs.get(policy)
                if logp is None:
                    n_missing_target[policy] += 1
                    skip_record = True
                    break
                valid_targets[policy] = logp

            if skip_record:
                continue

            formatted.append(
                {
                    "context": sample.prompt,
                    "response": sample.response,
                    "base_policy_logprob": sample.base_policy_logprob,
                    "reward": sample.reward,
                    "target_policy_logprobs": valid_targets,
                }
            )
            self._formatted_to_dataset_idx.append(i)

        # Report filtering statistics
        n_total = len(self.dataset.samples)
        n_valid = len(formatted)
        n_filtered = n_total - n_valid

        if n_filtered > 0:
            filter_pct = (n_filtered / n_total) * 100
            logger.warning(
                f"Filtered {n_filtered}/{n_total} samples ({filter_pct:.1f}%) due to missing log probabilities:\n"
                f"  - Missing base_policy_logprob: {n_missing_base}\n"
                f"  - Missing target policy logprobs: {n_missing_target}"
            )

            if filter_pct > 50:
                logger.error(
                    f"WARNING: More than 50% of samples filtered! Only {n_valid}/{n_total} samples remain. "
                    f"This may significantly impact estimation quality."
                )

        if not formatted:
            raise ValueError(
                f"No valid records after filtering! All {n_total} samples had invalid log probabilities.\n"
                f"  - Missing base_policy_logprob: {n_missing_base}\n"
                f"  - Missing target policy logprobs: {n_missing_target}"
            )

        if n_valid < 10:
            logger.warning(
                f"Only {n_valid} valid samples available for estimation. "
                f"Results may be unreliable with such small sample size."
            )

        return formatted

    def get_data_for_policy(self, target_policy: str) -> Optional[List[PolicyDataDict]]:
        """Get formatted data for a specific target policy.

        ⚠️ IMPORTANT: This method transforms the data structure:

        Transformations applied:
        1. Filters to only samples with valid logprobs for this policy
        2. Flattens metadata fields to top-level (e.g., metadata.judge_score → judge_score)
        3. Adds computed fields (cv_fold from prompt_id)
        4. Returns different structure than dataset.samples

        Args:
            target_policy: Name of target policy

        Returns:
            List of PolicyDataDict with structure:
            {
                "reward": float,                    # From sample.reward
                "base_policy_logprob": float,       # From sample.base_policy_logprob
                "policy_logprob": float,            # From sample.target_policy_logprobs[policy]
                "prompt": str,                      # From sample.prompt
                "response": str,                    # From sample.response
                "prompt_id": str,                   # From sample.prompt_id
                "judge_score": Optional[float],     # From sample.metadata["judge_score"]
                "cv_fold": int,                     # Computed from prompt_id
            }

            Returns None if policy not in target_policies.

        Example:
            >>> data = sampler.get_data_for_policy("gpt-4")
            >>> judge_scores = [d["judge_score"] for d in data]  # Top-level access
            >>> # NOT d["metadata"]["judge_score"] - already flattened!

        Note: This ensures consistency with weight computation by using the same
        filtered samples (formatted_data) that were used to compute weights.
        """
        if target_policy not in self.target_policies:
            return None

        # Use the same formatted_data that was used for weights to ensure consistency
        policy_data = []
        for i, record in enumerate(self.formatted_data):
            # Check if this record has the target policy logprob
            if target_policy in record["target_policy_logprobs"]:
                # Get the corresponding sample for metadata
                sample = self.dataset.samples[self._get_sample_index(i)]
                policy_data.append(
                    {
                        "reward": record["reward"],
                        "base_policy_logprob": record["base_policy_logprob"],
                        "policy_logprob": record["target_policy_logprobs"][
                            target_policy
                        ],
                        "prompt": record["context"],
                        "response": record["response"],
                        "prompt_id": sample.prompt_id,
                        "judge_score": sample.metadata.get("judge_score"),
                        # Compute cv_fold on-demand from prompt_id
                        # Use metadata if available, else defaults
                        "cv_fold": get_fold(
                            sample.prompt_id,
                            self.dataset.metadata.get("n_folds", 5),
                            self.dataset.metadata.get("fold_seed", 42),
                        ),
                    }
                )

        return policy_data if policy_data else None

    def __len__(self) -> int:
        """Return the number of valid samples in formatted data."""
        return self.n_valid_samples

    def _get_valid_indices(self, target_policy: str) -> np.ndarray:
        """Get indices of valid samples for a target policy.

        Returns indices into the original dataset.samples array.
        """
        valid_indices = []
        for i, sample in enumerate(self.dataset.samples):
            # Check if sample has valid data for this policy
            if (
                sample.base_policy_logprob is not None
                and sample.target_policy_logprobs.get(target_policy) is not None
            ):
                valid_indices.append(i)
        return np.array(valid_indices)

    def _get_sample_index(self, formatted_index: int) -> int:
        """Map from formatted_data index back to dataset.samples index.

        This is needed because formatted_data filters out invalid samples.
        """
        if formatted_index >= len(self._formatted_to_dataset_idx):
            raise IndexError(f"Formatted index {formatted_index} out of range")
        return self._formatted_to_dataset_idx[formatted_index]

    def compute_log_ratios(self, target_policy: str) -> np.ndarray:
        """Compute log importance ratios (log p_target - log p_base).

        Args:
            target_policy: Name of target policy

        Returns:
            Array of log ratios (may contain -inf for zero weights)
        """
        if target_policy not in self.target_policies:
            raise ValueError(f"Unknown target policy: {target_policy}")

        log_ratios = np.array(
            [
                record["target_policy_logprobs"][target_policy]
                - record["base_policy_logprob"]
                for record in self.formatted_data
            ],
            dtype=np.float64,
        )

        # NaN -> -inf (zero weight)
        log_ratios[np.isnan(log_ratios)] = -np.inf

        return log_ratios

    def compute_raw_weights(self, target_policy: str) -> np.ndarray:
        """Compute raw importance weights WITHOUT scaling.

        Returns truly raw weights: exp(log_p_target - log_p_base)
        Only guards against overflow to inf, no scaling applied.

        Args:
            target_policy: Name of target policy

        Returns:
            Array of raw importance weights
        """
        log_ratios = self.compute_log_ratios(target_policy)

        # Clamp only to avoid overflow to inf, keep underflow (->0) natural
        max_log = np.log(np.finfo(np.float64).max)  # ~709.78
        clamped = np.minimum(log_ratios, max_log)

        # Report extreme values
        n_clamped = np.sum(log_ratios > max_log)
        if n_clamped > 0:
            logger.debug(
                f"Clamped {n_clamped} extreme log-ratios for {target_policy} "
                f"(max was {np.max(log_ratios[np.isfinite(log_ratios)]):.1f}) to prevent overflow"
            )

        weights = np.exp(clamped)

        # Clean up non-finite values (from -inf log ratios)
        weights[~np.isfinite(weights)] = 0.0

        return np.asarray(weights)

    def compute_hajek_weights(self, target_policy: str) -> np.ndarray:
        """Compute mean-one (SNIPS/Hájek) weights using stable log-sum-exp.

        These weights have mean exactly 1.0 and are computed in a numerically
        stable way using the log-sum-exp trick.

        Args:
            target_policy: Name of target policy

        Returns:
            Array of Hájek weights with mean=1.0
        """
        log_ratios = self.compute_log_ratios(target_policy)
        n = len(log_ratios)

        # Handle all -inf case (all weights would be zero)
        finite_mask = np.isfinite(log_ratios)
        if not finite_mask.any():
            logger.warning(
                f"All log-ratios are -inf for {target_policy}; returning zeros"
            )
            return np.zeros_like(log_ratios)

        # Log-sum-exp trick: subtract max for numerical stability
        max_log = np.max(log_ratios[finite_mask])

        # Compute stable exponentials
        stable_exp = np.zeros_like(log_ratios)
        stable_exp[finite_mask] = np.exp(log_ratios[finite_mask] - max_log)

        # Sum of weights
        sum_weights = stable_exp.sum()
        if sum_weights == 0.0:
            logger.warning(
                f"Sum of weights is zero for {target_policy}; returning zeros"
            )
            return np.zeros_like(log_ratios)

        # Normalize to mean=1: w_i = n * exp(lr_i) / sum(exp(lr))
        hajek_weights = (n * stable_exp) / sum_weights

        # Verify mean is 1.0 (within floating point precision)
        actual_mean = hajek_weights.mean()
        if abs(actual_mean - 1.0) > 1e-10:
            logger.debug(
                f"Hájek weights for {target_policy} have mean {actual_mean:.12f} (expected 1.0)"
            )

        return np.asarray(hajek_weights)

    def compute_importance_weights(
        self,
        target_policy: str,
        clip_weight: Optional[float] = None,
        mode: str = "hajek",
    ) -> np.ndarray:
        """Compute importance weights for a target policy with optional clipping.

        Args:
            target_policy: Name of target policy
            clip_weight: Maximum weight value for variance control.
                        If None (default), no clipping is applied.
                        Set to a finite value (e.g., 100.0) to clip weights.
            mode: "hajek" for mean-one weights (default), "raw" for unnormalized

        Returns:
            Array of importance weights
        """
        # Get weights based on mode
        if mode == "hajek":
            weights_array = self.compute_hajek_weights(target_policy)
        elif mode == "raw":
            weights_array = self.compute_raw_weights(target_policy)
        else:
            raise ValueError(f"Unknown mode: {mode}. Use 'hajek' or 'raw'")

        # Apply clipping if requested (after Hájek normalization for interpretability)
        if clip_weight is not None and np.isfinite(clip_weight):
            n_clipped = np.sum(weights_array > clip_weight)
            if n_clipped > 0:
                max_weight = np.max(weights_array)
                weights_array = np.minimum(weights_array, clip_weight)
                logger.info(
                    f"Clipped {n_clipped}/{len(weights_array)} weights for {target_policy} "
                    f"to {clip_weight} (max was {max_weight:.2f})"
                )

        # Log statistics
        logger.debug(
            f"Weight statistics for '{target_policy}': "
            f"mean={weights_array.mean():.3f}, std={weights_array.std():.3f}, "
            f"min={weights_array.min():.3f}, max={weights_array.max():.3f}"
        )

        return np.asarray(weights_array)

    def get_rewards(self) -> np.ndarray:
        """Get array of calibrated rewards."""
        return np.array([s.reward for s in self.dataset.samples])

    def get_judge_scores(self) -> Optional[np.ndarray]:
        """Get array of judge scores from metadata.

        ⚠️ Note: This returns judge scores for the FILTERED samples (those with
        valid logprobs), not all samples in the dataset. The order matches
        the samples that would be used for weight computation.

        To get judge scores for a specific policy's data:
            >>> data = sampler.get_data_for_policy(policy)
            >>> scores = [d["judge_score"] for d in data]  # Already flattened

        Returns:
            Array of judge scores if available in all valid samples, None if any missing.
            Length matches len(formatted_data), not len(dataset.samples).
        """
        # Only get judge scores for valid (formatted) samples
        judge_scores = []
        for idx in self._formatted_to_dataset_idx:
            sample = self.dataset.samples[idx]
            score = sample.metadata.get("judge_score")
            if score is None:
                return None  # Not all samples have judge scores
            judge_scores.append(score)
        return np.array(judge_scores)

    def get_contexts(self) -> List[str]:
        """Get list of contexts/prompts."""
        return [s.prompt for s in self.dataset.samples]

    def get_responses(self) -> List[str]:
        """Get list of responses."""
        return [s.response for s in self.dataset.samples]

    def get_folds_for_policy(
        self, policy: str, n_folds: Optional[int] = None, seed: Optional[int] = None
    ) -> Optional[np.ndarray]:
        """Get consistent fold assignments for policy's valid samples.

        Returns folds for the FILTERED samples that align with
        get_data_for_policy(). This ensures folds match the actual
        data used for estimation.

        Args:
            policy: Target policy name
            n_folds: Number of cross-validation folds (uses metadata if None)
            seed: Random seed for reproducibility (uses metadata if None)

        Returns:
            Fold assignments for valid samples, or None if no data
        """
        # Use metadata values if not provided
        if n_folds is None:
            n_folds = self.dataset.metadata.get("n_folds", 5)
        if seed is None:
            seed = self.dataset.metadata.get("fold_seed", 42)

        data = self.get_data_for_policy(policy)
        if data is None:
            return None

        prompt_ids = [d["prompt_id"] for d in data]
        return get_folds_for_prompts(prompt_ids, n_folds, seed)

    @property
    def n_samples(self) -> int:
        """Number of samples in the dataset.

        Note: This returns the total number of samples in the dataset.
        For the number of samples with valid log probabilities that will
        be used for estimation, use n_valid_samples.
        """
        return self.dataset.n_samples

    @property
    def n_valid_samples(self) -> int:
        """Number of valid samples with all required log probabilities.

        This is the number of samples that will actually be used for estimation,
        after filtering out samples with missing log probabilities.
        """
        return len(self.formatted_data)

    @property
    def n_policies(self) -> int:
        """Number of target policies."""
        return len(self.target_policies)

    @property
    def oracle_coverage(self) -> Optional[float]:
        """Get oracle coverage (fraction of samples with oracle labels).

        Returns:
            Oracle coverage in [0, 1] if available, None if no information.

        Note:
            This first checks calibration metadata (most reliable),
            then falls back to scanning samples for oracle_label fields.
        """
        # First try: Check calibration metadata (most reliable)
        cal_info = self.dataset.metadata.get("calibration_info", {})
        if "n_oracle" in cal_info and "n_total" in cal_info:
            n_total = cal_info["n_total"]
            if n_total > 0:
                return float(cal_info["n_oracle"]) / float(n_total)

        # Second try: Scan samples for oracle labels
        # This handles cases where dataset wasn't created via calibrate_dataset
        n_with_oracle = 0
        n_total = len(self.dataset.samples)

        if n_total == 0:
            return None

        for sample in self.dataset.samples:
            if (
                "oracle_label" in sample.metadata
                and sample.metadata["oracle_label"] is not None
            ):
                n_with_oracle += 1

        # If we found any oracle labels, return the coverage
        if n_with_oracle > 0:
            return float(n_with_oracle) / float(n_total)

        # No oracle information found
        return None

    def summary(self) -> Dict[str, Any]:
        """Get summary statistics."""
        dataset_summary = self.dataset.summary()
        filter_rate = (
            1.0 - (self.n_valid_samples / self.n_samples) if self.n_samples > 0 else 0.0
        )

        return {
            "n_samples": self.n_samples,  # Keep for backwards compatibility
            "n_samples_valid": self.n_valid_samples,
            "n_samples_total": self.n_samples,  # Same as n_samples
            "n_samples_filtered": self.n_samples - self.n_valid_samples,
            "filter_rate": filter_rate,
            "n_policies": self.n_policies,
            "target_policies": self.target_policies,
            "reward_mean": dataset_summary["reward_mean"],
            "reward_std": dataset_summary["reward_std"],
            "valid_samples_per_policy": dataset_summary["valid_samples_per_policy"],
        }


=== ./cje/data/reward_utils.py ===

"""Utilities for judge calibration and data preparation.

This module provides utility functions for working with calibrated rewards.
Most data loading and calibration functionality has been moved to the Dataset class.
"""

import json
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path

from ..calibration.judge import JudgeCalibrator

logger = logging.getLogger(__name__)


def save_jsonl(data: List[Dict], file_path: str) -> None:
    """Save data to JSONL file."""
    with open(file_path, "w") as f:
        for record in data:
            f.write(json.dumps(record) + "\n")


def add_rewards_to_existing_data(
    data_path: str,
    calibrator: JudgeCalibrator,
    judge_score_field: str = "judge_score",
    output_path: Optional[str] = None,
    output_reward_field: str = "reward",
) -> str:
    """Add calibrated rewards to existing data using pre-fitted calibrator.

    Useful when you've already calibrated on one dataset and want to
    apply the same calibration to new data.

    Args:
        data_path: Path to JSONL file with judge scores
        calibrator: Pre-fitted JudgeCalibrator
        judge_score_field: Field containing judge scores
        output_path: Where to save (defaults to data_path with .rewards suffix)
        output_reward_field: Field name for calibrated rewards

    Returns:
        Path to output file
    """
    # Load raw data to preserve all fields
    raw_data = []
    with open(data_path, "r") as f:
        for line in f:
            raw_data.append(json.loads(line))

    # Also load through dataset for validation
    from cje import load_dataset_from_jsonl
    from .loaders import DatasetLoader

    dataset = load_dataset_from_jsonl(data_path)

    # Helper function to derive prompt_id consistently with DatasetLoader
    def derive_prompt_id(record: Dict, idx: int, prompt_field: str = "prompt") -> str:
        """Derive prompt_id using same logic as DatasetLoader."""
        # Check top-level first
        prompt_id = record.get("prompt_id")
        if prompt_id is not None:
            return str(prompt_id)

        # Check metadata
        prompt_id = record.get("metadata", {}).get("prompt_id")
        if prompt_id is not None:
            return str(prompt_id)

        # Generate from prompt hash
        prompt = record.get(prompt_field, "")
        if prompt:
            import hashlib

            prompt_hash = hashlib.sha256(prompt.encode()).hexdigest()[:12]
            return f"prompt_{prompt_hash}"

        # Fallback to index
        return f"sample_{idx:06d}"

    # Build mapping from prompt_id -> (judge_score, sample)
    prompt_to_sample = {}
    for sample in dataset.samples:
        # Get judge score from sample
        if judge_score_field in sample.metadata:
            score = sample.metadata[judge_score_field]
        else:
            # Will need to get from raw data later
            score = None

        prompt_to_sample[sample.prompt_id] = (score, sample)

    # Collect judge scores aligned with dataset samples
    judge_scores = []
    sample_ids = []
    for sample in dataset.samples:
        score, _ = prompt_to_sample[sample.prompt_id]

        # If score not in metadata, try to find in raw data
        if score is None:
            # Find matching raw record by prompt_id
            for idx, raw_record in enumerate(raw_data):
                raw_prompt_id = derive_prompt_id(raw_record, idx)
                if raw_prompt_id == sample.prompt_id:
                    if judge_score_field in raw_record:
                        score = raw_record[judge_score_field]
                    break

        if score is None:
            raise ValueError(
                f"Judge score field '{judge_score_field}' not found for prompt_id {sample.prompt_id}"
            )

        if isinstance(score, dict):
            score = score.get("mean", score.get("value"))

        judge_scores.append(float(score))
        sample_ids.append(sample.prompt_id)

    judge_scores_array = np.array(judge_scores)

    # Apply calibration
    calibrated_rewards = calibrator.predict(judge_scores_array)

    # Build mapping from prompt_id to calibrated reward
    reward_by_id = {
        pid: float(reward) for pid, reward in zip(sample_ids, calibrated_rewards)
    }

    # Add rewards to raw data, matching by prompt_id
    data = []
    skipped_count = 0
    for i, raw_record in enumerate(raw_data):
        # Start from the original raw record to preserve all fields
        record = dict(raw_record)  # Make a copy

        # Derive prompt_id for this raw record
        prompt_id = derive_prompt_id(raw_record, i)

        # Inject the calibrated reward if we have it
        if prompt_id in reward_by_id:
            record[output_reward_field] = reward_by_id[prompt_id]
        else:
            # This record was filtered by the loader, no reward to assign
            skipped_count += 1
            # Optionally set to None to be explicit
            # record[output_reward_field] = None

        data.append(record)

    if skipped_count > 0:
        logger.info(
            f"Skipped adding rewards to {skipped_count} records "
            f"(filtered by DatasetLoader)"
        )

    # Save
    if output_path is None:
        path = Path(data_path)
        output_path = str(path.parent / f"{path.stem}.rewards{path.suffix}")

    save_jsonl(data, output_path)
    return output_path


=== ./cje/data/validation.py ===

"""Data validation utilities for CJE.

This module provides functions to validate that input data has the required
fields for different CJE use cases.
"""

from typing import List, Dict, Any, Optional, Tuple
import logging

logger = logging.getLogger(__name__)


def validate_cje_data(
    data: List[Dict[str, Any]],
    reward_field: Optional[str] = None,
    judge_field: Optional[str] = None,
    oracle_field: Optional[str] = None,
) -> Tuple[bool, List[str]]:
    """Validate that data has required fields for CJE analysis.

    This function checks that the data has the core required fields
    (prompt, response, base_policy_logprob, target_policy_logprobs)
    and appropriate evaluation fields (either reward or judge scores).

    Args:
        data: List of data records to validate
        reward_field: Field name containing pre-calibrated rewards
        judge_field: Field name containing judge scores
        oracle_field: Field name containing oracle labels

    Returns:
        Tuple of (is_valid, list_of_issues)

    Example:
        >>> data = load_jsonl("data.jsonl")
        >>> is_valid, issues = validate_cje_data(
        ...     data,
        ...     judge_field="judge_score",
        ...     oracle_field="oracle_label"
        ... )
        >>> if not is_valid:
        ...     for issue in issues:
        ...         print(f"⚠️  {issue}")
    """
    issues = []

    if not data:
        issues.append("Data is empty")
        return False, issues

    # Check core fields in first sample (assume homogeneous)
    sample = data[0]
    core_fields = [
        "prompt_id",
        "prompt",
        "response",
        "base_policy_logprob",
        "target_policy_logprobs",
    ]

    for field in core_fields:
        if field not in sample:
            issues.append(f"Missing required field: {field}")

    # Check that target_policy_logprobs is a dict
    if "target_policy_logprobs" in sample:
        if not isinstance(sample["target_policy_logprobs"], dict):
            issues.append("target_policy_logprobs must be a dictionary")
        elif not sample["target_policy_logprobs"]:
            issues.append("target_policy_logprobs cannot be empty")

    # Check evaluation fields - scan a larger sample for robustness
    # Check up to 100 samples or 10% of data, whichever is smaller
    sample_size = min(100, max(10, len(data) // 10))
    has_reward = reward_field and reward_field in sample

    # Check for judge field - accept it either at top level or in metadata
    # Also validate that values are numeric and non-None
    judge_samples_checked = 0
    valid_judge_samples = 0
    invalid_judge_values = []

    if judge_field:
        for i, rec in enumerate(data[:sample_size]):
            judge_samples_checked += 1
            judge_val = None

            # Check top level first
            if judge_field in rec:
                judge_val = rec[judge_field]
            # Then check metadata
            elif "metadata" in rec and judge_field in rec["metadata"]:
                judge_val = rec["metadata"][judge_field]

            # Validate the value
            if judge_val is not None:
                if isinstance(judge_val, (int, float)):
                    valid_judge_samples += 1
                else:
                    invalid_judge_values.append((i, type(judge_val).__name__))

    has_judge = judge_field and (valid_judge_samples > 0)

    # Report invalid judge values if found
    if invalid_judge_values:
        examples = invalid_judge_values[:3]  # Show first 3 examples
        issues.append(
            f"Judge field '{judge_field}' has non-numeric values. "
            f"Examples: {examples}. Values must be numeric (int or float)."
        )

    if not has_reward and not has_judge:
        issues.append(
            "No evaluation field found. Need either:\n"
            "  - A 'reward' field with pre-calibrated values, OR\n"
            "  - Judge scores in metadata for calibration"
        )

    # If using judge scores, oracle labels are REQUIRED for calibration
    if has_judge and not has_reward:
        # Judge scores without rewards require oracle labels for calibration
        if not oracle_field:
            issues.append(
                "Judge scores require oracle labels for calibration. "
                "Provide oracle_field parameter pointing to oracle labels."
            )
        else:
            # Check for oracle labels - accept at top level or in metadata
            # Also validate that values are numeric
            oracle_count = 0
            invalid_oracle_values = []

            for i, rec in enumerate(data):
                oracle_val = None

                # Check top level first
                if oracle_field in rec:
                    oracle_val = rec[oracle_field]
                # Then check metadata
                elif "metadata" in rec and oracle_field in rec["metadata"]:
                    oracle_val = rec["metadata"][oracle_field]

                # Validate the value
                if oracle_val is not None:
                    if isinstance(oracle_val, (int, float)):
                        oracle_count += 1
                    else:
                        invalid_oracle_values.append((i, type(oracle_val).__name__))

            # Report invalid oracle values if found
            if invalid_oracle_values:
                examples = invalid_oracle_values[:3]  # Show first 3 examples
                issues.append(
                    f"Oracle field '{oracle_field}' has non-numeric values. "
                    f"Examples: {examples}. Values must be numeric (int or float)."
                )

            if oracle_count == 0:
                issues.append(
                    f"No valid oracle labels found in field '{oracle_field}'. "
                    "Judge scores require oracle labels for calibration. "
                    "Need at least 10 samples with oracle labels (50-100 recommended). "
                    "Check that oracle values are numeric and non-None."
                )
            elif oracle_count < 10:
                issues.append(
                    f"Too few oracle samples ({oracle_count}). "
                    "Absolute minimum is 10 samples. "
                    "Strongly recommend 50-100+ for robust calibration."
                )
            elif oracle_count < 50:
                logger.warning(
                    f"Found {oracle_count} oracle samples. "
                    f"Consider adding more (50-100 recommended) for better calibration."
                )
            else:
                logger.info(f"Found {oracle_count} oracle samples for calibration")

    # Check data consistency across samples
    n_samples = len(data)
    valid_base_lp = sum(1 for rec in data if rec.get("base_policy_logprob") is not None)

    if valid_base_lp < n_samples:
        pct_missing = 100 * (n_samples - valid_base_lp) / n_samples
        issues.append(
            f"{n_samples - valid_base_lp}/{n_samples} samples "
            f"({pct_missing:.1f}%) have missing base_policy_logprob"
        )

    # Check target policies consistency
    if data and "target_policy_logprobs" in data[0]:
        first_policies = set(data[0]["target_policy_logprobs"].keys())
        inconsistent = []

        for i, rec in enumerate(data[1:11], 1):  # Check first 10
            if "target_policy_logprobs" in rec:
                policies = set(rec["target_policy_logprobs"].keys())
                if policies != first_policies:
                    inconsistent.append(i)

        if inconsistent:
            issues.append(
                f"Inconsistent target policies in samples {inconsistent}. "
                f"Expected: {first_policies}"
            )

    is_valid = len(issues) == 0
    return is_valid, issues


def validate_for_precomputed_sampler(
    data: List[Dict[str, Any]], reward_field: str = "reward"
) -> Tuple[bool, List[str]]:
    """Validate data specifically for PrecomputedSampler.

    PrecomputedSampler requires rewards to be already present,
    either as pre-calibrated values or from judge calibration.

    Args:
        data: List of data records
        reward_field: Field name containing rewards

    Returns:
        Tuple of (is_valid, list_of_issues)
    """
    issues = []

    # First check basic CJE requirements including the reward field
    is_valid, base_issues = validate_cje_data(data, reward_field=reward_field)
    issues.extend(base_issues)

    # Check for rewards
    if not data:
        issues.append("Data is empty")
        return False, issues

    has_rewards = all(
        reward_field in rec and rec[reward_field] is not None
        for rec in data[: min(100, len(data))]
    )

    if not has_rewards:
        issues.append(
            f"PrecomputedSampler requires '{reward_field}' field. "
            "Either provide pre-calibrated rewards or use calibrate_dataset() first."
        )

    return len(issues) == 0, issues


=== ./cje/diagnostics/__init__.py ===

"""CJE Diagnostics System.

Consolidated module for all diagnostic functionality:
- Data models (IPSDiagnostics, DRDiagnostics)
- Weight diagnostics computation
- DR-specific diagnostics
- Stability and drift detection
- Display utilities
- Robust inference tools
"""

# Data models
from .models import (
    IPSDiagnostics,
    DRDiagnostics,
    Status,
)

# Weight diagnostics
from .weights import (
    compute_weight_diagnostics,
    effective_sample_size,
    compute_ess,
    hill_tail_index,
    hill_tail_index_stable,
    tail_weight_ratio,
    mass_concentration,
)

# DR diagnostics
from .dr import (
    compute_dr_policy_diagnostics,
    compute_dr_diagnostics_all,
    compute_dm_ips_decomposition,
    compute_orthogonality_score,
)

# Stability diagnostics
from .stability import (
    kendall_tau_drift,
    sequential_drift_detection,
    reliability_diagram,
    eif_qq_plot_data,
    compute_stability_diagnostics,
)

# Display utilities
from .display import (
    create_weight_summary_table,
    format_dr_diagnostic_summary,
    format_diagnostic_comparison,
)

# Robust inference
from .robust_inference import (
    stationary_bootstrap_se,
    moving_block_bootstrap_se,
    cluster_robust_se,
    benjamini_hochberg_correction,
    compute_simultaneous_bands,
    compute_robust_inference,
)

__all__ = [
    # Data models
    "IPSDiagnostics",
    "DRDiagnostics",
    "Status",
    # Weight diagnostics
    "compute_weight_diagnostics",
    "effective_sample_size",
    "compute_ess",
    "hill_tail_index",
    "hill_tail_index_stable",
    "tail_weight_ratio",
    "mass_concentration",
    # DR diagnostics
    "compute_dr_policy_diagnostics",
    "compute_dr_diagnostics_all",
    "compute_dm_ips_decomposition",
    "compute_orthogonality_score",
    # Stability
    "kendall_tau_drift",
    "sequential_drift_detection",
    "reliability_diagram",
    "eif_qq_plot_data",
    "compute_stability_diagnostics",
    # Display
    "create_weight_summary_table",
    "format_dr_diagnostic_summary",
    "format_diagnostic_comparison",
    # Robust inference
    "stationary_bootstrap_se",
    "moving_block_bootstrap_se",
    "cluster_robust_se",
    "benjamini_hochberg_correction",
    "compute_simultaneous_bands",
    "compute_robust_inference",
]


=== ./cje/diagnostics/display.py ===

"""
Display and formatting utilities for diagnostics.

Updated to work with both new diagnostic objects and legacy dictionaries.
"""

from typing import Dict, Any, Union, Optional, TYPE_CHECKING
import numpy as np

if TYPE_CHECKING:
    from .models import IPSDiagnostics, DRDiagnostics


def create_weight_summary_table(
    all_diagnostics: Union[Dict[str, Any], "IPSDiagnostics", "DRDiagnostics"],
) -> str:
    """Create a formatted table of weight diagnostics.

    Args:
        all_diagnostics: Either:
            - Dictionary of diagnostic values by policy
            - IPSDiagnostics or DRDiagnostics object

    Returns:
        Formatted table string
    """
    # Import here to avoid circular dependency
    from .models import IPSDiagnostics, DRDiagnostics

    lines = []
    lines.append("\nWeight Summary")
    lines.append("-" * 70)
    lines.append(f"{'Policy':<30} {'ESS':>8} {'Max Weight':>12} {'Status':<10}")
    lines.append("-" * 70)

    # Handle new diagnostic objects
    if isinstance(all_diagnostics, (IPSDiagnostics, DRDiagnostics)):
        diag = all_diagnostics
        for policy in diag.policies:
            ess = diag.ess_per_policy.get(policy, 0.0)
            max_w = diag.max_weight_per_policy.get(policy, 1.0)
            # Use per-policy status if available
            if (
                hasattr(diag, "status_per_policy")
                and diag.status_per_policy
                and policy in diag.status_per_policy
            ):
                status = diag.status_per_policy[policy].value
            else:
                # Fallback: Determine status based on ESS
                if ess > 0.5:
                    status = "GOOD"
                elif ess > 0.2:
                    status = "WARNING"
                else:
                    status = "CRITICAL"

            lines.append(f"{policy:<30} {ess:>7.1%} {max_w:>12.4f} {status:<10}")

        # Add overall summary
        lines.append("-" * 70)
        lines.append(
            f"{'Overall':<30} {diag.weight_ess:>7.1%} "
            f"{max(diag.max_weight_per_policy.values()):>12.4f} "
            f"{diag.weight_status.value:<10}"
        )

    # Handle legacy dictionary format
    elif isinstance(all_diagnostics, dict):
        for policy, diag in all_diagnostics.items():
            # Handle WeightDiagnostics objects or dict format
            if hasattr(diag, "ess_fraction"):  # WeightDiagnostics object
                ess = getattr(diag, "ess_fraction", 0.0)
                max_w = getattr(diag, "max_weight", 1.0)
                # Try both 'status' and 'consistency_flag' for compatibility
                status = getattr(
                    diag, "status", getattr(diag, "consistency_flag", "UNKNOWN")
                )
            elif isinstance(diag, dict):  # Legacy dict format
                ess = diag.get("ess_fraction", 0.0)
                max_w = diag.get("max_weight", 1.0)
                # Try both 'status' and 'consistency_flag' for compatibility
                status_val = diag.get("status", diag.get("consistency_flag", "UNKNOWN"))
                # Handle Status enum if present
                if hasattr(status_val, "value"):
                    status = status_val.value.upper()
                else:
                    status = str(status_val)
            else:
                ess = 0.0
                max_w = 1.0
                status = "UNKNOWN"

            lines.append(f"{policy:<30} {ess:>7.1%} {max_w:>12.4f} {status:<10}")

    return "\n".join(lines)


def format_dr_diagnostic_summary(
    diagnostics: Union[Dict[str, Any], "DRDiagnostics"],
) -> str:
    """Format DR diagnostics as a readable summary table.

    Args:
        diagnostics: Either:
            - Dictionary of DR diagnostics by policy (legacy)
            - DRDiagnostics object (new)

    Returns:
        Formatted summary string
    """
    # Import here to avoid circular dependency
    from .models import DRDiagnostics

    lines = []
    lines.append("=" * 100)
    lines.append("DR DIAGNOSTICS SUMMARY")
    lines.append("=" * 100)

    # Check if we have a DRDiagnostics object
    if isinstance(diagnostics, DRDiagnostics):
        # New diagnostic object
        lines.append(
            f"Estimator: {diagnostics.estimator_type} | "
            f"Method: {diagnostics.method} | "
            f"Cross-fitted: {diagnostics.dr_cross_fitted} ({diagnostics.dr_n_folds} folds)"
        )
        lines.append(
            f"Samples: {diagnostics.n_samples_valid}/{diagnostics.n_samples_total} | "
            f"Weight ESS: {diagnostics.weight_ess:.1%} | "
            f"Status: {diagnostics.overall_status.value}"
        )
        lines.append("-" * 100)

        # Header
        lines.append(
            f"{'Policy':<20} {'Estimate±SE':<20} "
            f"{'DM Mean':>10} {'IPS Corr':>10} {'IF Tail':>10}"
        )
        lines.append("-" * 100)

        # Per-policy rows
        for policy in diagnostics.policies:
            est = diagnostics.estimates.get(policy, 0.0)
            se = diagnostics.standard_errors.get(policy, 0.0)

            # Get detailed diagnostics if available
            policy_diag = diagnostics.get_policy_diagnostics(policy)
            if policy_diag:
                dm = policy_diag.get("dm_mean", 0.0)
                ips_corr = policy_diag.get("ips_corr_mean", 0.0)
                if_tail = policy_diag.get("if_tail_ratio_99_5", 0.0)
            else:
                # Try decomposition if policy diagnostics not available
                if (
                    diagnostics.dm_ips_decompositions
                    and policy in diagnostics.dm_ips_decompositions
                ):
                    decomp = diagnostics.dm_ips_decompositions[policy]
                    dm = decomp.get("dm_component", 0.0)
                    ips_corr = decomp.get("ips_augmentation", 0.0)
                else:
                    dm = 0.0
                    ips_corr = 0.0
                if_tail = 0.0

            lines.append(
                f"{policy:<20} {est:>7.3f}±{se:.3f}  "
                f"{dm:>10.3f} {ips_corr:>10.3f} {if_tail:>10.1f}"
            )

        lines.append("-" * 100)

        # Summary statistics
        min_r2, max_r2 = diagnostics.outcome_r2_range
        lines.append(
            f"Outcome R² range: [{min_r2:.3f}, {max_r2:.3f}] | "
            f"RMSE: {diagnostics.outcome_rmse_mean:.3f} | "
            f"Worst IF tail: {diagnostics.worst_if_tail_ratio:.1f}"
        )

        # Check if influence functions are stored
        if diagnostics.has_influence_functions():
            n_ifs = (
                len(diagnostics.influence_functions)
                if diagnostics.influence_functions
                else 0
            )
            lines.append(f"Influence functions stored for {n_ifs} policies")

    # Handle legacy dictionary format
    elif isinstance(diagnostics, dict):
        # Check if it's the old format with per_policy key
        if "per_policy" in diagnostics:
            diagnostics = diagnostics["per_policy"]

        # Header
        lines.append(
            f"{'Policy':<20} {'DM':>7} {'IPS':>7} {'DR±SE':<20} "
            f"{'Score(mean±se, p)':<25} {'RMSE(R,g)':>10} {'|IF| tail(p99/p5)':>17}"
        )
        lines.append("-" * 100)

        # Per-policy rows
        worst_if_tail = 0.0
        r2_values = []
        max_score_z = 0.0

        if not isinstance(diagnostics, dict):
            return "\n".join(lines)

        for policy, diag in diagnostics.items():
            if isinstance(diag, dict):
                dm = diag.get("dm_mean", 0.0)
                ips = diag.get("ips_corr_mean", 0.0)
                dr = diag.get("dr_estimate", 0.0)

                # Standard error (from influence functions if available)
                if "if_std" in diag and "n_samples" in diag:
                    se = diag["if_std"] / np.sqrt(diag["n_samples"])
                else:
                    se = 0.0

                # Score test (for TMLE)
                score_mean = diag.get("score_mean", 0.0)
                score_se = diag.get("score_se", 0.0)
                score_p = diag.get("score_p", 1.0)
                score_str = f"{score_mean:>7.3f}±{score_se:.3f} (p={score_p:.2f})"

                # Outcome model RMSE
                rmse = diag.get("residual_rmse", np.nan)

                # IF tail ratio
                if_tail = diag.get("if_tail_ratio_99_5", 0.0)

                lines.append(
                    f"{policy:<20} {dm:>7.3f} {ips:>7.3f} {dr:>7.3f}±{se:.3f}  "
                    f"{score_str:<25} {rmse:>10.3f} {if_tail:>17.1f}"
                )

                # Track worst metrics
                worst_if_tail = max(worst_if_tail, if_tail)
                if "r2_oof" in diag and not np.isnan(diag["r2_oof"]):
                    r2_values.append(diag["r2_oof"])
                if "score_z" in diag:
                    max_score_z = max(max_score_z, abs(diag["score_z"]))

        lines.append("-" * 100)

        # Summary statistics
        lines.append(f"Worst IF tail ratio (p99/p5): {worst_if_tail:.1f}")
        if r2_values:
            lines.append(f"R² OOF range: [{min(r2_values):.3f}, {max(r2_values):.3f}]")

        # TMLE-specific
        if isinstance(diagnostics, dict) and "tmle_max_score_z" in diagnostics:
            lines.append(
                f"TMLE max |score z|: {diagnostics['tmle_max_score_z']:.2f} (should be ~0)"
            )
        elif max_score_z > 0:
            lines.append(f"TMLE max |score z|: {max_score_z:.2f} (should be ~0)")

    lines.append("=" * 100)

    # Warnings (works for both formats)
    if isinstance(diagnostics, DRDiagnostics):
        worst_if_tail = diagnostics.worst_if_tail_ratio
    elif isinstance(diagnostics, dict):
        worst_if_tail = (
            max(
                diag.get("if_tail_ratio_99_5", 0.0)
                for diag in diagnostics.values()
                if isinstance(diag, dict)
            )
            if diagnostics
            else 0.0
        )
    else:
        worst_if_tail = 0.0

    if worst_if_tail > 100:
        lines.append("\n⚠️  Warning: Heavy-tailed influence functions detected")
        lines.append("   Consider using more fresh draws or checking policy overlap")

    return "\n".join(lines)


def format_diagnostic_comparison(
    diag1: Union["IPSDiagnostics", "DRDiagnostics"],
    diag2: Union["IPSDiagnostics", "DRDiagnostics"],
    label1: str = "Run 1",
    label2: str = "Run 2",
) -> str:
    """Compare two diagnostic objects side by side.

    Args:
        diag1: First diagnostic object
        diag2: Second diagnostic object
        label1: Label for first run
        label2: Label for second run

    Returns:
        Formatted comparison table
    """
    lines = []
    lines.append("=" * 80)
    lines.append(f"DIAGNOSTIC COMPARISON: {label1} vs {label2}")
    lines.append("=" * 80)

    # Basic info
    lines.append(f"{'Metric':<30} {label1:>20} {label2:>20} {'Δ':>10}")
    lines.append("-" * 80)

    # Sample counts
    lines.append(
        f"{'Total samples':<30} {diag1.n_samples_total:>20d} "
        f"{diag2.n_samples_total:>20d} "
        f"{diag2.n_samples_total - diag1.n_samples_total:>+10d}"
    )
    lines.append(
        f"{'Valid samples':<30} {diag1.n_samples_valid:>20d} "
        f"{diag2.n_samples_valid:>20d} "
        f"{diag2.n_samples_valid - diag1.n_samples_valid:>+10d}"
    )

    # ESS
    lines.append(
        f"{'Weight ESS':<30} {diag1.weight_ess:>20.1%} "
        f"{diag2.weight_ess:>20.1%} "
        f"{100*(diag2.weight_ess - diag1.weight_ess):>+9.1f}%"
    )

    # Calibration (if available)
    if diag1.is_calibrated and diag2.is_calibrated:
        r2_1 = diag1.calibration_r2 if diag1.calibration_r2 is not None else 0.0
        r2_2 = diag2.calibration_r2 if diag2.calibration_r2 is not None else 0.0
        lines.append(
            f"{'Calibration R²':<30} {r2_1:>20.3f} "
            f"{r2_2:>20.3f} "
            f"{r2_2 - r2_1:>+10.3f}"
        )

    # DR-specific (if both are DR)
    from .models import DRDiagnostics

    if isinstance(diag1, DRDiagnostics) and isinstance(diag2, DRDiagnostics):
        lines.append("-" * 80)
        lines.append("DR-specific metrics:")

        # Outcome R²
        min1, max1 = diag1.outcome_r2_range
        min2, max2 = diag2.outcome_r2_range
        lines.append(
            f"{'Outcome R² (min)':<30} {min1:>20.3f} "
            f"{min2:>20.3f} {min2 - min1:>+10.3f}"
        )
        lines.append(
            f"{'Outcome R² (max)':<30} {max1:>20.3f} "
            f"{max2:>20.3f} {max2 - max1:>+10.3f}"
        )

        # IF tail ratio
        lines.append(
            f"{'Worst IF tail ratio':<30} {diag1.worst_if_tail_ratio:>20.1f} "
            f"{diag2.worst_if_tail_ratio:>20.1f} "
            f"{diag2.worst_if_tail_ratio - diag1.worst_if_tail_ratio:>+10.1f}"
        )

    # Per-policy comparison
    lines.append("-" * 80)
    lines.append("Per-policy estimates:")

    common_policies = set(diag1.policies) & set(diag2.policies)
    for policy in sorted(common_policies):
        est1 = diag1.estimates.get(policy, 0.0)
        est2 = diag2.estimates.get(policy, 0.0)
        se1 = diag1.standard_errors.get(policy, 0.0)
        se2 = diag2.standard_errors.get(policy, 0.0)

        lines.append(
            f"{policy:<30} {est1:>8.3f}±{se1:.3f} "
            f"{est2:>8.3f}±{se2:.3f} "
            f"{est2 - est1:>+10.3f}"
        )

    lines.append("=" * 80)

    return "\n".join(lines)


=== ./cje/diagnostics/dr.py ===

"""
Doubly robust diagnostic computations.
"""

import numpy as np
from typing import Dict, Any, Optional, List
from scipy import stats
import logging

from .weights import tail_weight_ratio, mass_concentration

logger = logging.getLogger(__name__)


def _p_value_from_z(z: float) -> float:
    """Convert z-score to two-sided p-value."""
    return float(2 * (1 - stats.norm.cdf(abs(z))))


def compute_orthogonality_score(
    weights: np.ndarray,
    rewards: np.ndarray,
    outcome_predictions: np.ndarray,
    return_ci: bool = True,
    alpha: float = 0.05,
) -> Dict[str, Any]:
    """Compute the orthogonality score for DR estimation.

    The orthogonality score is E[W * (R - q̂)], which should be zero
    under correct specification and proper cross-fitting.

    Args:
        weights: Importance weights (should be mean-one)
        rewards: Observed rewards
        outcome_predictions: Outcome model predictions q̂(X, A)
        return_ci: Whether to compute confidence interval
        alpha: Significance level for CI (default 0.05 for 95% CI)

    Returns:
        Dictionary with:
        - 'score': The orthogonality score
        - 'se': Standard error of the score
        - 'ci_lower': Lower bound of CI (if return_ci=True)
        - 'ci_upper': Upper bound of CI (if return_ci=True)
        - 'p_value': P-value for test that score = 0
        - 'passes_test': Boolean, True if CI contains 0

    References:
        Section 9.3 of the CJE paper on orthogonality diagnostics.
    """
    n = len(weights)

    # Compute the orthogonality score
    residuals = rewards - outcome_predictions
    score_components = weights * residuals
    score = np.mean(score_components)

    # Compute standard error
    se = np.std(score_components) / np.sqrt(n)

    # Test statistic
    z_stat = score / se if se > 0 else 0
    p_value = _p_value_from_z(z_stat)

    result = {
        "score": float(score),
        "se": float(se),
        "z_statistic": float(z_stat),
        "p_value": float(p_value),
    }

    if return_ci:
        # Critical value for two-sided test
        z_crit = stats.norm.ppf(1 - alpha / 2)
        ci_lower = score - z_crit * se
        ci_upper = score + z_crit * se

        result["ci_lower"] = float(ci_lower)
        result["ci_upper"] = float(ci_upper)
        result["passes_test"] = ci_lower <= 0 <= ci_upper

    return result


def compute_dm_ips_decomposition(
    g_hat: np.ndarray,
    weights: np.ndarray,
    rewards: np.ndarray,
    q_hat: np.ndarray,
) -> Dict[str, Any]:
    """Compute the DM-IPS decomposition for DR estimation.

    Decomposes the DR estimate into:
    - Direct Method (DM): E[ĝ(X)]
    - IPS Augmentation: E[Ŵ * (R - q̂(X, A))]

    Args:
        g_hat: Outcome model predictions under target policy ĝ(X)
        weights: Importance weights Ŵ
        rewards: Observed rewards R
        q_hat: Outcome model predictions under logging policy q̂(X, A)

    Returns:
        Dictionary with:
        - 'dm_component': Direct method estimate
        - 'ips_augmentation': IPS correction term
        - 'total': Total DR estimate
        - 'dm_se': Standard error of DM component
        - 'ips_se': Standard error of IPS component
        - 'dm_contribution': Fraction of estimate from DM
        - 'ips_contribution': Fraction of estimate from IPS
        - 'correlation': Correlation between components
    """
    n = len(weights)

    # DM component
    dm_component = np.mean(g_hat)
    dm_se = np.std(g_hat) / np.sqrt(n)

    # IPS augmentation
    residuals = rewards - q_hat
    ips_terms = weights * residuals
    ips_augmentation = np.mean(ips_terms)
    ips_se = np.std(ips_terms) / np.sqrt(n)

    # Total DR estimate
    total = dm_component + ips_augmentation

    # Component contributions (as fractions)
    if abs(total) > 1e-10:
        dm_contribution = abs(dm_component) / (
            abs(dm_component) + abs(ips_augmentation)
        )
        ips_contribution = abs(ips_augmentation) / (
            abs(dm_component) + abs(ips_augmentation)
        )
    else:
        dm_contribution = 0.5
        ips_contribution = 0.5

    # Correlation between components
    if len(g_hat) > 1:
        corr_matrix = np.corrcoef(g_hat, ips_terms)
        correlation = corr_matrix[0, 1] if not np.isnan(corr_matrix[0, 1]) else 0.0
    else:
        correlation = 0.0

    return {
        "dm_component": float(dm_component),
        "ips_augmentation": float(ips_augmentation),
        "total": float(total),
        "dm_se": float(dm_se),
        "ips_se": float(ips_se),
        "dm_contribution": float(dm_contribution),
        "ips_contribution": float(ips_contribution),
        "correlation": float(correlation),
    }


def compute_dr_policy_diagnostics(
    dm_component: np.ndarray,
    ips_correction: np.ndarray,
    dr_estimate: float,
    fresh_rewards: Optional[np.ndarray] = None,
    outcome_predictions: Optional[np.ndarray] = None,
    influence_functions: Optional[np.ndarray] = None,
    unique_folds: Optional[List[int]] = None,
    policy: str = "unknown",
) -> Dict[str, Any]:
    """Compute comprehensive DR diagnostics for a single policy.

    Args:
        dm_component: Direct method (outcome model) component
        ips_correction: IPS correction component
        dr_estimate: Final DR estimate
        fresh_rewards: Fresh draw rewards (for outcome model R²)
        outcome_predictions: Outcome model predictions
        influence_functions: Per-sample influence functions
        unique_folds: Unique fold IDs used in cross-fitting
        policy: Policy name

    Returns:
        Dictionary with diagnostic metrics
    """
    n = len(dm_component)

    diagnostics = {
        "policy": policy,
        "n_samples": n,
        "dm_mean": float(dm_component.mean()),
        "ips_corr_mean": float(ips_correction.mean()),
        "dr_estimate": float(dr_estimate),
        "dm_std": float(dm_component.std()),
        "ips_corr_std": float(ips_correction.std()),
    }

    # Outcome model fit (if fresh rewards available)
    if fresh_rewards is not None and outcome_predictions is not None:
        mask = ~np.isnan(fresh_rewards) & ~np.isnan(outcome_predictions)
        if mask.sum() > 0:
            residuals = fresh_rewards[mask] - outcome_predictions[mask]
            diagnostics["residual_mean"] = float(residuals.mean())
            diagnostics["residual_std"] = float(residuals.std())
            diagnostics["residual_rmse"] = float(np.sqrt((residuals**2).mean()))

            # R² (out-of-fold if cross-fitted)
            ss_res = np.sum(residuals**2)
            ss_tot = np.sum((fresh_rewards[mask] - fresh_rewards[mask].mean()) ** 2)
            r2 = 1 - ss_res / max(ss_tot, 1e-12)
            diagnostics["r2_oof"] = float(r2)
        else:
            diagnostics["r2_oof"] = np.nan
            diagnostics["residual_rmse"] = np.nan

    # Influence function diagnostics
    if influence_functions is not None:
        diagnostics["if_mean"] = float(influence_functions.mean())
        diagnostics["if_std"] = float(influence_functions.std())
        diagnostics["if_var"] = float(influence_functions.var())

        # Check for heavy tails
        diagnostics["if_tail_ratio_99_5"] = tail_weight_ratio(
            np.abs(influence_functions), 0.05, 0.99
        )
        diagnostics["if_top1_mass"] = mass_concentration(
            np.abs(influence_functions), 0.01
        )

        # Score function test (should be mean zero for TMLE)
        score_mean = influence_functions.mean()
        score_se = influence_functions.std() / np.sqrt(n)
        score_z = score_mean / score_se if score_se > 0 else 0
        diagnostics["score_mean"] = float(score_mean)
        diagnostics["score_se"] = float(score_se)
        diagnostics["score_z"] = float(score_z)
        diagnostics["score_p"] = _p_value_from_z(score_z)

    # Cross-fitting info
    if unique_folds is not None:
        diagnostics["cross_fitted"] = True
        diagnostics["unique_folds"] = len(unique_folds)
    else:
        diagnostics["cross_fitted"] = False
        diagnostics["unique_folds"] = 1

    # Coverage check (do we have enough fresh draws?)
    diagnostics["coverage_ok"] = True
    if fresh_rewards is not None:
        coverage = (~np.isnan(fresh_rewards)).mean()
        diagnostics["fresh_draw_coverage"] = float(coverage)
        if coverage < 0.8:
            diagnostics["coverage_ok"] = False
            logger.warning(f"Low fresh draw coverage for {policy}: {coverage:.1%}")

    # Component correlation (ideally low for orthogonality)
    corr = np.corrcoef(dm_component, ips_correction)[0, 1]
    diagnostics["component_correlation"] = float(corr) if not np.isnan(corr) else 0.0

    return diagnostics


def compute_dr_diagnostics_all(
    estimator: Any,
    influence_functions: Optional[Dict[str, np.ndarray]] = None,
) -> Dict[str, Any]:
    """Compute DR diagnostics for all policies.

    Args:
        estimator: A DR estimator with _dm_component and _ips_correction
        influence_functions: Optional dict of influence functions by policy

    Returns:
        Dictionary with per-policy diagnostics and summary metrics
    """
    all_diagnostics = {}

    for policy in estimator.sampler.target_policies:
        # Get components
        dm = estimator._dm_component.get(policy)
        ips = estimator._ips_correction.get(policy)

        if dm is None or ips is None:
            logger.warning(f"Missing DR components for {policy}")
            continue

        # Get optional data
        fresh_rewards = None
        outcome_preds = None
        if hasattr(estimator, "_fresh_rewards"):
            fresh_rewards = estimator._fresh_rewards.get(policy)
        if hasattr(estimator, "_outcome_predictions"):
            outcome_preds = estimator._outcome_predictions.get(policy)

        # Get influence functions if available
        ifs = None
        if influence_functions and policy in influence_functions:
            ifs = influence_functions[policy]

        # Compute diagnostics
        all_diagnostics[policy] = compute_dr_policy_diagnostics(
            dm_component=dm,
            ips_correction=ips,
            dr_estimate=dm.mean() + ips.mean(),
            fresh_rewards=fresh_rewards,
            outcome_predictions=outcome_preds,
            influence_functions=ifs,
            policy=policy,
        )

    return all_diagnostics


=== ./cje/diagnostics/models.py ===

"""
Diagnostic data models for CJE.

This module contains the data structures for diagnostics.
Computation logic is in utils/diagnostics/.
"""

from dataclasses import dataclass, field
from typing import Dict, Optional, List, Any, Tuple
from enum import Enum
import numpy as np


class Status(Enum):
    """Health status for diagnostics."""

    GOOD = "good"
    WARNING = "warning"
    CRITICAL = "critical"


@dataclass
class IPSDiagnostics:
    """Diagnostics for IPS-based estimators (CalibratedIPS in both raw and calibrated modes)."""

    # ========== Core Info (always present) ==========
    estimator_type: str  # "CalibratedIPS"
    method: str
    n_samples_total: int
    n_samples_valid: int
    n_policies: int
    policies: List[str]

    # ========== Estimation Results (always present) ==========
    estimates: Dict[str, float]
    standard_errors: Dict[str, float]
    n_samples_used: Dict[str, int]

    # ========== Weight Diagnostics (always present) ==========
    weight_ess: float  # Overall effective sample size fraction
    weight_status: Status

    # Per-policy weight metrics
    ess_per_policy: Dict[str, float]
    max_weight_per_policy: Dict[str, float]
    status_per_policy: Optional[Dict[str, Status]] = None  # Per-policy status
    weight_tail_ratio_per_policy: Optional[Dict[str, float]] = (
        None  # DEPRECATED: Use tail_indices
    )
    tail_indices: Optional[Dict[str, Optional[float]]] = (
        None  # Hill tail index per policy
    )

    # ========== Overlap Metrics (new comprehensive diagnostics) ==========
    hellinger_affinity: Optional[float] = None  # Overall Hellinger affinity
    hellinger_per_policy: Optional[Dict[str, float]] = None  # Per-policy Hellinger
    overlap_quality: Optional[str] = None  # "good", "marginal", "poor", "catastrophic"

    # ========== Calibration Diagnostics (None for raw mode) ==========
    calibration_rmse: Optional[float] = None
    calibration_r2: Optional[float] = None
    calibration_coverage: Optional[float] = None  # P(|pred - oracle| < 0.1)
    n_oracle_labels: Optional[int] = None

    # ========== Computed Properties ==========

    @property
    def filter_rate(self) -> float:
        """Fraction of samples filtered out."""
        if self.n_samples_total > 0:
            return 1.0 - (self.n_samples_valid / self.n_samples_total)
        return 0.0

    @property
    def best_policy(self) -> str:
        """Policy with highest estimate."""
        if not self.estimates:
            return "none"
        return max(self.estimates.items(), key=lambda x: x[1])[0]

    @property
    def worst_weight_tail_ratio(self) -> float:
        """Worst tail ratio across policies.

        DEPRECATED: Use worst_tail_index instead.
        """
        if self.weight_tail_ratio_per_policy:
            return max(self.weight_tail_ratio_per_policy.values())
        return 0.0

    @property
    def worst_tail_index(self) -> Optional[float]:
        """Lowest (worst) Hill tail index across policies."""
        if self.tail_indices:
            valid_indices = [
                idx for idx in self.tail_indices.values() if idx is not None
            ]
            if valid_indices:
                return min(valid_indices)
        return None

    @property
    def is_calibrated(self) -> bool:
        """Check if this has calibration info."""
        return self.calibration_rmse is not None

    @property
    def overall_status(self) -> Status:
        """Overall health status based on diagnostics."""
        # Start with weight status
        if self.weight_status == Status.CRITICAL:
            return Status.CRITICAL
        elif self.weight_status == Status.WARNING:
            return Status.WARNING

        # Check calibration if present
        if self.is_calibrated:
            if self.calibration_r2 is not None and self.calibration_r2 < 0:
                return Status.CRITICAL
            elif self.calibration_r2 is not None and self.calibration_r2 < 0.5:
                return Status.WARNING

        return Status.GOOD

    def validate(self) -> List[str]:
        """Run self-consistency checks."""
        issues = []

        # Basic sanity checks
        if self.n_samples_valid > self.n_samples_total:
            issues.append(
                f"n_valid ({self.n_samples_valid}) > n_total ({self.n_samples_total})"
            )

        # Check for high filter rate
        if self.filter_rate > 0.5:
            issues.append(
                f"High filter rate: {self.filter_rate:.1%} of samples filtered"
            )

        # ESS should be <= 1 and check for low ESS
        if self.weight_ess > 1.0:
            issues.append(f"ESS fraction > 1.0: {self.weight_ess}")
        elif self.weight_ess < 0.1:
            issues.append(f"Very low ESS: {self.weight_ess:.1%}")

        for policy, ess in self.ess_per_policy.items():
            if ess > 1.0:
                issues.append(f"ESS fraction > 1.0 for {policy}: {ess}")
            elif ess < 0.1:
                issues.append(f"Low ESS for {policy}: {ess:.1%}")

        # Check for extreme weights
        for policy, max_w in self.max_weight_per_policy.items():
            if max_w > 100:
                issues.append(f"Extreme max weight for {policy}: {max_w:.1f}")

        # Check for heavy tails using Hill index
        if self.tail_indices:
            for policy, tail_idx in self.tail_indices.items():
                if tail_idx is not None:
                    if tail_idx < 1.5:
                        issues.append(
                            f"Extremely heavy tail for {policy}: α={tail_idx:.2f} (infinite mean risk)"
                        )
                    elif tail_idx < 2.0:
                        issues.append(
                            f"Heavy tail for {policy}: α={tail_idx:.2f} (infinite variance)"
                        )
        # Fallback to deprecated tail ratio if available
        elif self.weight_tail_ratio_per_policy:
            for policy, tail_ratio in self.weight_tail_ratio_per_policy.items():
                if tail_ratio > 100:
                    issues.append(f"Heavy tail for {policy}: ratio={tail_ratio:.1f}")

        # R² should be <= 1
        if self.calibration_r2 is not None and self.calibration_r2 > 1.0:
            issues.append(f"Calibration R² > 1.0: {self.calibration_r2}")

        # Check estimates match policies
        for policy in self.estimates:
            if policy not in self.policies:
                issues.append(f"Estimate for unknown policy: {policy}")

        return issues

    def summary(self) -> str:
        """Generate concise summary."""
        lines = [
            f"Estimator: {self.estimator_type}",
            f"Method: {self.method}",
            f"Status: {self.overall_status.value}",
            f"Samples: {self.n_samples_valid}/{self.n_samples_total} valid ({100*(1-self.filter_rate):.1f}%)",
            f"Policies: {', '.join(self.policies)}",
            f"Best policy: {self.best_policy}",
            f"Weight ESS: {self.weight_ess:.1%}",
        ]

        if self.is_calibrated:
            lines.append(f"Calibration RMSE: {self.calibration_rmse:.3f}")
            if self.calibration_r2 is not None:
                lines.append(f"Calibration R²: {self.calibration_r2:.3f}")

        # Add any validation issues
        issues = self.validate()
        if issues:
            lines.append("Issues: " + "; ".join(issues[:2]))

        return " | ".join(lines)

    def to_dict(self) -> Dict:
        """Export as dictionary for serialization."""
        from dataclasses import asdict

        d = asdict(self)
        # Convert enums to strings
        d["weight_status"] = self.weight_status.value
        d["overall_status"] = self.overall_status.value

        # Convert status_per_policy if present
        if d.get("status_per_policy"):
            d["status_per_policy"] = {
                policy: status.value if hasattr(status, "value") else status
                for policy, status in d["status_per_policy"].items()
            }

        return d

    def to_json(self, indent: int = 2) -> str:
        """Convert to JSON string."""
        import json

        return json.dumps(self.to_dict(), indent=indent, default=str)

    @classmethod
    def from_dict(cls, data: Dict) -> "IPSDiagnostics":
        """Create from dictionary."""
        # Convert status strings back to enum
        if "weight_status" in data and isinstance(data["weight_status"], str):
            data["weight_status"] = Status(data["weight_status"])
        # Remove computed fields that aren't in the constructor
        data.pop("overall_status", None)
        return cls(**data)

    def to_csv_row(self) -> Dict[str, Any]:
        """Export key metrics as a flat dict for CSV export."""
        row = {
            "estimator": self.estimator_type,
            "method": self.method,
            "n_samples_total": self.n_samples_total,
            "n_samples_valid": self.n_samples_valid,
            "filter_rate": self.filter_rate,
            "weight_ess": self.weight_ess,
            "weight_status": self.weight_status.value,
            "n_policies": self.n_policies,
            "best_policy": self.best_policy if self.policies else None,
            "worst_tail_ratio": self.worst_weight_tail_ratio,
        }
        # Add per-policy metrics
        for policy in self.policies:
            row[f"{policy}_estimate"] = self.estimates.get(policy)
            row[f"{policy}_se"] = self.standard_errors.get(policy)
            row[f"{policy}_ess"] = self.ess_per_policy.get(policy)
        # Add calibration metrics if available
        if self.calibration_rmse is not None:
            row["calibration_rmse"] = self.calibration_rmse
            row["calibration_r2"] = self.calibration_r2
        return row


@dataclass
class DRDiagnostics(IPSDiagnostics):
    """Diagnostics for DR estimators, extending IPS diagnostics."""

    # ========== DR-specific fields ==========
    dr_cross_fitted: bool = True
    dr_n_folds: int = 5

    # Outcome model performance summary
    outcome_r2_range: Tuple[float, float] = (0.0, 0.0)  # (min, max) across policies
    outcome_rmse_mean: float = 0.0  # Average RMSE across policies

    # Influence function summary
    worst_if_tail_ratio: float = 0.0  # Worst p99/p5 ratio across policies

    # Detailed per-policy diagnostics (for visualization and debugging)
    dr_diagnostics_per_policy: Dict[str, Dict[str, Any]] = field(default_factory=dict)

    # DR decomposition results
    dm_ips_decompositions: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    orthogonality_scores: Dict[str, Dict[str, Any]] = field(default_factory=dict)

    # Optional influence functions (can be large)
    influence_functions: Optional[Dict[str, np.ndarray]] = None

    # ========== Computed Properties (override parent) ==========

    @property
    def overall_status(self) -> Status:
        """Overall health status including DR-specific checks."""
        # Start with parent status
        parent_status = super().overall_status
        if parent_status == Status.CRITICAL:
            return Status.CRITICAL

        statuses: List[Status] = [parent_status]

        # Check outcome model R²
        min_r2, max_r2 = self.outcome_r2_range
        if min_r2 < 0:
            statuses.append(Status.CRITICAL)
        elif min_r2 < 0.1:
            statuses.append(Status.WARNING)

        # Check influence function tails
        if self.worst_if_tail_ratio > 1000:
            statuses.append(Status.CRITICAL)
        elif self.worst_if_tail_ratio > 100:
            statuses.append(Status.WARNING)

        # Return worst status
        if Status.CRITICAL in statuses:
            return Status.CRITICAL
        if Status.WARNING in statuses:
            return Status.WARNING
        return Status.GOOD

    def validate(self) -> List[str]:
        """Run self-consistency checks including DR-specific ones."""
        issues = super().validate()

        # Check outcome R² range
        min_r2, max_r2 = self.outcome_r2_range
        if min_r2 > max_r2:
            issues.append(f"Invalid R² range: [{min_r2:.3f}, {max_r2:.3f}]")
        if max_r2 > 1.0:
            issues.append(f"Outcome R² > 1.0: {max_r2}")
        if max_r2 < 0.3:
            issues.append(f"Poor outcome model R²: max={max_r2:.3f}")

        # Check influence function tail ratio
        if self.worst_if_tail_ratio > 100:
            issues.append(
                f"Heavy-tailed influence functions: tail ratio={self.worst_if_tail_ratio:.1f}"
            )

        # Check detailed diagnostics consistency
        if self.dr_diagnostics_per_policy:
            for policy in self.policies:
                if policy not in self.dr_diagnostics_per_policy:
                    issues.append(f"Missing detailed diagnostics for {policy}")

        return issues

    def summary(self) -> str:
        """Generate concise summary including DR info."""
        lines = [
            f"Estimator: {self.estimator_type}",
            f"Method: {self.method}",
            f"Status: {self.overall_status.value}",
            f"Samples: {self.n_samples_valid}/{self.n_samples_total} valid ({100*(1-self.filter_rate):.1f}%)",
            f"Policies: {', '.join(self.policies)}",
            f"Best policy: {self.best_policy}",
            f"Weight ESS: {self.weight_ess:.1%}",
        ]

        if self.is_calibrated:
            lines.append(f"Calibration R²: {self.calibration_r2:.3f}")

        # DR-specific info
        min_r2, max_r2 = self.outcome_r2_range
        lines.append(f"Outcome R²: [{min_r2:.3f}, {max_r2:.3f}]")
        lines.append(f"Cross-fitted: {self.dr_cross_fitted} ({self.dr_n_folds} folds)")

        # Add any validation issues
        issues = self.validate()
        if issues:
            lines.append("Issues: " + "; ".join(issues[:2]))

        return " | ".join(lines)

    def get_policy_diagnostics(self, policy: str) -> Optional[Dict[str, Any]]:
        """Get detailed diagnostics for a specific policy."""
        return self.dr_diagnostics_per_policy.get(policy)

    def has_influence_functions(self) -> bool:
        """Check if influence functions are stored."""
        return (
            self.influence_functions is not None and len(self.influence_functions) > 0
        )

    def to_dict(self) -> Dict:
        """Export as dictionary for serialization, handling numpy arrays."""
        import numpy as np
        from dataclasses import asdict

        d = asdict(self)
        # Convert enums to strings
        d["weight_status"] = self.weight_status.value
        d["overall_status"] = self.overall_status.value

        # Handle influence functions (numpy arrays)
        if self.influence_functions:
            # Convert numpy arrays to lists for JSON serialization
            # Or optionally exclude them to save space
            d["influence_functions"] = {
                k: v.tolist() if isinstance(v, np.ndarray) else v
                for k, v in self.influence_functions.items()
            }

        return d

    def to_dict_summary(self) -> Dict:
        """Export summary without large arrays (e.g., influence functions)."""
        d = super().to_dict()
        # Add DR-specific summary fields
        d["dr_cross_fitted"] = self.dr_cross_fitted
        d["dr_n_folds"] = self.dr_n_folds
        d["outcome_r2_range"] = self.outcome_r2_range
        d["outcome_rmse_mean"] = self.outcome_rmse_mean
        d["worst_if_tail_ratio"] = self.worst_if_tail_ratio
        # Exclude influence functions and detailed per-policy diagnostics
        d.pop("influence_functions", None)
        d.pop("dr_diagnostics_per_policy", None)
        return d

    def to_csv_row(self) -> Dict[str, Any]:
        """Export key metrics as a flat dict for CSV export."""
        # Start with parent's CSV row
        row = super().to_csv_row()
        # Add DR-specific metrics
        row["dr_cross_fitted"] = self.dr_cross_fitted
        row["dr_n_folds"] = self.dr_n_folds
        row["outcome_r2_min"] = self.outcome_r2_range[0]
        row["outcome_r2_max"] = self.outcome_r2_range[1]
        row["outcome_rmse_mean"] = self.outcome_rmse_mean
        row["worst_if_tail_ratio"] = self.worst_if_tail_ratio
        row["has_influence_functions"] = self.has_influence_functions()
        return row

    @classmethod
    def from_dict(cls, data: Dict) -> "DRDiagnostics":
        """Create from dictionary, handling numpy arrays."""
        import numpy as np

        # Convert status strings back to enum
        if "weight_status" in data and isinstance(data["weight_status"], str):
            data["weight_status"] = Status(data["weight_status"])

        # Convert influence function lists back to numpy arrays
        if "influence_functions" in data and data["influence_functions"]:
            data["influence_functions"] = {
                k: np.array(v) if isinstance(v, list) else v
                for k, v in data["influence_functions"].items()
            }

        # Remove computed fields
        data.pop("overall_status", None)

        # Handle tuple for outcome_r2_range
        if "outcome_r2_range" in data and isinstance(data["outcome_r2_range"], list):
            data["outcome_r2_range"] = tuple(data["outcome_r2_range"])

        return cls(**data)


=== ./cje/diagnostics/overlap.py ===

"""
Overlap metrics for importance sampling diagnostics.

These metrics quantify how well two policies overlap, which determines
the reliability of importance-weighted estimates. The key insight is that
some metrics (like Hellinger affinity) measure structural compatibility
that cannot be improved by calibration, while others (like ESS) can be
improved through techniques like SIMCal.
"""

from dataclasses import dataclass
from typing import Optional, Tuple, Dict, Any
import numpy as np
import logging

logger = logging.getLogger(__name__)


@dataclass
class OverlapMetrics:
    """Comprehensive overlap diagnostics between policies.

    Attributes:
        hellinger_affinity: Bhattacharyya coefficient ∈ (0,1], measures structural overlap.
            1.0 = perfect overlap, < 0.2 = catastrophic mismatch
        ess_fraction: Effective sample size as fraction of n ∈ (0,1].
            Measures statistical efficiency. 1.0 = perfect, < 0.1 = poor
        tail_index: Hill estimator of tail heaviness.
            None if n < 50, < 2 indicates infinite variance
        overlap_quality: Categorical assessment of overlap
        efficiency_loss: Fraction of data effectively wasted (1 - ess_fraction)
        can_calibrate: Whether calibration methods like SIMCal could help
        recommended_method: Suggested estimation method given the overlap
        confidence_penalty: How much wider CIs are vs uniform sampling
        auto_tuned_threshold: ESS threshold for target CI width (if computed)
    """

    # Core metrics
    hellinger_affinity: float  # ∈ (0,1], structural overlap
    ess_fraction: float  # ∈ (0,1], statistical efficiency
    tail_index: Optional[float]  # > 0, tail heaviness (None if n < 50)

    # Derived interpretations
    overlap_quality: str  # "good", "marginal", "poor", "catastrophic"
    efficiency_loss: float  # How much data we're effectively losing
    can_calibrate: bool  # Whether SIMCal can potentially help

    # Recommendations
    recommended_method: str  # "ips", "calibrated-ips", "dr", "refuse"
    confidence_penalty: float  # CI width multiplier vs uniform sampling

    # Auto-tuning info
    auto_tuned_threshold: Optional[float] = None  # ESS threshold for target CI

    def summary(self) -> str:
        """Human-readable summary of overlap diagnostics."""
        return (
            f"Overlap: {self.overlap_quality} "
            f"({self.hellinger_affinity:.0%} similarity, "
            f"{self.ess_fraction:.0%} efficiency). "
            f"Recommendation: {self.recommended_method}"
        )

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "hellinger_affinity": self.hellinger_affinity,
            "ess_fraction": self.ess_fraction,
            "tail_index": self.tail_index,
            "overlap_quality": self.overlap_quality,
            "efficiency_loss": self.efficiency_loss,
            "can_calibrate": self.can_calibrate,
            "recommended_method": self.recommended_method,
            "confidence_penalty": self.confidence_penalty,
            "auto_tuned_threshold": self.auto_tuned_threshold,
        }


def hellinger_affinity(weights: np.ndarray, epsilon: float = 1e-10) -> float:
    """
    Compute Bhattacharyya coefficient (Hellinger affinity).

    This measures the overlap between two distributions. For importance weights
    w = p'/p, the affinity is E[√w] under the base distribution.

    Key properties:
    - Value in (0, 1] where 1 indicates perfect overlap
    - Cannot be improved by weight calibration (measures structural mismatch)
    - Related to Hellinger distance: H = √(1 - A²)

    Args:
        weights: Importance weights (will be normalized to mean 1)
        epsilon: Small constant for numerical stability

    Returns:
        Hellinger affinity (Bhattacharyya coefficient)
    """
    weights = np.asarray(weights)

    # Handle empty or invalid input
    if len(weights) == 0:
        return float(np.nan)

    # Remove any negative or nan weights (shouldn't exist but be defensive)
    valid_mask = (weights >= 0) & np.isfinite(weights)
    if not np.any(valid_mask):
        logger.warning("No valid weights found for Hellinger affinity computation")
        return float(np.nan)

    weights_valid = weights[valid_mask]

    # Normalize to mean 1 for numerical stability and interpretability
    mean_w = np.mean(weights_valid)
    if mean_w <= epsilon:
        return 0.0  # Catastrophic case - no overlap

    normalized = weights_valid / mean_w

    # Compute affinity with numerical guards
    # For mean-1 weights, this equals E[√w]
    sqrt_weights = np.sqrt(np.maximum(normalized, epsilon))
    affinity = float(np.mean(sqrt_weights))

    # Theoretical bound: affinity ∈ (0, 1] for mean-1 weights
    # In practice might slightly exceed 1 due to numerics
    return min(affinity, 1.0)


def compute_auto_tuned_threshold(
    n: int, target_ci_halfwidth: float, level: str = "critical"
) -> float:
    """
    Compute ESS threshold for desired confidence interval width.

    Based on the variance bound for IPS with bounded rewards:
    Var(V_IPS) ≤ 1/(4n·ESS_fraction)

    This gives a 95% CI half-width of approximately:
    HW ≈ 1.96/(2√(n·ESS_fraction))

    Solving for ESS_fraction given target HW:
    ESS_fraction ≥ (1.96/(2·target))² / n

    Which simplifies to:
    ESS_fraction ≥ 0.9604 / (n·target²)

    Args:
        n: Sample size
        target_ci_halfwidth: Desired CI half-width (e.g., 0.01 for ±1%)
        level: "critical" or "warning" (warning uses half the critical threshold)

    Returns:
        Minimum ESS fraction needed for target precision
    """
    if n <= 0 or target_ci_halfwidth <= 0:
        return 0.1  # Fallback to default

    # Based on variance bound for bounded rewards
    # (1.96/2)² = 0.9604
    threshold = 0.9604 / (n * target_ci_halfwidth**2)

    if level == "warning":
        threshold *= 0.5  # Warning at half the critical level

    # Cap at reasonable bounds
    return min(max(threshold, 0.001), 1.0)


def compute_overlap_metrics(
    weights: np.ndarray,
    target_ci_halfwidth: float = 0.01,
    n_samples: Optional[int] = None,
    compute_tail_index: bool = True,
    auto_tune_threshold: bool = False,
) -> OverlapMetrics:
    """
    Compute comprehensive overlap diagnostics.

    This function computes three complementary metrics:
    1. Hellinger affinity: Structural overlap (cannot be improved)
    2. ESS fraction: Statistical efficiency (can be improved by calibration)
    3. Tail index: Pathological behavior (partially improvable)

    Args:
        weights: Importance weights (will be normalized to mean 1)
        target_ci_halfwidth: Desired CI half-width for auto-tuning
        n_samples: Sample size (defaults to len(weights))
        compute_tail_index: Whether to compute Hill tail index
        auto_tune_threshold: Whether to compute auto-tuned ESS threshold

    Returns:
        OverlapMetrics with diagnostics and recommendations
    """
    weights = np.asarray(weights)
    n = n_samples or len(weights)

    if len(weights) == 0:
        # Return worst-case metrics for empty input
        return OverlapMetrics(
            hellinger_affinity=0.0,
            ess_fraction=0.0,
            tail_index=None,
            overlap_quality="catastrophic",
            efficiency_loss=1.0,
            can_calibrate=False,
            recommended_method="refuse",
            confidence_penalty=np.inf,
            auto_tuned_threshold=None,
        )

    # Normalize to mean 1 for consistent metrics
    weights = weights / np.mean(weights)

    # 1. Hellinger affinity (structural overlap)
    hellinger = hellinger_affinity(weights)

    # 2. ESS fraction (statistical efficiency)
    ess = float(np.sum(weights) ** 2 / np.sum(weights**2))
    ess_fraction = ess / n

    # 3. Tail index (pathological behavior)
    tail_index = None
    if compute_tail_index and n >= 50:
        try:
            from .weights import hill_tail_index

            tail_index = hill_tail_index(weights)
        except (ImportError, ValueError) as e:
            logger.debug(f"Could not compute tail index: {e}")

    # 4. Auto-tuned threshold (if requested)
    auto_tuned_threshold = None
    if auto_tune_threshold:
        auto_tuned_threshold = compute_auto_tuned_threshold(
            n, target_ci_halfwidth, "critical"
        )

    # Interpret overlap quality based on Hellinger affinity
    if hellinger < 0.20:
        quality = "catastrophic"
        can_calibrate = False  # Too far gone
    elif hellinger < 0.35:
        quality = "poor"
        can_calibrate = True  # Might help somewhat
    elif hellinger < 0.50:
        quality = "marginal"
        can_calibrate = True
    else:
        quality = "good"
        can_calibrate = True

    # Compute efficiency loss (how much data we're wasting)
    efficiency_loss = 1.0 - ess_fraction

    # Confidence interval penalty vs uniform sampling
    # Based on Var ≤ 1/(4n·ESS_frac) for bounded rewards
    if ess_fraction > 0.001:
        confidence_penalty = 1.0 / np.sqrt(ess_fraction)
    else:
        confidence_penalty = np.inf

    # Recommendation engine based on all metrics
    if quality == "catastrophic":
        recommended = "refuse"
    elif tail_index and tail_index < 1.5:
        # Extremely heavy tails - need bias correction
        recommended = "dr"
    elif ess_fraction < 0.10:
        # Low ESS - depends on overlap quality
        if quality == "poor":
            recommended = "refuse"
        else:
            recommended = "dr"
    elif ess_fraction < 0.30 and can_calibrate:
        # Moderate ESS with decent overlap - calibration can help
        recommended = "calibrated-ips"
    else:
        # Good enough for standard IPS
        recommended = "ips"

    return OverlapMetrics(
        hellinger_affinity=hellinger,
        ess_fraction=ess_fraction,
        tail_index=tail_index,
        overlap_quality=quality,
        efficiency_loss=efficiency_loss,
        can_calibrate=can_calibrate,
        recommended_method=recommended,
        confidence_penalty=confidence_penalty,
        auto_tuned_threshold=auto_tuned_threshold,
    )


def diagnose_overlap_problems(
    metrics: OverlapMetrics, verbose: bool = True
) -> Tuple[bool, str]:
    """
    Diagnose overlap problems and suggest solutions.

    Provides human-readable explanations of overlap issues and
    actionable recommendations for addressing them.

    Args:
        metrics: Computed overlap metrics
        verbose: Whether to print diagnosis

    Returns:
        Tuple of (should_proceed, explanation)
    """
    msgs = []

    # Explain the problem in intuitive terms
    if metrics.overlap_quality == "catastrophic":
        msgs.append(
            f"❌ Catastrophic overlap ({metrics.hellinger_affinity:.0%} similarity)\n"
            f"   The policies are fundamentally incompatible - like comparing\n"
            f"   apples to oranges. No statistical method can fix this.\n"
            f"   {metrics.efficiency_loss:.0%} of your data is effectively ignored."
        )
        should_proceed = False

    elif metrics.overlap_quality == "poor":
        msgs.append(
            f"⚠️  Poor overlap ({metrics.hellinger_affinity:.0%} similarity)\n"
            f"   Only {metrics.ess_fraction:.0%} of your data is effectively used.\n"
            f"   Confidence intervals will be {metrics.confidence_penalty:.1f}× wider."
        )
        should_proceed = True

    elif metrics.overlap_quality == "marginal":
        msgs.append(
            f"⚠️  Marginal overlap ({metrics.hellinger_affinity:.0%} similarity)\n"
            f"   {metrics.ess_fraction:.0%} effective sample size.\n"
            f"   Some variance inflation expected."
        )
        should_proceed = True

    else:
        msgs.append(
            f"✓ Good overlap ({metrics.hellinger_affinity:.0%} similarity)\n"
            f"  {metrics.ess_fraction:.0%} effective sample size"
        )
        should_proceed = True

    # Add specific warnings about tail behavior
    if metrics.tail_index is not None:
        if metrics.tail_index < 1:
            msgs.append(
                f"⚠️  Extremely heavy tails (α={metrics.tail_index:.2f})\n"
                f"   Infinite mean - estimates are unreliable!"
            )
        elif metrics.tail_index < 2:
            msgs.append(
                f"⚠️  Heavy tails detected (α={metrics.tail_index:.2f})\n"
                f"   Infinite variance - estimates may be unstable."
            )

    # Provide actionable recommendations
    msgs.append("\n📊 Recommendation:")
    if metrics.recommended_method == "refuse":
        msgs.append("   Do not proceed with importance sampling estimation.")
        msgs.append("   Solutions:")
        msgs.append("   • Use policies with better overlap (>35% similarity)")
        msgs.append("   • Collect data under a more diverse logging policy")
        msgs.append("   • Consider online A/B testing instead")

    elif metrics.recommended_method == "dr":
        msgs.append("   Use doubly-robust methods with fresh draws.")
        msgs.append("   The outcome model can compensate for poor overlap.")

    elif metrics.recommended_method == "calibrated-ips":
        msgs.append("   Use CalibratedIPS for variance reduction.")
        msgs.append("   Weight calibration can improve efficiency by 2-3×.")

    else:
        msgs.append("   Standard IPS should work adequately.")

    # Add auto-tuning info if available
    if metrics.auto_tuned_threshold is not None:
        msgs.append(
            f"\n📏 Auto-tuned ESS threshold: {metrics.auto_tuned_threshold:.1%}"
        )
        if metrics.ess_fraction >= metrics.auto_tuned_threshold:
            msgs.append("   ✓ Meets threshold for target precision")
        else:
            msgs.append("   ✗ Below threshold for target precision")

    explanation = "\n".join(msgs)

    if verbose:
        print(explanation)

    return should_proceed, explanation


=== ./cje/diagnostics/robust_inference.py ===

"""
Robust inference utilities for handling dependence and multiple testing.

Implements dependence-robust standard errors and FDR control as specified
in Section 9.4 of the CJE paper.
"""

import numpy as np
from typing import Dict, List, Tuple, Optional, Any, Callable, Union
from scipy import stats
import logging

logger = logging.getLogger(__name__)


# ========== Dependence-Robust Standard Errors ==========


def stationary_bootstrap_se(
    data: np.ndarray,
    statistic_fn: Callable[[np.ndarray], float],
    n_bootstrap: int = 4000,
    mean_block_length: Optional[float] = None,
    alpha: float = 0.05,
    return_distribution: bool = False,
) -> Dict[str, Any]:
    """Compute standard errors using stationary bootstrap for time series.

    The stationary bootstrap (Politis & Romano, 1994) resamples blocks of
    random length with geometric distribution, preserving weak dependence.

    Args:
        data: Input data array (n_samples, ...)
        statistic_fn: Function that computes the statistic of interest
        n_bootstrap: Number of bootstrap iterations (default 4000)
        mean_block_length: Expected block length (auto if None)
        alpha: Significance level for CI (default 0.05 for 95% CI)
        return_distribution: If True, return bootstrap distribution

    Returns:
        Dictionary with:
        - 'estimate': Point estimate
        - 'se': Bootstrap standard error
        - 'ci_lower': Lower CI bound
        - 'ci_upper': Upper CI bound
        - 'mean_block_length': Block length used
        - 'distribution': Bootstrap distribution (if requested)

    References:
        Politis, D. N., & Romano, J. P. (1994). The stationary bootstrap.
        Journal of the American Statistical Association, 89(428), 1303-1313.
    """
    n = len(data)

    # Compute point estimate
    estimate = statistic_fn(data)

    # Determine block length if not provided
    if mean_block_length is None:
        # Use first-order autocorrelation to tune block length
        # Rule of thumb: block_length ≈ n^(1/3) * (ρ/(1-ρ))^(2/3)
        if data.ndim == 1:
            acf_1 = np.corrcoef(data[:-1], data[1:])[0, 1] if n > 1 else 0
        else:
            # For multi-dimensional, use first column
            acf_1 = np.corrcoef(data[:-1, 0], data[1:, 0])[0, 1] if n > 1 else 0

        # Ensure reasonable bounds
        acf_1 = np.clip(acf_1, -0.99, 0.99)

        if abs(acf_1) < 0.1:
            # Weak dependence, use smaller blocks
            mean_block_length = max(1, int(n**0.33))
        else:
            # Stronger dependence, use larger blocks
            mean_block_length = max(
                1, int(n**0.33 * (abs(acf_1) / (1 - abs(acf_1))) ** 0.67)
            )

        # Cap at n/4 to ensure variety
        mean_block_length = min(mean_block_length, n // 4)

    # Probability of continuing block (geometric distribution)
    p = 1.0 / mean_block_length

    # Bootstrap iterations
    bootstrap_estimates = []

    for _ in range(n_bootstrap):
        # Generate bootstrap sample using stationary bootstrap
        bootstrap_indices: List[int] = []
        i = 0

        while len(bootstrap_indices) < n:
            # Start new block at random position
            if i == 0 or np.random.random() < p:
                i = np.random.randint(0, n)

            bootstrap_indices.append(i)
            i = (i + 1) % n  # Wrap around

        bootstrap_indices = bootstrap_indices[:n]
        bootstrap_sample = data[bootstrap_indices]

        # Compute statistic on bootstrap sample
        try:
            boot_stat = statistic_fn(bootstrap_sample)
            bootstrap_estimates.append(boot_stat)
        except Exception as e:
            # Skip failed iterations (can happen with small samples)
            logger.debug(f"Bootstrap iteration failed: {e}")
            continue

    bootstrap_estimates = np.array(bootstrap_estimates)

    # Compute standard error
    se = np.std(bootstrap_estimates, ddof=1)

    # Compute confidence interval (percentile method)
    ci_lower = np.percentile(bootstrap_estimates, 100 * alpha / 2)
    ci_upper = np.percentile(bootstrap_estimates, 100 * (1 - alpha / 2))

    result: Dict[str, Any] = {
        "estimate": float(estimate),
        "se": float(se),
        "ci_lower": float(ci_lower),
        "ci_upper": float(ci_upper),
        "mean_block_length": float(mean_block_length),
        "n_bootstrap": len(bootstrap_estimates),
        "effective_samples": len(bootstrap_estimates),
    }

    if return_distribution:
        result["distribution"] = bootstrap_estimates

    return result


def moving_block_bootstrap_se(
    data: np.ndarray,
    statistic_fn: Callable[[np.ndarray], float],
    n_bootstrap: int = 4000,
    block_length: Optional[int] = None,
    alpha: float = 0.05,
) -> Dict[str, Any]:
    """Compute standard errors using moving block bootstrap.

    The moving block bootstrap (Künsch, 1989) resamples fixed-length
    contiguous blocks, preserving local dependence structure.

    Args:
        data: Input data array
        statistic_fn: Function that computes the statistic
        n_bootstrap: Number of bootstrap iterations
        block_length: Fixed block length (auto if None)
        alpha: Significance level for CI

    Returns:
        Dictionary with bootstrap results

    References:
        Künsch, H. R. (1989). The jackknife and the bootstrap for general
        stationary observations. The Annals of Statistics, 17(3), 1217-1241.
    """
    n = len(data)

    # Compute point estimate
    estimate = statistic_fn(data)

    # Determine block length if not provided
    if block_length is None:
        # Standard choice: n^(1/3) for optimal MSE
        block_length = max(1, int(n ** (1.0 / 3.0)))

    # Number of blocks needed
    n_blocks = int(np.ceil(n / block_length))

    # Bootstrap iterations
    bootstrap_estimates = []

    for _ in range(n_bootstrap):
        # Sample blocks with replacement
        bootstrap_indices = []

        for _ in range(n_blocks):
            # Random starting point for block
            start = np.random.randint(0, n - block_length + 1)
            block_indices = list(range(start, min(start + block_length, n)))
            bootstrap_indices.extend(block_indices)

        # Trim to original length
        bootstrap_indices = bootstrap_indices[:n]
        bootstrap_sample = data[bootstrap_indices]

        # Compute statistic
        try:
            boot_stat = statistic_fn(bootstrap_sample)
            bootstrap_estimates.append(boot_stat)
        except Exception as e:
            logger.debug(f"Bootstrap iteration failed: {e}")
            continue

    bootstrap_estimates = np.array(bootstrap_estimates)

    # Compute statistics
    se = np.std(bootstrap_estimates, ddof=1)
    ci_lower = np.percentile(bootstrap_estimates, 100 * alpha / 2)
    ci_upper = np.percentile(bootstrap_estimates, 100 * (1 - alpha / 2))

    return {
        "estimate": float(estimate),
        "se": float(se),
        "ci_lower": float(ci_lower),
        "ci_upper": float(ci_upper),
        "block_length": int(block_length),
        "n_bootstrap": len(bootstrap_estimates),
    }


def cluster_robust_se(
    data: np.ndarray,
    cluster_ids: np.ndarray,
    statistic_fn: Callable[[np.ndarray], float],
    influence_fn: Optional[Callable[[np.ndarray], np.ndarray]] = None,
    alpha: float = 0.05,
) -> Dict[str, Any]:
    """Compute cluster-robust (sandwich) standard errors.

    For data with cluster structure (e.g., multiple obs per user),
    accounts for within-cluster correlation.

    Args:
        data: Input data array
        cluster_ids: Cluster membership for each observation
        statistic_fn: Function that computes the statistic
        influence_fn: Function that computes influence functions
        alpha: Significance level for CI

    Returns:
        Dictionary with robust standard errors
    """
    n = len(data)
    estimate = statistic_fn(data)

    # Get unique clusters
    unique_clusters = np.unique(cluster_ids)
    n_clusters = len(unique_clusters)

    if influence_fn is None:
        # For the mean, the influence function is just (x_i - mean)
        # This is the standard influence function for the sample mean
        influences = data - estimate if data.ndim == 1 else data[:, 0] - estimate
    else:
        # Use provided influence function
        influences = influence_fn(data)

    # Aggregate influences by cluster
    cluster_sums = np.zeros(n_clusters)
    for i, cluster_id in enumerate(unique_clusters):
        mask = cluster_ids == cluster_id
        cluster_sums[i] = np.sum(influences[mask])

    # Cluster-robust variance
    # Variance of cluster sums, scaled by n^2
    var_cluster = np.sum((cluster_sums - np.mean(cluster_sums)) ** 2) / (n_clusters - 1)
    se_cluster = np.sqrt(var_cluster) / n

    # Confidence interval using t-distribution with n_clusters - 1 df
    t_crit = stats.t.ppf(1 - alpha / 2, n_clusters - 1)
    ci_lower = estimate - t_crit * se_cluster
    ci_upper = estimate + t_crit * se_cluster

    return {
        "estimate": float(estimate),
        "se": float(se_cluster),
        "ci_lower": float(ci_lower),
        "ci_upper": float(ci_upper),
        "n_clusters": int(n_clusters),
        "df": int(n_clusters - 1),
    }


# ========== Multiple Testing Correction ==========


def benjamini_hochberg_correction(
    p_values: np.ndarray,
    alpha: float = 0.05,
    labels: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Apply Benjamini-Hochberg FDR correction for multiple testing.

    Controls the False Discovery Rate when testing multiple hypotheses,
    as required by Section 9.4 of the CJE paper.

    Args:
        p_values: Array of p-values from individual tests
        alpha: FDR level (default 0.05)
        labels: Optional labels for each test

    Returns:
        Dictionary with:
        - 'adjusted_p_values': BH-adjusted p-values
        - 'significant': Boolean mask of significant results
        - 'n_significant': Number of significant results
        - 'threshold': Largest p-value threshold used
        - 'summary': List of (label, p_value, adjusted_p, significant)

    References:
        Benjamini, Y., & Hochberg, Y. (1995). Controlling the false
        discovery rate. Journal of the Royal Statistical Society B.
    """
    n = len(p_values)

    if n == 0:
        return {
            "adjusted_p_values": np.array([]),
            "significant": np.array([], dtype=bool),
            "n_significant": 0,
            "threshold": 0.0,
            "summary": [],
        }

    # Sort p-values and track original indices
    sorted_indices = np.argsort(p_values)
    sorted_p = p_values[sorted_indices]

    # BH adjustment: p_adj = min(1, p * n / rank)
    ranks = np.arange(1, n + 1)
    adjusted_p = np.minimum(1.0, sorted_p * n / ranks)

    # Enforce monotonicity (adjusted p-values should be non-decreasing)
    for i in range(n - 2, -1, -1):
        adjusted_p[i] = min(adjusted_p[i], adjusted_p[i + 1])

    # Find threshold (largest p where p <= alpha * rank / n)
    bh_threshold = 0.0
    significant_sorted = np.zeros(n, dtype=bool)

    for i in range(n - 1, -1, -1):
        if sorted_p[i] <= alpha * (i + 1) / n:
            bh_threshold = sorted_p[i]
            significant_sorted[: i + 1] = True
            break

    # Map back to original order
    adjusted_p_orig = np.zeros(n)
    significant_orig = np.zeros(n, dtype=bool)

    for i, orig_idx in enumerate(sorted_indices):
        adjusted_p_orig[orig_idx] = adjusted_p[i]
        significant_orig[orig_idx] = significant_sorted[i]

    # Create summary
    summary = []
    if labels is None:
        labels = [f"H{i+1}" for i in range(n)]

    for i in range(n):
        summary.append(
            {
                "label": labels[i],
                "p_value": float(p_values[i]),
                "adjusted_p": float(adjusted_p_orig[i]),
                "significant": bool(significant_orig[i]),
            }
        )

    # Sort summary by p-value for readability
    def get_p_value(item: Dict[str, Any]) -> float:
        return float(item["p_value"])

    summary.sort(key=get_p_value)

    return {
        "adjusted_p_values": adjusted_p_orig,
        "significant": significant_orig,
        "n_significant": int(np.sum(significant_orig)),
        "threshold": float(bh_threshold),
        "fdr_level": float(alpha),
        "n_tests": n,
        "summary": summary,
    }


def compute_simultaneous_bands(
    estimates: np.ndarray,
    standard_errors: np.ndarray,
    correlation_matrix: Optional[np.ndarray] = None,
    alpha: float = 0.05,
    labels: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Compute simultaneous confidence bands using max-t method.

    For a selected subset of policies, provides simultaneous coverage
    accounting for correlation between estimates.

    Args:
        estimates: Point estimates for each policy
        standard_errors: Standard errors for each estimate
        correlation_matrix: Correlation between estimates (identity if None)
        alpha: Significance level
        labels: Optional policy labels

    Returns:
        Dictionary with simultaneous confidence bands
    """
    k = len(estimates)

    if correlation_matrix is None:
        # Assume independence
        correlation_matrix = np.eye(k)

    # Standardize to get t-statistics
    t_stats = estimates / standard_errors

    # Critical value for simultaneous coverage
    # Using Bonferroni as conservative approximation
    # (Could use multivariate t simulation for exact)
    bonf_alpha = alpha / k
    z_crit = stats.norm.ppf(1 - bonf_alpha / 2)

    # Simultaneous bands
    lower_bands = estimates - z_crit * standard_errors
    upper_bands = estimates + z_crit * standard_errors

    # Check which are significantly different from 0
    significant = (lower_bands > 0) | (upper_bands < 0)

    if labels is None:
        labels = [f"Policy{i+1}" for i in range(k)]

    bands = []
    for i in range(k):
        bands.append(
            {
                "label": labels[i],
                "estimate": float(estimates[i]),
                "se": float(standard_errors[i]),
                "lower": float(lower_bands[i]),
                "upper": float(upper_bands[i]),
                "significant": bool(significant[i]),
            }
        )

    return {
        "bands": bands,
        "critical_value": float(z_crit),
        "n_policies": k,
        "bonferroni_alpha": float(bonf_alpha),
        "n_significant": int(np.sum(significant)),
    }


# ========== Integrated Robust Inference ==========


def compute_robust_inference(
    estimates: np.ndarray,
    influence_functions: Optional[np.ndarray] = None,
    data: Optional[np.ndarray] = None,
    method: str = "stationary_bootstrap",
    cluster_ids: Optional[np.ndarray] = None,
    alpha: float = 0.05,
    n_bootstrap: int = 4000,
    apply_fdr: bool = True,
    fdr_alpha: float = 0.05,
    policy_labels: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Comprehensive robust inference with dependence and multiplicity handling.

    Args:
        estimates: Point estimates for policies
        influence_functions: If provided, use for inference
        data: Raw data (if influence_functions not provided)
        method: "stationary_bootstrap", "moving_block", or "cluster"
        cluster_ids: For cluster-robust SEs
        alpha: Significance level for CIs
        n_bootstrap: Bootstrap iterations
        apply_fdr: Whether to apply FDR correction
        fdr_alpha: FDR control level
        policy_labels: Names for policies

    Returns:
        Dictionary with complete robust inference results
    """
    n_policies = len(estimates)

    # Compute robust SEs for each policy
    robust_ses = []
    robust_cis = []
    p_values = []

    for i in range(n_policies):
        if method == "stationary_bootstrap":
            if influence_functions is not None:
                # Use influence functions
                result = stationary_bootstrap_se(
                    (
                        influence_functions[:, i]
                        if influence_functions.ndim > 1
                        else influence_functions
                    ),
                    lambda x: np.mean(x),
                    n_bootstrap=n_bootstrap,
                    alpha=alpha,
                )
            elif data is not None:
                # Use raw data
                result = stationary_bootstrap_se(
                    data,
                    lambda x: estimates[i],  # Placeholder - would need actual estimator
                    n_bootstrap=n_bootstrap,
                    alpha=alpha,
                )
            else:
                raise ValueError("Need either influence_functions or data")

        elif method == "cluster" and cluster_ids is not None:
            result = cluster_robust_se(
                influence_functions[:, i] if influence_functions is not None else data,
                cluster_ids,
                lambda x: np.mean(x),
                alpha=alpha,
            )
        else:
            # Fallback to classical
            se = np.std(
                influence_functions[:, i] if influence_functions is not None else data
            ) / np.sqrt(len(data))
            result = {
                "se": se,
                "ci_lower": estimates[i] - 1.96 * se,
                "ci_upper": estimates[i] + 1.96 * se,
            }

        robust_ses.append(result["se"])
        robust_cis.append((result["ci_lower"], result["ci_upper"]))

        # Compute p-value for test that estimate != 0
        z_stat = estimates[i] / result["se"] if result["se"] > 0 else 0
        p_val = 2 * (1 - stats.norm.cdf(abs(z_stat)))
        p_values.append(p_val)

    robust_ses = np.array(robust_ses)
    p_values = np.array(p_values)

    # Apply FDR correction if requested
    fdr_results = None
    if apply_fdr and n_policies > 1:
        fdr_results = benjamini_hochberg_correction(
            p_values,
            alpha=fdr_alpha,
            labels=policy_labels,
        )

    return {
        "estimates": estimates,
        "robust_ses": robust_ses,
        "robust_cis": robust_cis,
        "p_values": p_values,
        "method": method,
        "fdr_results": fdr_results,
        "n_policies": n_policies,
        "inference_alpha": alpha,
        "fdr_alpha": fdr_alpha if apply_fdr else None,
    }


=== ./cje/diagnostics/stability.py ===

"""
Stability and drift detection diagnostics for judges and models.
"""

import numpy as np
from typing import Dict, Any, Optional, List, Tuple
from scipy import stats
import logging

logger = logging.getLogger(__name__)


def kendall_tau_drift(
    scores_1: np.ndarray,
    scores_2: np.ndarray,
    labels_1: Optional[np.ndarray] = None,
    labels_2: Optional[np.ndarray] = None,
) -> Dict[str, Any]:
    """Compute Kendall τ rank correlation for drift detection.

    Detects changes in judge ranking behavior over time/batches.

    Args:
        scores_1: Judge scores from period/batch 1
        scores_2: Judge scores from period/batch 2
        labels_1: Optional labels for period 1 (for paired comparison)
        labels_2: Optional labels for period 2 (for paired comparison)

    Returns:
        Dictionary with:
        - 'tau': Kendall τ statistic (-1 to 1)
        - 'p_value': Significance of difference from perfect correlation
        - 'drift_detected': Boolean (True if p < 0.05)
        - 'interpretation': String description

    References:
        Section 9.5 of the CJE paper on temporal stability.
    """
    # Basic Kendall τ between two score sets
    if len(scores_1) != len(scores_2):
        raise ValueError(
            f"Score arrays must have same length: {len(scores_1)} vs {len(scores_2)}"
        )

    tau, p_value = stats.kendalltau(scores_1, scores_2)

    # Detect significant drift (handle NaN)
    drift_detected = False
    if not np.isnan(tau) and not np.isnan(p_value):
        drift_detected = p_value < 0.05 and tau < 0.8

    # Interpretation (handle NaN)
    if np.isnan(tau):
        interpretation = "Cannot compute stability (constant values)"
    elif tau > 0.9:
        interpretation = "Excellent stability (τ > 0.9)"
    elif tau > 0.8:
        interpretation = "Good stability (0.8 < τ ≤ 0.9)"
    elif tau > 0.6:
        interpretation = "Moderate drift detected (0.6 < τ ≤ 0.8)"
    elif tau > 0.4:
        interpretation = "Significant drift detected (0.4 < τ ≤ 0.6)"
    else:
        interpretation = "Severe drift detected (τ ≤ 0.4)"

    result = {
        "tau": float(tau),
        "p_value": float(p_value),
        "drift_detected": bool(drift_detected),
        "interpretation": interpretation,
        "n_samples": len(scores_1),
    }

    # If labels provided, compute drift in score-label relationship
    if labels_1 is not None and labels_2 is not None:
        tau1, _ = stats.kendalltau(scores_1, labels_1)
        tau2, _ = stats.kendalltau(scores_2, labels_2)

        result["tau_with_labels_1"] = float(tau1)
        result["tau_with_labels_2"] = float(tau2)
        result["tau_change"] = float(tau2 - tau1)

        if abs(tau2 - tau1) > 0.1:
            result["calibration_drift"] = True
            result["calibration_drift_severity"] = (
                "high" if abs(tau2 - tau1) > 0.2 else "moderate"
            )

    return result


def sequential_drift_detection(
    score_batches: List[np.ndarray],
    window_size: int = 2,
    overlap: int = 1,
) -> Dict[str, Any]:
    """Detect drift across multiple sequential batches.

    Args:
        score_batches: List of score arrays for each time period
        window_size: Number of batches to compare
        overlap: Number of overlapping batches between windows

    Returns:
        Dictionary with:
        - 'tau_sequence': List of τ values between consecutive batches
        - 'drift_points': Indices where drift detected
        - 'overall_stability': Summary metric
    """
    if len(score_batches) < 2:
        return {
            "tau_sequence": [],
            "drift_points": [],
            "overall_stability": 1.0,
            "insufficient_data": True,
        }

    tau_sequence = []
    drift_points = []

    # Compare consecutive batches
    for i in range(len(score_batches) - 1):
        # Need same size for comparison
        min_len = min(len(score_batches[i]), len(score_batches[i + 1]))
        if min_len < 10:
            continue

        scores1 = score_batches[i][:min_len]
        scores2 = score_batches[i + 1][:min_len]

        result = kendall_tau_drift(scores1, scores2)
        tau_sequence.append(result["tau"])

        if result["drift_detected"]:
            drift_points.append(i + 1)

    # Overall stability as minimum tau (handle NaN and None values)
    if tau_sequence:
        # Filter out NaN and None values before taking min
        valid_taus = [t for t in tau_sequence if t is not None and not np.isnan(t)]
        overall_stability = min(valid_taus) if valid_taus else np.nan
    else:
        overall_stability = 1.0

    return {
        "tau_sequence": tau_sequence,
        "drift_points": drift_points,
        "overall_stability": float(overall_stability),
        "n_batches": len(score_batches),
        "has_drift": len(drift_points) > 0,
    }


def reliability_diagram(
    predicted_probs: np.ndarray,
    true_binary: np.ndarray,
    n_bins: int = 10,
) -> Dict[str, Any]:
    """Compute reliability diagram statistics and Brier score decomposition.

    For assessing calibration quality of probabilistic predictions.

    Args:
        predicted_probs: Predicted probabilities [0, 1]
        true_binary: True binary outcomes {0, 1}
        n_bins: Number of bins for reliability diagram

    Returns:
        Dictionary with:
        - 'bin_edges': Bin boundaries
        - 'bin_frequencies': Fraction of predictions in each bin
        - 'bin_accuracies': Actual positive rate in each bin
        - 'bin_confidences': Mean predicted probability in each bin
        - 'ece': Expected Calibration Error
        - 'mce': Maximum Calibration Error
        - 'brier_score': Overall Brier score
        - 'brier_reliability': Reliability component
        - 'brier_resolution': Resolution component
        - 'brier_uncertainty': Uncertainty component

    References:
        Guo et al. (2017) "On Calibration of Modern Neural Networks"
    """
    n = len(predicted_probs)

    # Create bins
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_indices = np.digitize(predicted_probs, bin_edges[1:-1])

    bin_frequencies: List[float] = []
    bin_accuracies: List[float] = []
    bin_confidences: List[float] = []

    for i in range(n_bins):
        mask = bin_indices == i
        n_in_bin = mask.sum()

        if n_in_bin > 0:
            bin_frequencies.append(n_in_bin / n)
            bin_accuracies.append(true_binary[mask].mean())
            bin_confidences.append(predicted_probs[mask].mean())
        else:
            bin_frequencies.append(0.0)
            bin_accuracies.append(0.0)
            bin_confidences.append((bin_edges[i] + bin_edges[i + 1]) / 2)

    bin_frequencies_arr = np.array(bin_frequencies)
    bin_accuracies_arr = np.array(bin_accuracies)
    bin_confidences_arr = np.array(bin_confidences)

    # Expected Calibration Error (ECE)
    ece = np.sum(bin_frequencies_arr * np.abs(bin_accuracies_arr - bin_confidences_arr))

    # Maximum Calibration Error (MCE)
    mce = np.max(np.abs(bin_accuracies_arr - bin_confidences_arr))

    # Brier Score and decomposition
    brier_score = np.mean((predicted_probs - true_binary) ** 2)

    # Decomposition: BS = Reliability - Resolution + Uncertainty
    base_rate = true_binary.mean()
    uncertainty = base_rate * (1 - base_rate)

    # Reliability: weighted squared difference between confidence and accuracy
    reliability = np.sum(
        bin_frequencies_arr * (bin_confidences_arr - bin_accuracies_arr) ** 2
    )

    # Resolution: how much the predictions vary from base rate
    resolution = np.sum(bin_frequencies_arr * (bin_accuracies_arr - base_rate) ** 2)

    return {
        "bin_edges": bin_edges.tolist(),
        "bin_frequencies": bin_frequencies_arr.tolist(),
        "bin_accuracies": bin_accuracies_arr.tolist(),
        "bin_confidences": bin_confidences_arr.tolist(),
        "ece": float(ece),
        "mce": float(mce),
        "brier_score": float(brier_score),
        "brier_reliability": float(reliability),
        "brier_resolution": float(resolution),
        "brier_uncertainty": float(uncertainty),
        "is_calibrated": ece < 0.1,  # Rule of thumb threshold
    }


def eif_qq_plot_data(
    influence_functions: np.ndarray,
    standardize: bool = True,
) -> Dict[str, Any]:
    """Generate data for EIF Q-Q plot to check normality assumptions.

    Args:
        influence_functions: Array of influence function values
        standardize: Whether to standardize before comparison

    Returns:
        Dictionary with:
        - 'theoretical_quantiles': Expected normal quantiles
        - 'sample_quantiles': Observed quantiles
        - 'shapiro_stat': Shapiro-Wilk test statistic
        - 'shapiro_p': Shapiro-Wilk p-value
        - 'is_normal': Boolean (p > 0.05)
        - 'skewness': Skewness of distribution
        - 'kurtosis': Excess kurtosis
        - 'outlier_indices': Indices of potential outliers
    """
    n = len(influence_functions)

    # Standardize if requested
    if standardize:
        ifs = (
            influence_functions - influence_functions.mean()
        ) / influence_functions.std()
    else:
        ifs = influence_functions.copy()

    # Q-Q plot data
    sample_quantiles = np.sort(ifs)
    theoretical_quantiles = stats.norm.ppf((np.arange(n) + 0.5) / n)

    # Shapiro-Wilk test for normality (works up to n=5000)
    if n <= 5000:
        shapiro_stat, shapiro_p = stats.shapiro(ifs)
    else:
        # Use Anderson-Darling for large samples
        ad_result = stats.anderson(ifs, dist="norm")
        shapiro_stat = ad_result.statistic
        # Approximate p-value from critical values
        shapiro_p = 0.01 if ad_result.statistic > ad_result.critical_values[-1] else 0.5

    # Skewness and kurtosis
    skewness = stats.skew(ifs)
    kurtosis = stats.kurtosis(ifs)  # Excess kurtosis (0 for normal)

    # Detect outliers (> 3 std from mean in standardized data)
    if standardize:
        outlier_mask = np.abs(ifs) > 3
    else:
        z_scores = np.abs((ifs - ifs.mean()) / ifs.std())
        outlier_mask = z_scores > 3

    outlier_indices = np.where(outlier_mask)[0].tolist()

    return {
        "theoretical_quantiles": theoretical_quantiles.tolist(),
        "sample_quantiles": sample_quantiles.tolist(),
        "shapiro_stat": float(shapiro_stat),
        "shapiro_p": float(shapiro_p),
        "is_normal": shapiro_p > 0.05,
        "skewness": float(skewness),
        "kurtosis": float(kurtosis),
        "n_outliers": len(outlier_indices),
        "outlier_fraction": len(outlier_indices) / n,
        "outlier_indices": outlier_indices[:10],  # Limit to first 10
    }


def compute_stability_diagnostics(
    dataset: Any,
    batch_size: Optional[int] = None,
    judge_field: str = "judge_score",
    oracle_field: Optional[str] = None,
) -> Dict[str, Any]:
    """Compute comprehensive stability diagnostics for a dataset.

    Args:
        dataset: CJE dataset with samples
        batch_size: Size of batches for drift detection (auto if None)
        judge_field: Field name for judge scores
        oracle_field: Optional field name for oracle labels

    Returns:
        Dictionary with stability metrics
    """
    # Extract judge scores and oracle labels separately
    all_judge_scores = []
    oracle_labels = []
    paired_judge_scores = []

    for sample in dataset.samples:
        if judge_field in sample.metadata:
            judge_score = sample.metadata[judge_field]
            all_judge_scores.append(judge_score)

            # Collect oracle labels separately for calibration checking
            if oracle_field and oracle_field in sample.metadata:
                oracle_label = sample.metadata[oracle_field]
                # Only include non-None oracle labels
                if oracle_label is not None:
                    oracle_labels.append(oracle_label)
                    paired_judge_scores.append(judge_score)

    # Use all judge scores for drift detection
    judge_scores = np.array(all_judge_scores)
    n = len(judge_scores)

    if n == 0:
        return {"error": "No judge scores found"}

    # Determine batch size
    if batch_size is None:
        batch_size = max(50, n // 10)  # At least 50, or 10% of data

    # Create batches for drift detection
    n_batches = n // batch_size
    score_batches = []

    for i in range(n_batches):
        start = i * batch_size
        end = min((i + 1) * batch_size, n)
        score_batches.append(judge_scores[start:end])

    # Sequential drift detection
    drift_result = sequential_drift_detection(score_batches)

    result = {
        "n_samples": n,
        "n_batches": n_batches,
        "batch_size": batch_size,
        "drift_detection": drift_result,
    }

    # If we have oracle labels, check calibration stability
    if len(oracle_labels) > 0:
        oracle_labels = np.array(oracle_labels)
        paired_judge_scores = np.array(paired_judge_scores)

        # Overall correlation (using only paired data)
        overall_tau, _ = stats.kendalltau(paired_judge_scores, oracle_labels)
        # Handle potential None or NaN from kendalltau
        if overall_tau is None:
            result["overall_tau_with_oracle"] = np.nan
        else:
            result["overall_tau_with_oracle"] = float(overall_tau)

        # Check correlation stability across batches (using paired data)
        tau_per_batch = []
        n_paired = len(paired_judge_scores)
        paired_batch_size = max(
            10, n_paired // 10
        )  # At least 10, or 10% of paired data
        n_paired_batches = n_paired // paired_batch_size

        for i in range(n_paired_batches):
            start = i * paired_batch_size
            end = min((i + 1) * paired_batch_size, n_paired)

            if end - start >= 2:  # Need at least 2 points for correlation
                batch_tau, _ = stats.kendalltau(
                    paired_judge_scores[start:end], oracle_labels[start:end]
                )
                # Ensure we handle potential None values from kendalltau
                if batch_tau is None:
                    tau_per_batch.append(np.nan)
                else:
                    tau_per_batch.append(batch_tau)

        result["tau_with_oracle_per_batch"] = tau_per_batch
        # Handle NaN and None values in tau_per_batch when computing std
        valid_taus = [t for t in tau_per_batch if t is not None and not np.isnan(t)]
        if valid_taus:
            result["tau_stability"] = float(np.std(valid_taus))
        else:
            result["tau_stability"] = np.nan

    return result


=== ./cje/diagnostics/weights.py ===

"""
Weight diagnostic computations for importance sampling.
"""

import numpy as np
from typing import Dict, Any, Optional
import logging

from .models import Status

logger = logging.getLogger(__name__)


# ========== Core Metrics ==========


def effective_sample_size(weights: np.ndarray) -> float:
    """Compute ESS = (sum(w))^2 / sum(w^2).

    ESS measures how many "effective" samples we have after weighting.
    ESS = n means perfect overlap, ESS << n means poor overlap.
    """
    s = weights.sum()
    s2 = np.sum(weights**2)
    return float((s * s) / np.maximum(s2, 1e-12))


def compute_ess(weights: np.ndarray) -> float:
    """Alias for effective_sample_size() - kept for backward compatibility."""
    return effective_sample_size(weights)


def tail_weight_ratio(
    weights: np.ndarray, q_low: float = 0.05, q_high: float = 0.99
) -> float:
    """Compute ratio of high to low quantiles.

    DEPRECATED: Use hill_tail_index() instead for more theoretically grounded
    tail behavior assessment.

    Args:
        weights: Importance weights
        q_low: Lower quantile (default 0.05 to avoid instability)
        q_high: Upper quantile (default 0.99)

    Returns:
        Ratio of high/low quantiles (inf if low quantile is ~0)
    """
    lo = np.quantile(weights, q_low)
    hi = np.quantile(weights, q_high)
    if lo <= 1e-12:
        return float(np.inf)
    return float(hi / lo)


def mass_concentration(weights: np.ndarray, top_pct: float = 0.01) -> float:
    """Fraction of total weight held by top x% of samples.

    Args:
        weights: Importance weights
        top_pct: Top percentage to consider (0.01 = top 1%)

    Returns:
        Fraction of total weight in top samples
    """
    n = len(weights)
    k = max(1, int(n * top_pct))
    sorted_weights = np.sort(weights)[::-1]  # Descending
    return float(sorted_weights[:k].sum() / weights.sum())


# ========== Tail Index Estimation ==========


def hill_tail_index(
    weights: np.ndarray,
    k_fraction: float = 0.05,
    min_k: int = 10,
    max_k: Optional[int] = None,
) -> float:
    """Estimate the tail index using Hill's estimator.

    The Hill estimator quantifies the heaviness of the right tail.
    For a Pareto-type tail with P(X > x) ~ x^(-α), it estimates α.

    Interpretation:
    - α > 3: Light tails, all moments exist
    - 2 < α ≤ 3: Moderate tails, variance exists
    - 1 < α ≤ 2: Heavy tails, variance may be infinite
    - α ≤ 1: Very heavy tails, even mean may be infinite

    Args:
        weights: Importance weights (positive)
        k_fraction: Fraction of largest values to use (default 0.05 = top 5%)
        min_k: Minimum number of order statistics to use
        max_k: Maximum number of order statistics (default: 10% of n)

    Returns:
        Estimated tail index α. Lower values = heavier tails.
        Returns np.inf if estimation fails.

    References:
        Hill, B. M. (1975). A simple general approach to inference about
        the tail of a distribution. Ann. Statist. 3(5): 1163-1174.
    """
    n = len(weights)
    if n < min_k:
        logger.warning(f"Too few samples ({n}) for Hill estimator, need >= {min_k}")
        return float(np.inf)

    # Determine k (number of order statistics to use)
    k = max(min_k, int(n * k_fraction))
    if max_k is not None:
        k = min(k, max_k)
    else:
        k = min(k, int(n * 0.1))  # Default max: 10% of samples

    # Get the k largest weights
    sorted_weights = np.sort(weights)[::-1]  # Descending order

    # Check for degenerate cases
    if sorted_weights[k] <= 0 or sorted_weights[0] <= 0:
        logger.warning("Zero or negative weights in tail, cannot estimate tail index")
        return float(np.inf)

    # Hill estimator: α̂ = 1/k * Σ(log(X_i) - log(X_{k+1}))
    # where X_i are the top k order statistics
    log_ratios = np.log(sorted_weights[:k]) - np.log(sorted_weights[k])

    # Check for numerical issues
    if not np.all(np.isfinite(log_ratios)):
        logger.warning("Numerical issues in Hill estimator (inf/nan in log ratios)")
        return float(np.inf)

    # Check for zero sum (would cause division by zero)
    log_ratio_sum = np.sum(log_ratios)
    if abs(log_ratio_sum) < 1e-10:
        logger.debug("Hill estimator undefined (uniform weights in tail)")
        return None  # Undefined for uniform weights

    hill_estimate = k / log_ratio_sum

    # Sanity check the estimate
    if hill_estimate <= 0 or not np.isfinite(hill_estimate):
        logger.warning(f"Invalid Hill estimate: {hill_estimate}")
        return float(np.inf)

    return float(hill_estimate)


def hill_tail_index_stable(
    weights: np.ndarray,
    k_fractions: Optional[np.ndarray] = None,
) -> Dict[str, float]:
    """Compute Hill estimator over multiple k values for stability assessment.

    Args:
        weights: Importance weights
        k_fractions: Array of fractions to try (default: [0.01, 0.02, 0.05, 0.1])

    Returns:
        Dictionary with:
        - 'estimate': Median estimate across k values
        - 'min': Minimum estimate
        - 'max': Maximum estimate
        - 'std': Standard deviation of estimates
    """
    if k_fractions is None:
        k_fractions = np.array([0.01, 0.02, 0.05, 0.1])

    estimates = []
    for k_frac in k_fractions:
        est = hill_tail_index(weights, k_fraction=k_frac)
        if np.isfinite(est):
            estimates.append(est)

    if not estimates:
        return {
            "estimate": float(np.inf),
            "min": float(np.inf),
            "max": float(np.inf),
            "std": 0.0,
        }

    estimates = np.array(estimates)
    return {
        "estimate": float(np.median(estimates)),
        "min": float(np.min(estimates)),
        "max": float(np.max(estimates)),
        "std": float(np.std(estimates)),
    }


# ========== Diagnostic Computation ==========


def compute_weight_diagnostics(
    weights: np.ndarray,
    policy: str = "unknown",
    compute_hill: bool = True,
) -> Dict[str, Any]:
    """Compute weight diagnostics for a single policy.

    Returns dict with: ess_fraction, max_weight, tail_index (if computed), status
    """
    n = len(weights)
    ess = effective_sample_size(weights)
    ess_fraction = ess / n if n > 0 else 0.0

    # Hill tail index (primary tail measure)
    if compute_hill and n >= 50:  # Need reasonable sample size
        tail_index = hill_tail_index(weights)
    else:
        tail_index = None

    # Determine status based on ESS and tail index
    if ess_fraction < 0.01:
        status = Status.CRITICAL
    elif tail_index is not None and tail_index < 1:
        # Very heavy tail - infinite mean risk
        status = Status.CRITICAL
    elif ess_fraction < 0.1:
        status = Status.WARNING
    elif tail_index is not None and tail_index < 2:
        # Heavy tail - infinite variance risk
        status = Status.WARNING
    else:
        status = Status.GOOD

    result = {
        "ess_fraction": ess_fraction,
        "max_weight": float(weights.max()),
        "status": status,
    }

    if tail_index is not None:
        result["tail_index"] = tail_index

    return result


=== ./cje/estimators/__init__.py ===

"""Core CJE estimators and types.

This module contains:
- Estimators: CalibratedIPS and base classes
- Data models: Pydantic models for type safety
- Types: Data structures for results and error handling
"""

from .base_estimator import BaseCJEEstimator
from .calibrated_ips import CalibratedIPS
from .stacking import StackedDREstimator
from ..data.models import (
    Sample,
    Dataset,
    EstimationResult,
    LogProbResult,
    LogProbStatus,
)

# Import DR estimators for convenience
try:
    from .dr_base import DRCPOEstimator
    from .tmle import TMLEEstimator
    from .mrdr import MRDREstimator

    _dr_available = True
except ImportError:
    _dr_available = False

__all__ = [
    # Estimators
    "BaseCJEEstimator",
    "CalibratedIPS",
    "StackedDREstimator",
    # Data models
    "Sample",
    "Dataset",
    "EstimationResult",
    # Types
    "LogProbResult",
    "LogProbStatus",
]

if _dr_available:
    __all__.extend(["DRCPOEstimator", "TMLEEstimator", "MRDREstimator"])


=== ./cje/estimators/base_estimator.py ===

"""Base class for CJE estimators."""

from abc import ABC, abstractmethod
from typing import Optional, Dict, Any, Union
import numpy as np
import logging

from ..data.models import Dataset, EstimationResult
from ..data.precomputed_sampler import PrecomputedSampler
from ..calibration.oracle_slice import OracleSliceAugmentation, OracleSliceConfig
from ..calibration.iic import IsotonicInfluenceControl, IICConfig

logger = logging.getLogger(__name__)


class BaseCJEEstimator(ABC):
    """Abstract base class for CJE estimators.

    All estimators must implement:
    - fit(): Prepare the estimator (e.g., calibrate weights)
    - estimate(): Compute estimates and diagnostics

    The estimate() method must populate EstimationResult.diagnostics
    with IPSDiagnostics or DRDiagnostics as appropriate.
    """

    def __init__(
        self,
        sampler: PrecomputedSampler,
        run_diagnostics: bool = True,
        diagnostic_config: Optional[Dict[str, Any]] = None,
        oracle_slice_config: Union[str, bool, OracleSliceConfig, None] = "auto",
        use_iic: bool = True,  # Default to True - free variance reduction!
        iic_config: Optional[IICConfig] = None,
    ):
        """Initialize estimator.

        Args:
            sampler: Data sampler with precomputed log probabilities
            run_diagnostics: Whether to compute diagnostics (default True)
            diagnostic_config: Optional configuration dict (for future use)
            oracle_slice_config: Oracle slice augmentation configuration:
                - "auto" (default): Automatically detect and enable if oracle coverage < 100%
                - True: Always enable with default configuration
                - False/None: Disable augmentation
                - OracleSliceConfig object: Use provided configuration
            use_iic: Whether to use Isotonic Influence Control for variance reduction (default True)
            iic_config: Optional IIC configuration (uses defaults if None)
        """
        self.sampler = sampler
        self.run_diagnostics = run_diagnostics
        self.diagnostic_config = diagnostic_config
        self._fitted = False
        self._weights_cache: Dict[str, np.ndarray] = {}
        self._influence_functions: Dict[str, np.ndarray] = {}
        self._results: Optional[EstimationResult] = None

        # Configure oracle slice augmentation (canonical for all estimators)
        self.oracle_augmentation = self._configure_oracle_augmentation(
            oracle_slice_config
        )
        self._aug_diagnostics: Dict[str, Dict] = {}  # Store augmentation diagnostics

        # Configure IIC for variance reduction
        self.use_iic = use_iic
        self.iic = IsotonicInfluenceControl(iic_config) if use_iic else None
        self._iic_diagnostics: Dict[str, Dict] = {}  # Store IIC diagnostics

    @abstractmethod
    def fit(self) -> None:
        """Fit the estimator (e.g., calibrate weights)."""
        pass

    @abstractmethod
    def estimate(self) -> EstimationResult:
        """Compute estimates for all target policies."""
        pass

    def fit_and_estimate(self) -> EstimationResult:
        """Convenience method to fit and estimate in one call."""
        self.fit()
        result = self.estimate()

        # All estimators now create diagnostics directly in estimate()
        # The DiagnosticSuite system has been removed for simplicity
        # per CLAUDE.md principles (YAGNI, Do One Thing Well)

        # Verify diagnostics were created
        if self.run_diagnostics and result is not None:
            if not hasattr(result, "diagnostics") or result.diagnostics is None:
                # This shouldn't happen anymore, but log a warning
                import logging

                logger = logging.getLogger(__name__)
                logger.warning(
                    f"{self.__class__.__name__} did not create diagnostics. "
                    "All estimators should create IPSDiagnostics or DRDiagnostics directly."
                )

        return result

    def get_influence_functions(self, policy: Optional[str] = None) -> Optional[Any]:
        """Get influence functions for a policy or all policies.

        Influence functions capture the per-sample contribution to the estimate
        and are essential for statistical inference (standard errors, confidence
        intervals, hypothesis tests).

        Args:
            policy: Specific policy name, or None for all policies

        Returns:
            If policy specified: array of influence functions for that policy
            If policy is None: dict of all influence functions by policy
            Returns None if not yet estimated
        """
        if not self._influence_functions:
            return None

        if policy is not None:
            return self._influence_functions.get(policy)

        return self._influence_functions

    def get_weights(self, target_policy: str) -> Optional[np.ndarray]:
        """Get importance weights for a target policy.

        Args:
            target_policy: Name of target policy

        Returns:
            Array of weights or None if not available
        """
        return self._weights_cache.get(target_policy)

    def get_raw_weights(self, target_policy: str) -> Optional[np.ndarray]:
        """Get raw (uncalibrated) importance weights for a target policy.

        Computes raw weights directly from the sampler. These are the
        importance weights p_target/p_base without any calibration or clipping.

        Args:
            target_policy: Name of target policy

        Returns:
            Array of raw weights or None if not available.
        """
        # Get truly raw weights (not Hajek normalized)
        return self.sampler.compute_importance_weights(
            target_policy, clip_weight=None, mode="raw"
        )

    @property
    def is_fitted(self) -> bool:
        """Check if estimator has been fitted."""
        return self._fitted

    def _validate_fitted(self) -> None:
        """Ensure estimator is fitted before making predictions."""
        if not self._fitted:
            raise RuntimeError("Estimator must be fitted before calling estimate()")

    def get_diagnostics(self) -> Optional[Any]:
        """Get the diagnostics from the last estimation.

        Returns:
            Diagnostics if estimate() has been called, None otherwise
        """
        if self._results and self._results.diagnostics:
            return self._results.diagnostics
        return None

    def _is_dr_estimator(self) -> bool:
        """Check if this is a DR-based estimator.

        Returns:
            True if this is a DR variant, False otherwise
        """
        class_name = self.__class__.__name__
        return any(x in class_name for x in ["DR", "MRDR", "TMLE"])

    def _configure_oracle_augmentation(
        self, config: Union[str, bool, OracleSliceConfig, None]
    ) -> OracleSliceAugmentation:
        """Configure oracle slice augmentation based on settings and detected coverage.

        This is the canonical augmentation for all CJE estimators.

        Args:
            config: Configuration setting:
                - "auto": Auto-detect and enable if coverage < 100%
                - True: Always enable with default config
                - False/None: Disable
                - OracleSliceConfig: Use provided config

        Returns:
            Configured OracleSliceAugmentation instance
        """
        # Handle explicit OracleSliceConfig
        if isinstance(config, OracleSliceConfig):
            logger.info("Oracle slice augmentation configured with custom settings")
            return OracleSliceAugmentation(config)

        # Handle boolean/string config
        if config is False or config is None:
            # Explicitly disabled
            return OracleSliceAugmentation(OracleSliceConfig(enable_augmentation=False))

        # For "auto" or True, check if we should enable
        oracle_coverage = self.sampler.oracle_coverage

        if config == "auto":
            # Auto-detect: enable if we have partial oracle coverage
            if oracle_coverage is not None and 0 < oracle_coverage < 1.0:
                logger.info(
                    f"Oracle slice augmentation auto-enabled (coverage={oracle_coverage:.1%})"
                )
                return OracleSliceAugmentation(
                    OracleSliceConfig(enable_augmentation=True, enable_cross_fit=True)
                )
            else:
                # No oracle info or full coverage - disable
                if oracle_coverage == 1.0:
                    logger.debug("Oracle slice augmentation not needed (100% coverage)")
                elif oracle_coverage == 0.0:
                    logger.debug(
                        "Oracle slice augmentation disabled (no oracle labels)"
                    )
                return OracleSliceAugmentation(
                    OracleSliceConfig(enable_augmentation=False)
                )

        elif config is True:
            # Explicitly enabled
            if oracle_coverage is not None:
                logger.info(
                    f"Oracle slice augmentation enabled (coverage={oracle_coverage:.1%})"
                )
            else:
                logger.info("Oracle slice augmentation enabled (coverage unknown)")
            return OracleSliceAugmentation(
                OracleSliceConfig(enable_augmentation=True, enable_cross_fit=True)
            )

        # Should not reach here
        return OracleSliceAugmentation(OracleSliceConfig(enable_augmentation=False))

    def get_mhat(self, target_policy: str) -> Optional[np.ndarray]:
        """Get cached m̂(S) = E[W|S] for oracle augmentation.

        Args:
            target_policy: Name of the target policy

        Returns:
            m_hat: Estimated E[W|S] normalized to mean 1, or None if not fitted
        """
        return self.oracle_augmentation._m_hat_cache.get(target_policy)

    def _apply_iic(
        self, influence: np.ndarray, policy: str, fold_ids: Optional[np.ndarray] = None
    ) -> np.ndarray:
        """Apply Isotonic Influence Control to reduce variance.

        This residualizes the influence function against judge scores,
        reducing variance without changing the estimand.

        Args:
            influence: Raw influence function values
            policy: Policy name
            fold_ids: Optional fold assignments for cross-fitting

        Returns:
            Residualized influence function with reduced variance
        """
        if not self.use_iic or self.iic is None:
            return influence

        # Get judge scores for this policy
        data = self.sampler.get_data_for_policy(policy)
        if not data:
            logger.warning(f"No data for policy {policy}, skipping IIC")
            return influence

        judge_scores = np.array([d.get("judge_score", np.nan) for d in data])

        # Handle missing judge scores
        if np.all(np.isnan(judge_scores)):
            logger.warning(f"All judge scores missing for {policy}, skipping IIC")
            return influence

        # Apply IIC
        residualized, diagnostics = self.iic.residualize(
            influence, judge_scores, policy, fold_ids
        )

        # Store diagnostics
        self._iic_diagnostics[policy] = diagnostics

        if diagnostics.get("applied", False):
            logger.debug(
                f"IIC applied to {policy}: SE reduction={diagnostics.get('se_reduction', 0):.1%}"
            )

        return residualized


=== ./cje/estimators/calibrated_ips.py ===

"""Calibrated Inverse Propensity Scoring (IPS) estimator with stacked SIMCal.

This is the core CJE estimator that uses stacked Score-Indexed Monotone Calibration
(SIMCal) to stabilize IPS in heavy-tail regimes. It combines {baseline, increasing,
decreasing} candidates via convex optimization to minimize OOF influence function
variance, then blends toward uniform to meet variance/ESS constraints.
"""

import numpy as np
from typing import Dict, Optional, Set, Any
import logging

from .base_estimator import BaseCJEEstimator
from ..data.models import EstimationResult
from ..data.precomputed_sampler import PrecomputedSampler
from ..diagnostics import IPSDiagnostics, Status
from ..diagnostics import compute_weight_diagnostics
from ..calibration.simcal import SIMCalibrator, SimcalConfig

logger = logging.getLogger(__name__)


class CalibratedIPS(BaseCJEEstimator):
    """IPS estimator with optional SIMCal weight calibration.

    Can operate in two modes:
    1. calibrate=True (default): Uses stacked Score-Indexed Monotone Calibration (SIMCal)
       to reduce variance and heavy-tail pathologies in importance weights
    2. calibrate=False: Uses raw importance weights directly (equivalent to traditional IPS)

    Features when calibrated:
    - Stacked calibration combining multiple candidates optimally
    - OOF influence function variance minimization
    - ESS floor and variance cap constraints
    - Judge score-indexed calibration for better alignment
    - Automatic DR-aware calibration when calibrator available

    Features in both modes:
    - Oracle slice augmentation for honest confidence intervals
    - Comprehensive diagnostics
    - Optional weight clipping

    Args:
        sampler: PrecomputedSampler with data
        calibrate: Whether to apply SIMCal calibration (default True)
        clip_weight: Maximum weight value before calibration (default None = no clipping)
        ess_floor: Minimum ESS as fraction of n (default 0.2 = 20% ESS) [only used if calibrate=True]
        var_cap: Maximum allowed variance of calibrated weights (default None = no cap) [only used if calibrate=True]
        calibrator: Optional JudgeCalibrator for DR influence functions [only used if calibrate=True]
        include_baseline: Whether to include raw weights in the stack (default True) [only used if calibrate=True]
        baseline_shrink: Shrinkage toward baseline for stability (default 0.05) [only used if calibrate=True]
        refuse_unreliable: Whether to refuse (return NaN) for unreliable estimates (default False)
        **kwargs: Additional arguments passed to BaseCJEEstimator (e.g., oracle_slice_config)
    """

    def __init__(
        self,
        sampler: PrecomputedSampler,
        calibrate: bool = True,
        clip_weight: Optional[float] = None,
        ess_floor: Optional[float] = 0.2,
        var_cap: Optional[float] = None,
        calibrator: Optional[Any] = None,
        include_baseline: bool = False,
        baseline_shrink: float = 0.0,
        run_diagnostics: bool = True,
        refuse_unreliable: bool = False,
        **kwargs: Any,
    ):
        # Pass oracle_slice_config to base if provided, otherwise use default "auto"
        super().__init__(
            sampler=sampler,
            run_diagnostics=run_diagnostics,
            diagnostic_config=None,  # Will use defaults
            **kwargs,  # Passes oracle_slice_config if provided
        )
        self.calibrate = calibrate
        self.clip_weight = clip_weight
        self.ess_floor = ess_floor if calibrate else None
        self.var_cap = var_cap if calibrate else None
        self.calibrator = calibrator if calibrate else None
        self.include_baseline = include_baseline if calibrate else True
        self.baseline_shrink = baseline_shrink if calibrate else 0.0
        self.refuse_unreliable = refuse_unreliable
        self._no_overlap_policies: Set[str] = set()
        self._calibration_info: Dict[str, Dict] = {}  # Store calibration details
        self._diagnostics: Optional[IPSDiagnostics] = None

    def fit(self) -> None:
        """Fit weights for all target policies (with or without calibration)."""
        # Get judge scores once (same for all policies)
        judge_scores = self.sampler.get_judge_scores()

        for policy in self.sampler.target_policies:
            # Get raw weights (with optional pre-clipping)
            raw_weights = self.sampler.compute_importance_weights(
                policy, clip_weight=self.clip_weight
            )
            if raw_weights is None:
                continue

            # Check for no overlap (all weights are zero)
            if np.all(raw_weights == 0):
                logger.warning(
                    f"Policy '{policy}' has no overlap with base policy (all weights zero)."
                )
                self._no_overlap_policies.add(policy)
                continue

            # If not calibrating, just use raw weights
            if not self.calibrate:
                logger.debug(
                    f"Raw IPS weights for policy '{policy}': "
                    f"mean={raw_weights.mean():.3f}, std={raw_weights.std():.3f}, "
                    f"min={raw_weights.min():.3f}, max={raw_weights.max():.3f}"
                )

                # Cache raw weights
                self._weights_cache[policy] = raw_weights

                # Fit m̂(S) for oracle slice augmentation
                if judge_scores is not None:
                    self.oracle_augmentation.fit_m_hat(
                        raw_weights, judge_scores, policy, cv_folds=None
                    )

                continue  # Skip calibration for this policy

            # ========== Calibration path (original code) ==========
            logger.debug(
                f"Calibrating weights for policy '{policy}': "
                f"n_samples={len(raw_weights)}, raw_mean={raw_weights.mean():.3f}"
            )

            # Use SIMCal calibration with appropriate ordering index
            # When calibrator available: use cross-fitted g(s) for better alignment with DR
            # Otherwise: fall back to raw judge scores
            if judge_scores is None:
                raise ValueError(
                    "Judge scores are required for SIMCal calibration. "
                    "Ensure samples have 'judge_score' in metadata."
                )

            # Get rewards for this policy (always needed for influence functions)
            data = self.sampler.get_data_for_policy(policy)
            if not data:
                logger.warning(f"No data for policy '{policy}'. Skipping.")
                continue
            rewards = np.array([d["reward"] for d in data], dtype=float)

            # Try to get DR residuals and cross-fitted rewards if calibrator available
            residuals = None
            fold_ids = None
            g_oof = None
            if self.calibrator is not None and hasattr(self.calibrator, "predict_oof"):
                try:
                    # Extract fold IDs from data
                    fold_list = [d.get("cv_fold") for d in data]
                    if all(v is not None for v in fold_list) and len(fold_list) == len(
                        judge_scores
                    ):
                        fold_ids = np.asarray(fold_list, dtype=int)
                        # Compute cross-fitted predictions
                        g_oof = self.calibrator.predict_oof(judge_scores, fold_ids)
                        residuals = rewards - g_oof
                        logger.debug(f"Using DR residuals for policy '{policy}'")
                        logger.debug(
                            f"Using cross-fitted rewards as SIMCal ordering index"
                        )
                except Exception as e:
                    logger.debug(f"Could not compute DR residuals: {e}")

            # Determine the ordering index for SIMCal
            # Use cross-fitted calibrated rewards if available, otherwise raw judge scores
            # This aligns the monotone projection with the actual nuisance function used in DR
            ordering_index = g_oof if g_oof is not None else judge_scores

            # Run stacked SIMCal calibration
            cfg = SimcalConfig(
                ess_floor=self.ess_floor,
                var_cap=self.var_cap,
                include_baseline=self.include_baseline,
                baseline_shrink=self.baseline_shrink,
            )
            sim = SIMCalibrator(cfg)
            calibrated, calib_info = sim.transform(
                raw_weights,
                ordering_index,  # Now uses g_oof when available, judge_scores otherwise
                rewards=rewards,  # Always provide rewards
                residuals=residuals,  # Provide if available for DR
                fold_ids=fold_ids,  # Provide if available for consistent OOF
            )

            # Cache results
            self._weights_cache[policy] = calibrated
            self._calibration_info[policy] = calib_info

            # Fit m̂(S) for oracle slice augmentation
            # Use the calibrated weights we'll actually use in estimation
            self.oracle_augmentation.fit_m_hat(
                calibrated, judge_scores, policy, cv_folds=fold_ids
            )

        self._fitted = True

    def estimate(self) -> EstimationResult:
        """Compute estimates for all target policies with diagnostics."""
        self._validate_fitted()

        estimates = []
        standard_errors = []
        n_samples_used = {}
        influence_functions = {}

        # Compute estimates for each policy
        for policy in self.sampler.target_policies:
            if policy in self._no_overlap_policies:
                # No overlap - return NaN
                estimates.append(np.nan)
                standard_errors.append(np.nan)
                n_samples_used[policy] = 0
                logger.warning(f"Policy '{policy}' has no overlap - returning NaN")
                continue

            # Get calibrated weights
            weights = self._weights_cache.get(policy)
            if weights is None:
                estimates.append(np.nan)
                standard_errors.append(np.nan)
                n_samples_used[policy] = 0
                continue

            # Get rewards
            data = self.sampler.get_data_for_policy(policy)
            if data is None or len(data) == 0:
                estimates.append(np.nan)
                standard_errors.append(np.nan)
                n_samples_used[policy] = 0
                continue

            # Extract rewards from the list of dictionaries
            rewards = np.array([d["reward"] for d in data])
            n = len(rewards)
            n_samples_used[policy] = n

            # SAFETY CHECK: Refuse to provide unreliable estimates
            # Following CLAUDE.md: "Fail Fast and Clearly"

            # Check effective sample size
            ess = np.sum(weights) ** 2 / np.sum(weights**2) / n

            # Check weight concentration: What fraction of total weight is on top 5% of samples?
            sorted_weights = np.sort(weights)[::-1]
            top_5pct_count = max(1, int(0.05 * n))
            top_5pct_weight = np.sum(sorted_weights[:top_5pct_count]) / np.sum(weights)

            # Check raw weights for hidden problems (calibration can mask issues)
            raw_weights = self.get_raw_weights(policy)
            raw_near_zero = 0.0
            if raw_weights is not None:
                raw_near_zero = np.sum(raw_weights < 1e-10) / len(raw_weights)

            # Coefficient of variation as additional check
            cv_weights = (
                np.std(weights) / np.mean(weights)
                if np.mean(weights) > 0
                else float("inf")
            )

            # Refuse if multiple indicators suggest unreliability
            # We use percentage-based gates because they measure distribution overlap quality,
            # not just statistical power. Poor overlap means the estimate is dominated by
            # a small subset of data, making it practically unreliable even if statistically valid.
            refuse = False
            reasons = []

            if ess < 0.30:  # Less than 30% effective sample size
                refuse = True
                reasons.append(f"ESS={ess:.1%}")

            if raw_near_zero > 0.85:  # More than 85% of raw weights near zero
                refuse = True
                reasons.append(f"raw_near_zero={raw_near_zero:.1%}")

            if (
                top_5pct_weight > 0.30 and cv_weights > 2.0
            ):  # High concentration AND high variability
                refuse = True
                reasons.append(f"top_5%={top_5pct_weight:.1%} with CV={cv_weights:.1f}")

            if refuse:
                # Provide detailed explanation of what low ESS means practically
                warning_msg = (
                    f"Policy '{policy}' has poor overlap: only {ess:.1%} effective overlap. "
                    f"This means {(1-ess)*100:.0f}% of your data is essentially ignored. "
                    f"Reasons: {', '.join(reasons)}. "
                    f"Solutions: (1) Use policies with better overlap, "
                    f"(2) Try DR methods with fresh draws, "
                    f"(3) Collect data from more diverse base policies."
                )

                if self.refuse_unreliable:
                    logger.error(f"Cannot reliably estimate {warning_msg}")
                    estimates.append(np.nan)
                    standard_errors.append(np.nan)
                    influence_functions[policy] = np.full(n, np.nan)
                    continue
                else:
                    # Provide estimate with strong warning
                    logger.warning(f"⚠️ UNRELIABLE ESTIMATE: {warning_msg}")

            # Base IPS contribution
            base_contrib = weights * rewards

            # Add oracle slice augmentation for honest CIs
            aug, aug_diagnostics = self.oracle_augmentation.compute_augmentation(
                policy, rewards, data, self.sampler.dataset.samples
            )
            self._aug_diagnostics[policy] = aug_diagnostics

            # Total contribution with augmentation
            total_contrib = base_contrib + aug
            estimate = float(total_contrib.mean())
            estimates.append(estimate)

            # Compute influence functions
            influence = total_contrib - estimate

            # Apply IIC for variance reduction (if enabled)
            influence = self._apply_iic(influence, policy)

            # Compute standard error from the (possibly residualized) influence functions
            se = float(np.std(influence, ddof=1) / np.sqrt(n))
            standard_errors.append(se)

            # Add slice variance share to diagnostics
            if aug_diagnostics:
                var_base = (
                    np.var(base_contrib - base_contrib.mean(), ddof=1) if n > 1 else 0.0
                )
                var_total = (
                    np.var(total_contrib - total_contrib.mean(), ddof=1)
                    if n > 1
                    else 0.0
                )
                aug_diagnostics["slice_variance_share"] = (
                    float(aug_diagnostics.get("aug_var", 0.0) / var_total)
                    if var_total > 0
                    else 0.0
                )

            # Store influence functions (always needed for proper inference)
            influence_functions[policy] = influence

        # Store influence functions for later use
        self._influence_functions = influence_functions

        # Create result with clean separation of concerns
        result = EstimationResult(
            estimates=np.array(estimates),
            standard_errors=np.array(standard_errors),
            n_samples_used=n_samples_used,
            method="calibrated_ips" if self.calibrate else "raw_ips",
            influence_functions=influence_functions,
            metadata={
                "target_policies": list(self.sampler.target_policies),
                "iic_diagnostics": self._iic_diagnostics if self.use_iic else None,
                "calibration_method": "simcal" if self.calibrate else None,
                "ess_floor": self.ess_floor,
                "var_cap": self.var_cap,
                "calibration_info": self._calibration_info,  # TODO: Move to diagnostics
                "slice_augmentation": self._aug_diagnostics,  # Oracle slice augmentation info
            },
        )

        # Build and attach diagnostics directly
        diagnostics = self._build_diagnostics(result)
        result.diagnostics = diagnostics
        self._diagnostics = diagnostics

        # Store for later access
        self._results = result

        return result

    def get_raw_weights(self, target_policy: str) -> Optional[np.ndarray]:
        """Get raw (uncalibrated) importance weights.

        Args:
            target_policy: Name of target policy

        Returns:
            Array of raw weights or None if not available
        """
        return self.sampler.compute_importance_weights(
            target_policy, clip_weight=None, mode="raw"
        )

    def get_calibration_info(self, target_policy: str) -> Optional[Dict]:
        """Get calibration information for a policy.

        Args:
            target_policy: Name of target policy

        Returns:
            Dictionary with calibration details or None
        """
        return self._calibration_info.get(target_policy)

    def _build_diagnostics(self, result: EstimationResult) -> IPSDiagnostics:
        """Build simplified diagnostics for this estimation.

        Args:
            result: The estimation result

        Returns:
            IPSDiagnostics object
        """
        # Get dataset info
        dataset = getattr(self.sampler, "dataset", None) or getattr(
            self.sampler, "_dataset", None
        )
        n_total = 0
        if dataset:
            n_total = (
                dataset.n_samples
                if hasattr(dataset, "n_samples")
                else len(dataset.samples)
            )

        # Build estimates dict
        estimates_dict = {}
        se_dict = {}
        policies = list(self.sampler.target_policies)
        for i, policy in enumerate(policies):
            if i < len(result.estimates):
                estimates_dict[policy] = float(result.estimates[i])
                se_dict[policy] = float(result.standard_errors[i])

        # Compute weight diagnostics
        ess_per_policy = {}
        max_weight_per_policy = {}
        tail_indices = {}
        status_per_policy = {}
        hellinger_per_policy = {}  # New: Hellinger affinity per policy
        overall_ess = 0.0
        total_n = 0

        for policy in policies:
            weights = self.get_weights(policy)
            if weights is not None and len(weights) > 0:
                w_diag = compute_weight_diagnostics(weights, policy, compute_hill=True)
                ess_per_policy[policy] = w_diag["ess_fraction"]
                max_weight_per_policy[policy] = w_diag["max_weight"]
                status_per_policy[policy] = w_diag["status"]  # Store per-policy status

                # Hill tail index is now computed in compute_weight_diagnostics
                if "tail_index" in w_diag:
                    tail_indices[policy] = w_diag["tail_index"]
                else:
                    tail_indices[policy] = None

                # Compute Hellinger affinity for this policy (use raw weights)
                raw_weights = self.get_raw_weights(policy)
                if raw_weights is not None and len(raw_weights) > 0:
                    from ..diagnostics.overlap import hellinger_affinity

                    hellinger_per_policy[policy] = hellinger_affinity(raw_weights)

                # Track overall
                n = len(weights)
                overall_ess += w_diag["ess_fraction"] * n
                total_n += n

        # Compute overall weight ESS
        weight_ess = overall_ess / total_n if total_n > 0 else 0.0

        # Compute overall Hellinger affinity (average across policies)
        overall_hellinger = None
        overlap_quality = None
        if hellinger_per_policy:
            overall_hellinger = float(np.mean(list(hellinger_per_policy.values())))
            # Determine overlap quality based on Hellinger
            if overall_hellinger < 0.20:
                overlap_quality = "catastrophic"
            elif overall_hellinger < 0.35:
                overlap_quality = "poor"
            elif overall_hellinger < 0.50:
                overlap_quality = "marginal"
            else:
                overlap_quality = "good"

        # Determine status based on ESS, Hellinger, and tail indices
        worst_tail_idx = min(
            (idx for idx in tail_indices.values() if idx is not None),
            default=float("inf"),
        )

        # Include Hellinger in status determination
        if overlap_quality == "catastrophic" or weight_ess < 0.01:
            weight_status = Status.CRITICAL
        elif worst_tail_idx < 1.5:  # Very heavy tails
            weight_status = Status.CRITICAL
        elif overlap_quality == "poor" or weight_ess < 0.1:
            weight_status = Status.WARNING
        elif worst_tail_idx < 2.0:  # Heavy tails (infinite variance)
            weight_status = Status.WARNING
        else:
            weight_status = Status.GOOD

        # Get calibration info if available
        calibration_rmse = None
        calibration_r2 = None
        n_oracle_labels = None

        # If dataset has calibration info in metadata
        if dataset and hasattr(dataset, "metadata"):
            cal_info = dataset.metadata.get("calibration_info", {})
            calibration_rmse = cal_info.get("rmse")
            calibration_r2 = cal_info.get("r2")  # May be None if not computed
            n_oracle_labels = cal_info.get("n_oracle")

        # Store tail indices in result metadata
        if tail_indices:
            result.metadata["tail_indices"] = tail_indices

        # Create IPSDiagnostics with new overlap metrics
        diagnostics = IPSDiagnostics(
            estimator_type="CalibratedIPS",
            method="calibrated_ips" if self.calibrate else "raw_ips",
            n_samples_total=n_total,
            n_samples_valid=self.sampler.n_valid_samples,
            n_policies=len(policies),
            policies=policies,
            estimates=estimates_dict,
            standard_errors=se_dict,
            n_samples_used=result.n_samples_used,
            weight_ess=weight_ess,
            weight_status=weight_status,
            ess_per_policy=ess_per_policy,
            max_weight_per_policy=max_weight_per_policy,
            status_per_policy=status_per_policy,
            tail_indices=tail_indices,  # Use Hill indices instead of tail ratios
            # New overlap metrics
            hellinger_affinity=overall_hellinger,
            hellinger_per_policy=hellinger_per_policy if hellinger_per_policy else None,
            overlap_quality=overlap_quality,
            # Calibration fields
            calibration_rmse=calibration_rmse,
            calibration_r2=calibration_r2,
            n_oracle_labels=n_oracle_labels,
        )

        return diagnostics

    def get_diagnostics(self) -> Optional[IPSDiagnostics]:
        """Get the diagnostics object."""
        return self._diagnostics


=== ./cje/estimators/dr_base.py ===

"""Base class for Doubly Robust (DR) estimators.

DR estimators combine a direct method (outcome model) with an IPS correction
to achieve better bias-variance tradeoffs and double robustness properties.
"""

import numpy as np
from typing import Dict, List, Optional, Any, Union
import logging
import dataclasses
from pathlib import Path

from .calibrated_ips import CalibratedIPS
from .base_estimator import BaseCJEEstimator
from .outcome_models import IsotonicOutcomeModel, CalibratorBackedOutcomeModel
from ..data.models import EstimationResult
from ..diagnostics import DRDiagnostics, IPSDiagnostics
from ..data.precomputed_sampler import PrecomputedSampler
from ..data.fresh_draws import FreshDrawDataset, validate_fresh_draws
from ..diagnostics.dr import (
    compute_dr_policy_diagnostics,
    compute_orthogonality_score,
    compute_dm_ips_decomposition,
)

logger = logging.getLogger(__name__)


class DREstimator(BaseCJEEstimator):
    """Base class for Doubly Robust estimators with flexible weight method.

    Key insight: DR = Direct Method + IPS correction
    This class uses CalibratedIPS for the importance weighting component,
    which can operate in calibrated or raw mode for flexibility.

    The DR formula from the paper (equation 13):
    V_DR(π') = (1/n) Σ [g(X_i, A'_i, S'_i) + W_i * (R_i - g(X_i, A_i, S_i))]

    Where:
    - g is the outcome model (uses cross-fitted isotonic calibration)
    - A'_i are pre-generated fresh draws from the target policy
    - S'_i are pre-evaluated judge scores on fresh draws
    - W_i are the importance weights (raw or calibrated)
    - R_i are the rewards on logged data (from full calibration model)

    Args:
        sampler: PrecomputedSampler with logged data
        outcome_model: Outcome model for predictions (default: IsotonicOutcomeModel)
        n_folds: Number of cross-fitting folds (default 5)
        use_calibrated_weights: If True, use SIMCal calibration; if False, use raw weights (default True)
        calibrator: Optional calibrator for CalibratorBackedOutcomeModel
        **kwargs: Additional arguments passed to the base class (e.g., oracle_slice_config)
    """

    def __init__(
        self,
        sampler: PrecomputedSampler,
        outcome_model: Optional[Any] = None,
        n_folds: int = 5,
        use_calibrated_weights: bool = True,
        calibrator: Optional[Any] = None,
        random_seed: int = 42,
        run_diagnostics: bool = True,
        **kwargs: Any,
    ):
        # Pass oracle_slice_config to base class (now handles it for all estimators)
        super().__init__(
            sampler=sampler,
            run_diagnostics=run_diagnostics,
            diagnostic_config=None,  # Will use defaults
            **kwargs,  # Passes oracle_slice_config if provided
        )

        self.n_folds = n_folds
        self.calibrator = calibrator
        self.use_calibrated_weights = use_calibrated_weights
        self.random_seed = random_seed

        # Initialize the IPS estimator with appropriate mode
        self.ips_estimator: CalibratedIPS
        # Pass calibrator to CalibratedIPS for DR-aware direction selection if calibrating
        ips_kwargs = {
            "calibrate": use_calibrated_weights,
            "run_diagnostics": run_diagnostics,
        }
        if use_calibrated_weights and calibrator is not None:
            ips_kwargs["calibrator"] = calibrator

        self.ips_estimator = CalibratedIPS(sampler, **ips_kwargs)

        logger.info(
            f"Using CalibratedIPS with calibrate={use_calibrated_weights} for importance weights in DR"
        )

        # IMPORTANT: Share the IPS estimator's augmentation object
        # DR uses IPS weights, so it should use IPS's m(S) fitting
        self.oracle_augmentation = self.ips_estimator.oracle_augmentation

        # Choose default outcome model based on available calibrator
        if outcome_model is None:
            if (
                calibrator is not None
                and hasattr(calibrator, "_fold_models")
                and calibrator._fold_models
            ):
                # We have a cross-fitted calibrator with standard isotonic models, use it for outcome model
                logger.info(
                    "Using CalibratorBackedOutcomeModel (reusing calibration models)"
                )
                outcome_model = CalibratorBackedOutcomeModel(
                    calibrator, n_folds=n_folds
                )
            else:
                # Check if any samples have cv_fold metadata
                has_cv_fold = any(
                    "cv_fold" in s.metadata
                    for s in sampler.dataset.samples[
                        : min(10, len(sampler.dataset.samples))
                    ]
                )

                if has_cv_fold:
                    logger.warning(
                        "Samples have cv_fold metadata but no calibrator provided. "
                        "Consider passing calibrator from calibrate_dataset() for optimal DR."
                    )

                # Fall back to standard isotonic outcome model
                # Pass calibrator if available for proper index transformation
                outcome_model = IsotonicOutcomeModel(
                    n_folds=n_folds, calibrator=calibrator
                )
        self.outcome_model = outcome_model

        # Storage for fresh draws (added via add_fresh_draws)
        self._fresh_draws: Dict[str, FreshDrawDataset] = {}
        self._outcome_fitted = False

        # Store components for diagnostics
        self._dm_component: Dict[str, np.ndarray] = {}
        self._ips_correction: Dict[str, np.ndarray] = {}
        self._fresh_rewards: Dict[str, np.ndarray] = {}
        self._outcome_predictions: Dict[str, np.ndarray] = {}
        self._orthogonality_scores: Dict[str, Dict[str, Any]] = {}
        self._dm_ips_decompositions: Dict[str, Dict[str, Any]] = {}

        # Note: Fold assignments are now computed on-demand from prompt_ids
        # This ensures correct folds even for filtered data

    def add_fresh_draws(self, policy: str, fresh_draws: FreshDrawDataset) -> None:
        """Add pre-generated fresh draws for a target policy.

        Fresh draws must have complete coverage - every logged sample with
        a valid importance weight for this policy must have corresponding
        fresh draws.

        Args:
            policy: Target policy name
            fresh_draws: Pre-generated fresh draw dataset

        Raises:
            ValueError: If fresh draws don't have complete coverage
        """
        # Validate coverage
        validate_fresh_draws(fresh_draws, self.sampler.dataset, policy)

        # Store the fresh draws
        self._fresh_draws[policy] = fresh_draws

        logger.info(
            f"Added fresh draws for policy '{policy}': "
            f"{len(fresh_draws.samples)} samples, "
            f"{fresh_draws.draws_per_prompt} draws/prompt"
        )

    def _auto_load_fresh_draws(self) -> None:
        """Attempt to auto-load fresh draws from standard locations.

        Looks for fresh draws in:
        1. Same directory as dataset
        2. responses/ subdirectory
        3. fresh_draws/ subdirectory
        """
        logger.info("Attempting to auto-load fresh draws...")

        # Try to infer data directory from sampler's dataset path if available
        data_dir = None

        # Check if sampler has dataset_path attribute or metadata
        if hasattr(self.sampler, "dataset_path"):
            data_dir = Path(self.sampler.dataset_path).parent
            logger.debug(f"Found dataset_path on sampler: {self.sampler.dataset_path}")
        elif (
            hasattr(self.sampler, "metadata")
            and "dataset_path" in self.sampler.metadata
        ):
            data_dir = Path(self.sampler.metadata["dataset_path"]).parent
            logger.debug(
                f"Found dataset_path in sampler metadata: {self.sampler.metadata['dataset_path']}"
            )
        else:
            # Try current directory and parent
            logger.debug(f"No dataset_path found, checking cwd: {Path.cwd()}")
            for potential_dir in [Path.cwd(), Path.cwd().parent]:
                if (potential_dir / "data").exists():
                    data_dir = potential_dir / "data"
                    logger.debug(f"Found data directory at: {data_dir}")
                    break

        if data_dir is None:
            logger.warning(
                "Could not determine data directory for auto-loading fresh draws"
            )
            return

        # Try to load fresh draws for each policy
        from ..data.fresh_draws import load_fresh_draws_auto

        for policy in self.sampler.target_policies:
            if policy in self._fresh_draws:
                # Already loaded
                continue

            try:
                fresh_draws = load_fresh_draws_auto(data_dir, policy, verbose=False)
                self._fresh_draws[policy] = fresh_draws
                logger.info(f"Auto-loaded fresh draws for policy '{policy}'")
            except Exception as e:
                logger.debug(f"Could not auto-load fresh draws for '{policy}': {e}")

    def _compute_policy_diagnostics(
        self, policy: str, estimate: float
    ) -> Dict[str, Any]:
        """Compute diagnostics for a single policy.

        This helper method ensures consistent diagnostic computation across
        all DR estimator subclasses.

        Args:
            policy: Policy name
            estimate: The DR estimate for this policy

        Returns:
            Dictionary of diagnostic metrics
        """
        return compute_dr_policy_diagnostics(
            dm_component=self._dm_component.get(policy, np.array([])),
            ips_correction=self._ips_correction.get(policy, np.array([])),
            dr_estimate=estimate,
            fresh_rewards=self._fresh_rewards.get(policy),  # Always use stored rewards
            outcome_predictions=self._outcome_predictions.get(policy),
            influence_functions=self._influence_functions.get(policy),
            unique_folds=list(range(self.n_folds)),
            policy=policy,
        )

    def fit(self) -> None:
        """Fit weight calibration (if applicable) and outcome model."""
        # First fit the IPS weights
        self.ips_estimator.fit()

        # Then fit the outcome model on logged data
        self._fit_outcome_model()

        self._fitted = True

    def _fit_outcome_model(self) -> None:
        """Fit the outcome model on logged data."""
        # Get indices of samples that are valid for at least one policy
        valid_for_any: set[int] = set()
        for policy in self.sampler.target_policies:
            valid_indices = self.sampler._get_valid_indices(policy)
            valid_for_any.update(valid_indices)

        # Sort to maintain order
        valid_indices_list = sorted(valid_for_any)

        # Upfront validation: Check all samples have judge scores
        missing_judge_scores = []
        invalid_judge_scores = []
        for idx in valid_indices_list:
            sample = self.sampler.dataset.samples[idx]
            if "judge_score" not in sample.metadata:
                missing_judge_scores.append((idx, sample.prompt_id))
            elif sample.metadata["judge_score"] is None:
                invalid_judge_scores.append((idx, sample.prompt_id))
            elif not isinstance(sample.metadata["judge_score"], (int, float)):
                invalid_judge_scores.append((idx, sample.prompt_id))

        if missing_judge_scores:
            example_ids = [str(pid) for _, pid in missing_judge_scores[:3]]
            raise ValueError(
                f"DR requires judge_score for all samples. Missing {len(missing_judge_scores)} scores. "
                f"Example prompt_ids: {example_ids}. "
                f"Run calibrate_dataset(..., enable_cross_fit=True) with judge_field specified."
            )

        if invalid_judge_scores:
            example_ids = [str(pid) for _, pid in invalid_judge_scores[:3]]
            raise ValueError(
                f"DR requires numeric judge_score for all samples. {len(invalid_judge_scores)} invalid. "
                f"Example prompt_ids: {example_ids}."
            )

        # Collect logged data
        prompts = []
        responses = []
        rewards = []
        judge_scores = []
        valid_fold_assignments = []

        for idx in valid_indices_list:
            sample = self.sampler.dataset.samples[idx]
            prompts.append(sample.prompt)
            responses.append(sample.response)

            # Get calibrated reward (from full model)
            if sample.reward is not None:
                rewards.append(sample.reward)
            else:
                raise ValueError("All samples must have calibrated rewards for DR")

            # Get judge score from metadata
            if "judge_score" in sample.metadata:
                judge_scores.append(sample.metadata["judge_score"])
            else:
                raise ValueError("All samples must have judge scores for DR")

            # Get fold assignment using unified system
            # Note: We compute fold from prompt_id to handle filtered data correctly
            from ..data.folds import get_fold

            fold = get_fold(sample.prompt_id, self.n_folds, self.random_seed)
            valid_fold_assignments.append(fold)

        rewards_array = np.array(rewards)
        judge_scores_array = np.array(judge_scores)
        fold_assignments_array = (
            np.array(valid_fold_assignments) if valid_fold_assignments else None
        )

        # Pass fold assignments to outcome model
        self.outcome_model.fit(
            prompts,
            responses,
            rewards_array,
            judge_scores_array,
            fold_assignments_array,
        )

        # Store the valid indices for later use
        self._outcome_valid_indices = valid_indices_list

        # Precompute prompt_id to fold mapping for O(1) lookup in estimate()
        self._promptid_to_fold = {}
        if fold_assignments_array is not None:
            for idx, fold in zip(valid_indices_list, fold_assignments_array):
                sample = self.sampler.dataset.samples[idx]
                pid = str(sample.prompt_id)
                self._promptid_to_fold[pid] = int(fold)

        self._outcome_fitted = True

        logger.info(f"Fitted outcome model on {len(prompts)} logged samples")

    def estimate(self) -> EstimationResult:
        """Compute DR estimates for all target policies.

        DR formula: V_DR(π') = E[g(X, A', S')] + E[W * (R - g(X, A, S))]
        Where the first term is the Direct Method and second is IPS correction.

        Will attempt to auto-load fresh draws if not already added.
        """
        self._validate_fitted()

        # Auto-load fresh draws if not already loaded
        self._auto_load_fresh_draws()

        estimates = []
        standard_errors = []
        n_samples_used = {}

        for policy in self.sampler.target_policies:
            # Check fresh draws are available
            if policy not in self._fresh_draws:
                raise ValueError(
                    f"No fresh draws for policy '{policy}'. "
                    f"Tried auto-loading but failed. Call add_fresh_draws() manually."
                )

            # Get components
            weights = self.ips_estimator.get_weights(policy)
            if weights is None:
                # Check if this is a no_overlap case
                # get_diagnostics() doesn't take policy argument, it returns all
                logger.warning(f"No weights for policy '{policy}', skipping")
                estimates.append(np.nan)
                standard_errors.append(np.nan)
                n_samples_used[policy] = 0
                continue

            # Get rewards (already filtered to valid samples)
            data = self.sampler.get_data_for_policy(policy)
            if data is None:
                logger.warning(f"No data for policy '{policy}', skipping")
                estimates.append(np.nan)
                standard_errors.append(np.nan)
                n_samples_used[policy] = 0
                continue
            logged_rewards = np.array([d["reward"] for d in data])

            # Sanity check: weights and logged data should be aligned
            if len(weights) != len(logged_rewards):
                raise ValueError(
                    f"Weights and logged data length mismatch for policy '{policy}': "
                    f"weights={len(weights)}, data={len(logged_rewards)}"
                )

            # Get logged data for outcome model
            logged_prompts = [d["prompt"] for d in data]
            logged_responses = [d["response"] for d in data]
            logged_scores = np.array([d.get("judge_score") for d in data])
            # Require prompt_ids for DR (no fallback to index)
            logged_prompt_ids = []
            for i, d in enumerate(data):
                if "prompt_id" not in d:
                    raise ValueError(
                        f"Data entry {i} for policy '{policy}' missing 'prompt_id'. "
                        f"DR estimation requires prompt_id to align with fresh draws."
                    )
                logged_prompt_ids.append(str(d["prompt_id"]))

            # Get fold assignments using precomputed mapping (O(1) lookups)
            # Strict mode: error if any prompt_id is missing fold assignment
            valid_fold_ids_list = []
            if self._promptid_to_fold:
                unknown_pids = []
                for pid in logged_prompt_ids:
                    if pid not in self._promptid_to_fold:
                        unknown_pids.append(pid)
                    else:
                        valid_fold_ids_list.append(self._promptid_to_fold[pid])

                if unknown_pids:
                    raise ValueError(
                        f"Missing fold assignments for {len(unknown_pids)} samples in policy '{policy}'. "
                        f"Example prompt_ids: {unknown_pids[:3]}. "
                        f"Ensure calibration was done with enable_cross_fit=True or provide explicit fold assignments."
                    )
            else:
                raise ValueError(
                    f"No fold assignments available for DR estimation. "
                    f"Ensure calibration was done with enable_cross_fit=True."
                )
            valid_fold_ids = np.array(valid_fold_ids_list)

            # Get outcome model predictions for logged data (using cross-fitted models)
            # Both IsotonicOutcomeModel and BaseOutcomeModel-derived classes need fold_ids
            if hasattr(self.outcome_model, "predict"):
                # Our outcome models accept fold_ids for cross-fitting
                g_logged = self.outcome_model.predict(
                    logged_prompts, logged_responses, logged_scores, valid_fold_ids
                )
            else:
                # Fallback for other models
                g_logged = self.outcome_model.predict(
                    logged_prompts, logged_responses, logged_scores
                )

            # Get fresh draws
            fresh_dataset = self._fresh_draws[policy]

            # Collect fresh scores for each logged sample
            g_fresh_all = []
            fresh_draw_var_per_prompt_list = []  # For diagnostics

            for i, prompt_id in enumerate(logged_prompt_ids):
                # Get fresh judge scores for this prompt
                if prompt_id is None:
                    raise ValueError(f"Missing prompt_id for sample {i}")
                fresh_scores = fresh_dataset.get_scores_for_prompt_id(prompt_id)

                # Get fresh samples to validate fold assignments
                fresh_samples = fresh_dataset.get_samples_for_prompt_id(prompt_id)

                # Create dummy prompts/responses for outcome model interface
                fresh_prompts = [logged_prompts[i]] * len(fresh_scores)
                fresh_responses = [""] * len(fresh_scores)  # Not used in isotonic model

                # Use same fold for all fresh draws from this prompt
                fresh_fold_ids = np.full(len(fresh_scores), valid_fold_ids[i])

                # Validate that fresh draws have matching fold assignments if available
                for j, fresh_sample in enumerate(fresh_samples):
                    if (
                        fresh_sample.fold_id is not None
                        and fresh_sample.fold_id != valid_fold_ids[i]
                    ):
                        logger.warning(
                            f"Fold mismatch for prompt_id '{prompt_id}': "
                            f"logged fold={valid_fold_ids[i]}, fresh fold={fresh_sample.fold_id}"
                        )

                # Get predictions for fresh draws
                # Note: Our models need fold_ids for cross-fitting
                # They all use the same fold for each prompt's fresh draws
                if hasattr(self.outcome_model, "predict"):
                    g_fresh_prompt = self.outcome_model.predict(
                        fresh_prompts, fresh_responses, fresh_scores, fresh_fold_ids
                    )
                else:
                    # Fallback for other models
                    g_fresh_prompt = self.outcome_model.predict(
                        fresh_prompts, fresh_responses, fresh_scores
                    )

                # Average over draws for this prompt
                g_fresh_all.append(g_fresh_prompt.mean())

                # Track variance for diagnostics
                if len(g_fresh_prompt) > 1:
                    fresh_draw_var_per_prompt_list.append(g_fresh_prompt.var())
                else:
                    fresh_draw_var_per_prompt_list.append(0.0)

            g_fresh = np.array(g_fresh_all)
            fresh_draw_var_per_prompt = np.array(fresh_draw_var_per_prompt_list)

            # Store for MC variance computation later
            # Also track actual draws per prompt (M_i) which may vary
            draws_per_prompt_list = []
            for prompt_id in logged_prompt_ids:
                if prompt_id is not None:
                    fresh_scores = fresh_dataset.get_scores_for_prompt_id(prompt_id)
                    draws_per_prompt_list.append(len(fresh_scores))
                else:
                    draws_per_prompt_list.append(1)  # Fallback

            if not hasattr(self, "_fresh_draw_stats"):
                self._fresh_draw_stats = {}
            self._fresh_draw_stats[policy] = {
                "variances": fresh_draw_var_per_prompt,
                "draws_per_prompt": np.array(
                    draws_per_prompt_list
                ),  # Now per-prompt M_i
                "n_prompts": len(fresh_draw_var_per_prompt),
            }

            # Sanity check: weights should have mean approximately 1.0
            weights_mean = weights.mean()
            # With mean-one calibration, weights should be very close to 1.0
            # Allow small tolerance for numerical precision
            if not (0.99 <= weights_mean <= 1.01):
                weights_min = weights.min()
                weights_max = weights.max()
                weights_std = weights.std()
                logger.warning(
                    f"Weights for policy '{policy}' deviate from expected mean=1.0: "
                    f"mean={weights_mean:.3f}, std={weights_std:.3f}, "
                    f"min={weights_min:.3e}, max={weights_max:.3e}. "
                    f"This may indicate calibration issues or poor policy overlap."
                )

            # DR estimate components
            dm_term = g_fresh.mean()  # Direct method term
            ips_correction_base = weights * (logged_rewards - g_logged)

            # Add oracle slice augmentation
            # DR shares the IPS estimator's augmentation (which has m̂(S) fitted)
            # The augmentation corrects for uncertainty in the calibrated rewards f̂(S)
            aug_vector, aug_diagnostics = self.oracle_augmentation.compute_augmentation(
                policy,
                logged_rewards,  # Always use calibrated rewards
                data,
                self.sampler.dataset.samples,
            )
            self._aug_diagnostics[policy] = aug_diagnostics

            # Total IPS correction with augmentation
            ips_correction = (ips_correction_base + aug_vector).mean()
            dr_estimate = dm_term + ips_correction

            # Store components for diagnostics (avoid recomputation later)
            self._dm_component[policy] = g_fresh
            self._ips_correction[policy] = (
                ips_correction_base + aug_vector
            )  # Include augmentation
            self._fresh_rewards[policy] = logged_rewards  # Actually logged rewards
            self._outcome_predictions[policy] = g_logged

            # Compute standard error using influence function
            # Include augmentation in the influence function
            if_contributions = g_fresh + ips_correction_base + aug_vector - dr_estimate

            # Apply IIC for variance reduction (if enabled)
            if_contributions = self._apply_iic(if_contributions, policy)

            # Base SE from influence functions (across-prompt variance)
            base_se = np.std(if_contributions, ddof=1) / np.sqrt(len(if_contributions))

            # Add Monte Carlo variance component from finite fresh draws
            mc_var = 0.0
            if hasattr(self, "_fresh_draw_stats") and policy in self._fresh_draw_stats:
                stats = self._fresh_draw_stats[policy]
                fresh_var = stats["variances"]
                M = stats["draws_per_prompt"]  # Now per-prompt M_i array

                # MC variance: (1/n^2) * sum_i (1-w_i)^2 * (s2_i / M_i)
                # Handle variable M_i per prompt
                mc_var = np.sum(
                    ((1.0 - weights) ** 2) * (fresh_var / np.maximum(M, 1))
                ) / (len(data) ** 2)

                # Store MC diagnostics
                if not hasattr(self, "_mc_diagnostics"):
                    self._mc_diagnostics = {}
                self._mc_diagnostics[policy] = {
                    "base_se": base_se,
                    "mc_var": mc_var,
                    "mc_share": (
                        mc_var / (base_se**2 + mc_var)
                        if (base_se**2 + mc_var) > 0
                        else 0
                    ),
                    "avg_draws_per_prompt": float(M.mean()),
                    "min_draws_per_prompt": int(M.min()),
                    "max_draws_per_prompt": int(M.max()),
                }

            # Total SE including MC component
            total_var = base_se**2 + mc_var
            se = np.sqrt(total_var)

            if mc_var > 0:
                logger.debug(
                    f"SE for '{policy}': base={base_se:.4f}, with MC={se:.4f} "
                    f"(MC adds {100*mc_var/total_var:.1f}% to variance)"
                )

            # Store influence functions (always needed for proper inference)
            self._influence_functions[policy] = if_contributions
            logger.debug(
                f"Stored {len(if_contributions)} influence values for {policy}"
            )

            estimates.append(dr_estimate)
            standard_errors.append(se)
            n_samples_used[policy] = len(data)

            logger.info(
                f"DR estimate for policy '{policy}': {dr_estimate:.4f} ± {se:.4f} "
                f"(DM={dm_term:.4f}, IPS_corr={ips_correction:.4f})"
            )

            # Compute orthogonality score (new)
            ortho_result = compute_orthogonality_score(
                weights=weights,
                rewards=logged_rewards,
                outcome_predictions=g_logged,
                return_ci=True,
            )
            self._orthogonality_scores[policy] = ortho_result

            # Compute DM-IPS decomposition (new)
            decomp_result = compute_dm_ips_decomposition(
                g_hat=g_fresh,
                weights=weights,
                rewards=logged_rewards,
                q_hat=g_logged,
            )
            self._dm_ips_decompositions[policy] = decomp_result

        # Build DR diagnostics using stored components
        dr_diagnostics_per_policy: Dict[str, Dict[str, Any]] = {}

        for idx, policy in enumerate(self.sampler.target_policies):
            if policy not in self._dm_component or np.isnan(estimates[idx]):
                continue

            # Use helper method for consistent diagnostic computation
            dr_diagnostics_per_policy[policy] = self._compute_policy_diagnostics(
                policy, estimates[idx]
            )

        # Collect oracle augmentation diagnostics if available
        oracle_aug_diagnostics: Dict[str, Any] = {}
        if self.use_calibrated_weights and hasattr(
            self.ips_estimator, "_aug_diagnostics"
        ):
            oracle_aug_diagnostics = self.ips_estimator._aug_diagnostics.copy()

        # Add DR-specific metadata
        dr_metadata = {
            "fresh_draws_policies": list(self._fresh_draws.keys()),
            "cross_fitted": True,
            "n_folds": self.n_folds,
            "oracle_slice_augmentation": oracle_aug_diagnostics,  # Add augmentation info
        }

        # Add MC variance diagnostics if available
        if hasattr(self, "_mc_diagnostics"):
            dr_metadata["mc_variance_diagnostics"] = self._mc_diagnostics
            dr_metadata["mc_variance_included"] = True

        # Create overview
        dr_overview = {}
        if dr_diagnostics_per_policy:
            dr_overview = {
                "policies": list(dr_diagnostics_per_policy.keys()),
                "dm_vs_ips": {
                    p: (d["dm_mean"], d["ips_corr_mean"])
                    for p, d in dr_diagnostics_per_policy.items()
                },
                "worst_if_tail_ratio_99_5": max(
                    d.get("if_tail_ratio_99_5", 0.0)
                    for d in dr_diagnostics_per_policy.values()
                ),
            }

            # For TMLE specifically (will be overridden in subclass)
            if self.__class__.__name__ == "TMLEEstimator":
                dr_overview["tmle_score_abs_mean"] = {
                    p: abs(d["score_mean"])
                    for p, d in dr_diagnostics_per_policy.items()
                }

        # Build metadata (keep dr_diagnostics for backward compatibility with visualization)
        metadata = {
            "target_policies": list(self.sampler.target_policies),
            "weight_method": "calibrated" if self.use_calibrated_weights else "raw",
            "dr_diagnostics": dr_diagnostics_per_policy,  # Keep for visualization
            "dr_overview": dr_overview,
            "orthogonality_scores": self._orthogonality_scores,  # New: orthogonality diagnostics
            "dm_ips_decompositions": self._dm_ips_decompositions,  # New: DM-IPS breakdown
            "dr_influence": self._influence_functions,  # Store influence functions for CF-bits analysis
        }

        # Get IPS diagnostics if available
        ips_diag = None
        if hasattr(self.ips_estimator, "get_diagnostics"):
            ips_diag = self.ips_estimator.get_diagnostics()

        # Build DR diagnostics directly
        dr_diagnostics = self._build_dr_diagnostics(
            estimates,
            standard_errors,
            n_samples_used,
            dr_diagnostics_per_policy,
            ips_diag,
        )

        # Add IIC diagnostics to metadata
        if self.use_iic and self._iic_diagnostics:
            metadata["iic_diagnostics"] = self._iic_diagnostics

        return EstimationResult(
            estimates=np.array(estimates),
            standard_errors=np.array(standard_errors),
            n_samples_used=n_samples_used,
            method="dr_base",
            influence_functions=self._influence_functions,
            diagnostics=dr_diagnostics,
            metadata=metadata,
        )

    def _build_dr_diagnostics(
        self,
        estimates: List[float],
        standard_errors: List[float],
        n_samples_used: Dict[str, int],
        dr_diagnostics_per_policy: Dict[str, Dict[str, Any]],
        ips_diagnostics: Optional[IPSDiagnostics],
    ) -> DRDiagnostics:
        """Build DRDiagnostics object from components.

        Args:
            estimates: List of estimates per policy
            standard_errors: List of SEs per policy
            n_samples_used: Dict of samples used per policy
            dr_diagnostics_per_policy: Detailed DR diagnostics
            ips_diagnostics: IPSDiagnostics from internal IPS estimator

        Returns:
            DRDiagnostics object
        """
        # Build estimates/SE dicts
        policies = list(self.sampler.target_policies)
        estimates_dict = {
            p: float(e) for p, e in zip(policies, estimates) if not np.isnan(e)
        }
        se_dict = {
            p: float(se) for p, se in zip(policies, standard_errors) if not np.isnan(se)
        }

        # Extract summary metrics from detailed diagnostics
        r2_values = []
        rmse_values = []
        if_tail_ratios = []

        for policy, diag in dr_diagnostics_per_policy.items():
            if "r2_oof" in diag and diag["r2_oof"] is not None:
                r2_values.append(diag["r2_oof"])
            if "residual_rmse" in diag and diag["residual_rmse"] is not None:
                rmse_values.append(diag["residual_rmse"])
            if "if_tail_ratio_99_5" in diag:
                if_tail_ratios.append(diag["if_tail_ratio_99_5"])
            else:
                # Use a default value if influence functions weren't computed
                if_tail_ratios.append(0.0)

        # Compute ranges
        outcome_r2_range = (min(r2_values), max(r2_values)) if r2_values else (0.0, 0.0)
        outcome_rmse_mean = np.mean(rmse_values) if rmse_values else 0.0
        worst_if_tail = max(if_tail_ratios) if if_tail_ratios else 0.0

        # Build DRDiagnostics
        if ips_diagnostics is not None:
            # Copy fields from IPS diagnostics
            diagnostics = DRDiagnostics(
                estimator_type=f"DR_{ips_diagnostics.estimator_type}",
                method=self.__class__.__name__.lower().replace("estimator", ""),
                n_samples_total=ips_diagnostics.n_samples_total,
                n_samples_valid=ips_diagnostics.n_samples_valid,
                n_policies=len(policies),
                policies=policies,
                estimates=estimates_dict,
                standard_errors=se_dict,
                n_samples_used=n_samples_used,
                # Weight fields from IPS
                weight_ess=ips_diagnostics.weight_ess,
                weight_status=ips_diagnostics.weight_status,
                ess_per_policy=ips_diagnostics.ess_per_policy,
                max_weight_per_policy=ips_diagnostics.max_weight_per_policy,
                weight_tail_ratio_per_policy=ips_diagnostics.weight_tail_ratio_per_policy,
                # Calibration fields (may be None)
                calibration_rmse=ips_diagnostics.calibration_rmse,
                calibration_r2=ips_diagnostics.calibration_r2,
                calibration_coverage=ips_diagnostics.calibration_coverage,
                n_oracle_labels=ips_diagnostics.n_oracle_labels,
                # DR-specific fields
                dr_cross_fitted=True,
                dr_n_folds=self.n_folds,
                outcome_r2_range=outcome_r2_range,
                outcome_rmse_mean=outcome_rmse_mean,
                worst_if_tail_ratio=worst_if_tail,
                dr_diagnostics_per_policy=dr_diagnostics_per_policy,
                dm_ips_decompositions=self._dm_ips_decompositions,
                orthogonality_scores=self._orthogonality_scores,
                influence_functions=self._influence_functions,
            )
        else:
            # No IPS diagnostics available, create minimal version
            from ..diagnostics import Status

            diagnostics = DRDiagnostics(
                estimator_type="DR",
                method=self.__class__.__name__.lower().replace("estimator", ""),
                n_samples_total=len(self.sampler.dataset.samples),
                n_samples_valid=self.sampler.n_valid_samples,
                n_policies=len(policies),
                policies=policies,
                estimates=estimates_dict,
                standard_errors=se_dict,
                n_samples_used=n_samples_used,
                # Minimal weight fields
                weight_ess=0.0,
                weight_status=Status.WARNING,
                ess_per_policy={},
                max_weight_per_policy={},
                weight_tail_ratio_per_policy={},
                # DR-specific fields
                dr_cross_fitted=True,
                dr_n_folds=self.n_folds,
                outcome_r2_range=outcome_r2_range,
                outcome_rmse_mean=outcome_rmse_mean,
                worst_if_tail_ratio=worst_if_tail,
                dr_diagnostics_per_policy=dr_diagnostics_per_policy,
                dm_ips_decompositions=self._dm_ips_decompositions,
                orthogonality_scores=self._orthogonality_scores,
                influence_functions=self._influence_functions,
            )

        return diagnostics

    def get_weights(self, policy: str) -> Optional[np.ndarray]:
        """Get importance weights for a policy.

        Args:
            policy: Target policy name

        Returns:
            Array of importance weights or None if not fitted
        """
        if not self._fitted:
            return None
        return self.ips_estimator.get_weights(policy)

    def get_weight_diagnostics(self) -> Optional[IPSDiagnostics]:
        """Get weight diagnostics from internal IPS estimator.

        This helper method provides easy access to weight diagnostics
        for DR estimators, which internally use an IPS estimator for weights.

        Returns:
            IPSDiagnostics object from the internal IPS estimator, or None
        """
        if hasattr(self.ips_estimator, "get_diagnostics"):
            diag = self.ips_estimator.get_diagnostics()
            # Ensure it's IPSDiagnostics (not some other type)
            if isinstance(diag, IPSDiagnostics):
                return diag
        return None

    def get_diagnostics(self) -> Dict[str, Any]:
        """Get diagnostic information about the estimation.

        Returns:
            Dictionary with diagnostic metrics
        """
        diagnostics: Dict[str, Any] = {
            "weight_method": "calibrated" if self.use_calibrated_weights else "raw",
            "outcome_model": type(self.outcome_model).__name__,
            "n_folds": self.n_folds,
            "policies_with_fresh_draws": list(self._fresh_draws.keys()),
        }

        # Add IPS diagnostics if available
        if hasattr(self.ips_estimator, "get_diagnostics"):
            ips_diag = self.ips_estimator.get_diagnostics()
            # ips_diag is an IPSDiagnostics object, not a dict
            if ips_diag is not None:
                # Convert to dict for legacy compatibility
                diagnostics["ips_weight_ess"] = ips_diag.weight_ess
                diagnostics["ips_n_samples"] = ips_diag.n_samples_valid

        return diagnostics

    def get_oracle_jackknife(self, policy: str) -> Optional[np.ndarray]:
        """Compute leave-one-oracle-fold-out estimates for oracle uncertainty.

        This method computes K estimates, each leaving out one fold of oracle samples,
        to quantify uncertainty from the finite oracle slice used for calibration.
        This is used by CF-bits to compute var_oracle for proper IFR_OUA calculation.

        Args:
            policy: Target policy name

        Returns:
            Array of K jackknife estimates, or None if not applicable

        Note:
            The jackknife variance is computed as: var_oracle = (K-1)/K * Var(estimates)
            This represents the additional uncertainty from learning f: judge → oracle
            from a finite sample of oracle labels.
        """
        # Check if we have the necessary components
        if not self._fitted:
            logger.warning("Estimator not fitted, cannot compute oracle jackknife")
            return None

        if self.calibrator is None:
            logger.debug("No calibrator available for oracle jackknife")
            return None

        if not hasattr(self.calibrator, "_fold_models"):
            logger.debug("Calibrator has no fold models for oracle jackknife")
            return None

        if policy not in self._fresh_draws:
            logger.warning(
                f"No fresh draws for policy {policy}, cannot compute oracle jackknife"
            )
            return None

        # Cache jackknife results to avoid recomputation
        if not hasattr(self, "_oracle_jackknife_cache"):
            self._oracle_jackknife_cache: Dict[str, np.ndarray] = {}

        if policy in self._oracle_jackknife_cache:
            return self._oracle_jackknife_cache[policy]

        try:
            # Get the number of folds from calibrator
            n_folds = len(self.calibrator._fold_models)
            jackknife_estimates = []

            # Get base components that don't change
            fresh_draw_data = self._fresh_draws[policy]
            data = self.sampler.get_data_for_policy(policy)
            if data is None:
                logger.warning(f"No data for policy {policy}")
                return None
            weights = self.ips_estimator.get_weights(policy)

            # For each fold, compute leave-that-fold-out estimate
            for fold_id in range(n_folds):
                # Use the model that was trained WITHOUT this fold's oracle samples
                # The calibrator._fold_models[fold_id] was trained on all folds EXCEPT fold_id
                fold_model = self.calibrator._fold_models.get(fold_id)
                if fold_model is None:
                    logger.debug(f"No fold model for fold {fold_id}")
                    continue

                # Get judge scores for logged data
                # Note: get_data_for_policy flattens judge_score to top level
                judge_scores_logged = np.array(
                    [d["judge_score"] for d in data if d.get("judge_score") is not None]
                )

                if len(judge_scores_logged) == 0:
                    logger.warning(f"No judge scores found in data for fold {fold_id}")
                    continue

                # Recalibrate logged rewards with leave-one-out model
                logged_rewards_loo = np.clip(
                    fold_model.predict(judge_scores_logged), 0.0, 1.0
                )

                # Get fresh draw judge scores and recalibrate
                # Need to collect all fresh draw scores across all prompts
                fresh_scores_list = []
                for prompt_id in set(d["prompt_id"] for d in data):
                    prompt_fresh_scores = fresh_draw_data.get_scores_for_prompt_id(
                        prompt_id
                    )
                    fresh_scores_list.extend(prompt_fresh_scores)

                if len(fresh_scores_list) == 0:
                    logger.warning(f"No fresh draw scores found for fold {fold_id}")
                    continue

                judge_scores_fresh = np.array(fresh_scores_list)
                fresh_rewards_loo = np.clip(
                    fold_model.predict(judge_scores_fresh), 0.0, 1.0
                )

                # Get outcome model predictions (these use cross-fitted models already)
                g_logged = self._outcome_predictions[policy]

                # Compute leave-one-out DR estimate
                dm_term = fresh_rewards_loo.mean()
                ips_correction = (weights * (logged_rewards_loo - g_logged)).mean()

                # Note: We're not recomputing augmentation for each fold
                # This is a simplification - proper implementation would recompute
                # augmentation with the leave-one-out calibrator
                # For now, we ignore augmentation in jackknife to focus on main effect

                dr_estimate_loo = dm_term + ips_correction
                jackknife_estimates.append(dr_estimate_loo)

            if len(jackknife_estimates) < 2:
                logger.warning(
                    f"Not enough jackknife estimates for {policy}: {len(jackknife_estimates)}"
                )
                return None

            jackknife_array = np.array(jackknife_estimates)
            self._oracle_jackknife_cache[policy] = jackknife_array

            logger.debug(
                f"Oracle jackknife for {policy}: {len(jackknife_estimates)} estimates, "
                f"mean={jackknife_array.mean():.4f}, std={jackknife_array.std():.4f}"
            )

            return jackknife_array

        except Exception as e:
            logger.error(
                f"Failed to compute oracle jackknife for {policy}: {e}", exc_info=True
            )
            return None


class DRCPOEstimator(DREstimator):
    """DR-CPO: Default DR estimator using isotonic outcome model.

    This is the simplest DR variant that uses g(x,a,s) = f(s) where
    f is the isotonic calibration function learned from judge scores.

    For logged data: Uses cross-fitted predictions f^(-k)(S_i)
    For fresh draws: Uses cross-fitted predictions f^(-k)(S'_i)

    This is theoretically sound under the monotone sufficiency assumption
    (A-J2S) from the paper: E[Y | X, A, S] = μ(S) for some non-decreasing μ.

    By default uses IsotonicOutcomeModel with cross-fitting.
    """

    def __init__(
        self,
        sampler: PrecomputedSampler,
        outcome_model: Optional[Any] = None,
        n_folds: int = 5,
        use_calibrated_weights: bool = True,
        calibrator: Optional[Any] = None,
        random_seed: int = 42,
        **kwargs: Any,
    ):
        # Pass everything to parent - it will choose the right outcome model
        super().__init__(
            sampler=sampler,
            outcome_model=outcome_model,
            n_folds=n_folds,
            use_calibrated_weights=use_calibrated_weights,
            calibrator=calibrator,
            random_seed=random_seed,
            **kwargs,
        )

    def estimate(self) -> EstimationResult:
        """Override to set correct method name."""
        result = super().estimate()
        result.method = "dr_cpo"
        return result


=== ./cje/estimators/mrdr.py ===

# cje/core/mrdr.py
"""
MRDR estimator: policy-specific, cross-fitted weighted isotonic outcome models.

This estimator properly inherits from DREstimator, using the base DR infrastructure
while supporting policy-specific weighted outcome models.
"""

from __future__ import annotations
from typing import Dict, Optional, Any, List
import logging
import numpy as np
from sklearn.isotonic import IsotonicRegression

from .dr_base import DREstimator
from .outcome_models import BaseOutcomeModel
from ..data.precomputed_sampler import PrecomputedSampler
from ..data.models import EstimationResult
from ..data.fresh_draws import FreshDrawDataset

logger = logging.getLogger(__name__)


class WeightedIsotonicOutcomeModel(BaseOutcomeModel):
    """Weighted isotonic outcome model for MRDR.

    Extends BaseOutcomeModel to support sample weights in isotonic regression.
    """

    def __init__(self, n_folds: int = 5, calibrator: Optional[Any] = None):
        super().__init__(n_folds)
        self.sample_weights: Optional[np.ndarray] = None
        self.calibrator = calibrator
        self._promptid_to_fold: Dict[str, int] = {}  # Store for MRDR to access

    def set_weights(self, weights: np.ndarray) -> None:
        """Set the sample weights for training."""
        self.sample_weights = weights

    def _get_fold_fit_kwargs(self, train_mask: np.ndarray) -> dict:
        """Get fold-specific kwargs for training (subsets sample weights)."""
        if self.sample_weights is None:
            return {}
        # Subset weights to match training fold
        return {"sample_weight": self.sample_weights[train_mask]}

    def _fit_single_model(
        self,
        prompts: List[str],
        responses: List[str],
        rewards: np.ndarray,
        judge_scores: np.ndarray,  # These will be pre-transformed by fit() if calibrator exists
        sample_weight: Optional[np.ndarray] = None,
    ) -> Any:
        """Fit a weighted isotonic regression model on training data."""
        model = IsotonicRegression(out_of_bounds="clip")

        # Use provided sample weights (already subset by _get_fold_fit_kwargs)
        if sample_weight is not None:
            model.fit(judge_scores, rewards, sample_weight=sample_weight)
        else:
            model.fit(judge_scores, rewards)
        return model

    def _predict_single_model(
        self,
        model: Any,
        prompts: List[str],
        responses: List[str],
        judge_scores: np.ndarray,  # These will be pre-transformed by predict() if calibrator exists
    ) -> np.ndarray:
        """Predict using the fitted isotonic model."""
        predictions: np.ndarray = model.predict(judge_scores)
        return predictions

    def fit(
        self,
        prompts: List[str],
        responses: List[str],
        rewards: np.ndarray,
        judge_scores: Optional[np.ndarray] = None,
        fold_ids: Optional[np.ndarray] = None,
        prompt_ids: Optional[List[str]] = None,
    ) -> None:
        """Fit with optional prompt_id tracking for cross-fitting."""
        # Store prompt_id to fold mapping if provided
        if fold_ids is not None and prompt_ids is not None:
            self._promptid_to_fold = {
                pid: int(fid) for pid, fid in zip(prompt_ids, fold_ids)
            }

        # Pre-compute transformed indices if calibrator is available
        if self.calibrator is not None and hasattr(self.calibrator, "index"):
            # Get OOF indices for all data at once
            transformed_scores = self.calibrator.index(judge_scores, fold_ids)
        else:
            transformed_scores = judge_scores

        # Call base class fit with transformed scores
        super().fit(prompts, responses, rewards, transformed_scores, fold_ids)

    def predict(
        self,
        prompts: List[str],
        responses: List[str],
        judge_scores: Optional[np.ndarray] = None,
        fold_ids: Optional[np.ndarray] = None,
    ) -> np.ndarray:
        """Predict using cross-fitted models with proper index transformation."""
        if judge_scores is None:
            raise ValueError("judge_scores required for prediction")

        # Transform judge scores if calibrator is available
        if self.calibrator is not None and hasattr(self.calibrator, "index"):
            # For prediction, use the ensemble index (folds=None)
            transformed_scores = self.calibrator.index(judge_scores, folds=None)
        else:
            transformed_scores = judge_scores

        # Call parent predict with transformed scores
        return super().predict(prompts, responses, transformed_scores, fold_ids)


class MRDREstimator(DREstimator):
    """MRDR estimator with policy-specific weighted isotonic outcome models.

    Implements Multiple Robust Doubly Robust (MRDR) estimation with separate
    weighted outcome models for each target policy. The weights (omega) for
    each policy's outcome model are derived from that policy's importance weights.

    Args:
        sampler: PrecomputedSampler with calibrated rewards
        n_folds: Cross-fitting folds (default 5)
        omega_mode: Weighting for the MRDR regression. One of:
            - "w":     |W|         [default; most stable, avoids extreme concentration]
            - "w2":    W^2         [moderate concentration]
            - "snips": (W - 1)^2   [can lead to extreme concentration with high-variance weights]
        min_sample_weight: Floor applied to ω to avoid degenerate 0-weight fits
        use_calibrated_weights: Use CalibratedIPS (default True)
        use_policy_specific_models: If True, fit separate weighted models per policy.
                                   If False, use single shared model (simplified version).
        **kwargs: Passed through to DREstimator
    """

    def __init__(
        self,
        sampler: PrecomputedSampler,
        n_folds: int = 5,
        omega_mode: str = "w",
        min_sample_weight: float = 1e-8,
        use_calibrated_weights: bool = True,
        use_policy_specific_models: bool = True,
        calibrator: Optional[Any] = None,
        **kwargs: Any,
    ):
        if omega_mode not in {"snips", "w2", "w"}:
            raise ValueError(
                f"omega_mode must be one of ['snips','w2','w'], got {omega_mode}"
            )

        # Use standard isotonic as default (will be overridden if policy-specific)
        from .outcome_models import IsotonicOutcomeModel

        # Pass calibrator for proper index transformation with two-stage calibration
        outcome_model = IsotonicOutcomeModel(n_folds=n_folds, calibrator=calibrator)

        # Initialize DR base (which will pass calibrator to CalibratedIPS)
        super().__init__(
            sampler=sampler,
            outcome_model=outcome_model,
            n_folds=n_folds,
            use_calibrated_weights=use_calibrated_weights,
            calibrator=calibrator,
            **kwargs,
        )

        self.omega_mode = omega_mode
        self.min_sample_weight = min_sample_weight
        self.use_policy_specific_models = use_policy_specific_models

        # Store policy-specific models
        self._policy_models: Dict[str, WeightedIsotonicOutcomeModel] = {}

        if use_policy_specific_models:
            logger.info(
                f"MRDREstimator: Using policy-specific weighted models with omega_mode='{omega_mode}'"
            )
        else:
            logger.info(
                "MRDREstimator: Using simplified version with shared outcome model"
            )

    def _omega_from_weights(self, w: np.ndarray, mode: str) -> np.ndarray:
        """Compute MRDR regression weights ω from mean-one IPS weights W."""
        if mode == "snips":
            # Recommended with Hájek (mean-one) weights
            return (w - 1.0) ** 2
        if mode == "w2":
            return w**2
        if mode == "w":
            return np.asarray(np.abs(w))
        raise ValueError(f"Unknown omega_mode: {mode}")

    def fit(self) -> None:
        """Fit weight calibration and policy-specific weighted outcome models.

        For each target policy, fits a separate WeightedIsotonicOutcomeModel
        using omega weights derived from that policy's importance weights.
        """
        # First fit IPS weights using base class
        self.ips_estimator.fit()
        self._fitted = True

        if not self.use_policy_specific_models:
            # Use base implementation with shared model
            super().fit()
            return

        # Build prompt_id -> fold map from dataset metadata (if available)
        # This ensures we reuse the same folds as reward calibration
        cv_map = {}
        if hasattr(self.sampler, "dataset") and self.sampler.dataset:
            cv_map = {
                str(s.prompt_id): int(s.metadata["cv_fold"])
                for s in self.sampler.dataset.samples
                if "cv_fold" in s.metadata and s.metadata["cv_fold"] is not None
            }
            if cv_map:
                logger.info(
                    f"Reusing calibration folds for MRDR: {len(cv_map)} samples with cv_fold metadata"
                )

        # Fit policy-specific weighted models
        for policy in self.sampler.target_policies:
            # Get IPS weights for this policy
            weights = self.get_weights(policy)
            if weights is None:
                logger.warning(f"No weights available for policy '{policy}'. Skipping.")
                continue

            # Compute omega weights for outcome model
            omega = self._omega_from_weights(weights, self.omega_mode)
            omega = np.maximum(
                omega, self.min_sample_weight
            )  # Floor to avoid zero weights

            # Get data for this policy
            data = self.sampler.get_data_for_policy(policy)
            if not data:
                logger.warning(f"No data available for policy '{policy}'. Skipping.")
                continue

            # Extract arrays
            prompts = [d["prompt"] for d in data]
            responses = [d["response"] for d in data]
            rewards = np.array([d["reward"] for d in data], dtype=float)
            judge_scores = np.array([d.get("judge_score") for d in data], dtype=float)
            prompt_ids = [str(d.get("prompt_id")) for d in data]

            # Create and fit weighted model for this policy
            # Pass calibrator for proper index transformation with two-stage calibration
            model = WeightedIsotonicOutcomeModel(
                n_folds=self.n_folds, calibrator=self.calibrator
            )
            model.set_weights(omega)

            # Use cv_map if available (from calibration), otherwise create new folds
            if cv_map:
                # Reuse folds from calibration for consistency
                fold_ids = np.array(
                    [cv_map.get(pid, 0) for pid in prompt_ids], dtype=int
                )
                model.fit(
                    prompts, responses, rewards, judge_scores, fold_ids, prompt_ids
                )
            else:
                # Create fold assignments if not provided
                from sklearn.model_selection import KFold

                kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)
                fold_ids = np.zeros(len(prompts), dtype=int)
                for fold_idx, (_, test_idx) in enumerate(kf.split(prompts)):
                    fold_ids[test_idx] = fold_idx
                model.fit(
                    prompts, responses, rewards, judge_scores, fold_ids, prompt_ids
                )

            self._policy_models[policy] = model
            logger.debug(
                f"Fitted weighted outcome model for policy '{policy}' with omega_mode='{self.omega_mode}'"
            )

        # Store prompt_id to fold mapping if available
        if cv_map:
            # Use the cv_map we built from dataset metadata
            self._promptid_to_fold = cv_map
        elif self._policy_models:
            # Create from first policy model if available
            first_model = next(iter(self._policy_models.values()))
            if hasattr(first_model, "_promptid_to_fold"):
                self._promptid_to_fold = first_model._promptid_to_fold
            else:
                self._promptid_to_fold = {}
        else:
            self._promptid_to_fold = {}

        logger.info(
            f"MRDR fitted with {len(self._policy_models)} policy-specific models"
        )

    def estimate(self) -> EstimationResult:
        """Compute MRDR estimates using policy-specific weighted outcome models.

        Each policy uses its own outcome model trained with omega weights
        derived from that policy's importance weights.
        """
        if not self.use_policy_specific_models:
            # Use base implementation
            result = super().estimate()
            result.method = "mrdr"
            if result.metadata is None:
                result.metadata = {}
            result.metadata.update(
                {
                    "omega_mode": self.omega_mode,
                    "min_sample_weight": self.min_sample_weight,
                    "note": "Simplified MRDR using shared outcome model",
                }
            )
            return result

        self._validate_fitted()

        # Auto-load fresh draws if not already loaded
        self._auto_load_fresh_draws()

        estimates: List[float] = []
        standard_errors: List[float] = []
        n_samples_used: Dict[str, int] = {}

        # Store components for diagnostics
        self._dm_component: Dict[str, np.ndarray] = {}
        self._ips_correction: Dict[str, np.ndarray] = {}
        self._outcome_predictions: Dict[str, np.ndarray] = {}

        for policy in self.sampler.target_policies:
            # Check if we have a model for this policy
            if policy not in self._policy_models:
                logger.warning(f"No outcome model for policy '{policy}'. Using NaN.")
                estimates.append(np.nan)
                standard_errors.append(np.nan)
                n_samples_used[policy] = 0
                continue

            # Get fresh draws (required for DR)
            if policy not in self._fresh_draws:
                logger.warning(
                    f"No fresh draws for policy '{policy}'. Skipping DR estimation."
                )
                estimates.append(np.nan)
                standard_errors.append(np.nan)
                n_samples_used[policy] = 0
                continue

            fresh_dataset = self._fresh_draws[policy]

            # Get data and weights
            data = self.sampler.get_data_for_policy(policy)
            if not data:
                estimates.append(np.nan)
                standard_errors.append(np.nan)
                n_samples_used[policy] = 0
                continue

            weights = self.get_weights(policy)
            if weights is None or len(weights) != len(data):
                raise ValueError(f"Weight/data mismatch for policy '{policy}'")

            # Extract arrays
            rewards = np.array([d["reward"] for d in data], dtype=float)
            judge_scores = np.array([d.get("judge_score") for d in data], dtype=float)
            prompt_ids = [str(d.get("prompt_id")) for d in data]
            prompts = [d["prompt"] for d in data]
            responses = [d["response"] for d in data]

            # Get fold assignments
            if self._promptid_to_fold:
                fold_ids = np.array(
                    [self._promptid_to_fold.get(pid, 0) for pid in prompt_ids],
                    dtype=int,
                )
            else:
                fold_ids = np.zeros(len(prompt_ids), dtype=int)

            # Get policy-specific outcome model
            outcome_model = self._policy_models[policy]

            # Get predictions on logged data
            g_logged = outcome_model.predict(prompts, responses, judge_scores, fold_ids)

            # Get predictions on fresh draws
            g_fresh_all = []
            for i, prompt_id in enumerate(prompt_ids):
                fresh_scores = fresh_dataset.get_scores_for_prompt_id(prompt_id)
                fresh_prompts = [prompts[i]] * len(fresh_scores)
                fresh_responses = [""] * len(fresh_scores)
                fresh_fold_ids = np.full(len(fresh_scores), fold_ids[i])

                g_fresh_prompt = outcome_model.predict(
                    fresh_prompts, fresh_responses, fresh_scores, fresh_fold_ids
                )
                g_fresh_all.append(g_fresh_prompt.mean())

            g_fresh = np.array(g_fresh_all)

            # Compute DR estimate with oracle augmentation
            dm_term = float(g_fresh.mean())
            ips_corr_base = weights * (rewards - g_logged)

            # Fit m̂(S) = E[W|S] for oracle augmentation if not already fitted
            if policy not in self.oracle_augmentation._m_hat_cache:
                # Get fold IDs for cross-fitting consistency using the stored mapping
                if hasattr(self, "_promptid_to_fold") and self._promptid_to_fold:
                    fold_ids_for_mhat = np.array(
                        [self._promptid_to_fold.get(pid, 0) for pid in prompt_ids]
                    )
                else:
                    fold_ids_for_mhat = fold_ids  # Use the existing fold_ids
                self.oracle_augmentation.fit_m_hat(
                    weights, judge_scores, policy, cv_folds=fold_ids_for_mhat
                )

            # Add oracle slice augmentation for honest CIs
            aug_vector, aug_diagnostics = self.oracle_augmentation.compute_augmentation(
                policy,
                rewards,  # calibrated rewards
                data,
                self.sampler.dataset.samples,
            )
            self._aug_diagnostics[policy] = aug_diagnostics

            # Total IPS correction with augmentation
            ips_corr_total = ips_corr_base + aug_vector
            ips_corr = float(np.mean(ips_corr_total))
            psi = dm_term + ips_corr

            # Store components for diagnostics
            self._dm_component[policy] = g_fresh
            self._ips_correction[policy] = ips_corr_total  # Include augmentation
            self._fresh_rewards[policy] = (
                rewards  # Store logged rewards for diagnostics
            )
            self._outcome_predictions[policy] = g_logged

            # Compute influence functions and standard error (including augmentation)
            if_contrib = g_fresh + ips_corr_total - psi

            # Apply IIC if enabled
            if self.use_iic:
                if_contrib = self._apply_iic(if_contrib, policy)

            se = (
                float(np.std(if_contrib, ddof=1) / np.sqrt(len(if_contrib)))
                if len(if_contrib) > 1
                else 0.0
            )

            # Store influence functions (always needed for proper inference)
            self._influence_functions[policy] = if_contrib

            estimates.append(psi)
            standard_errors.append(se)
            n_samples_used[policy] = len(rewards)

            logger.info(
                f"MRDR[{policy}]: {psi:.4f} ± {se:.4f} (DM={dm_term:.4f}, IPS_corr={ips_corr:.4f})"
            )

        # Build DR diagnostics using stored components
        from ..diagnostics.dr import compute_dr_policy_diagnostics

        dr_diagnostics_per_policy: Dict[str, Dict[str, Any]] = {}
        for idx, policy in enumerate(self.sampler.target_policies):
            if policy not in self._dm_component or np.isnan(estimates[idx]):
                continue

            # Use base class helper for consistent diagnostic computation
            dr_diagnostics_per_policy[policy] = self._compute_policy_diagnostics(
                policy, estimates[idx]
            )

        # Build diagnostics
        diagnostics = self._build_dr_diagnostics(
            estimates=estimates,
            standard_errors=standard_errors,
            n_samples_used=n_samples_used,
            dr_diagnostics_per_policy=dr_diagnostics_per_policy,
            ips_diagnostics=(
                self.ips_estimator.get_diagnostics()
                if hasattr(self.ips_estimator, "get_diagnostics")
                else None
            ),
        )

        # Create result with MRDR metadata (no influence functions here - they're first-class)
        metadata = {
            "omega_mode": self.omega_mode,
            "min_sample_weight": self.min_sample_weight,
            "use_policy_specific_models": self.use_policy_specific_models,
            "n_policy_models": len(self._policy_models),
            "cross_fitted": True,
            "n_folds": self.n_folds,
        }

        # Add IIC diagnostics if available
        if self.use_iic and self._iic_diagnostics:
            metadata["iic_diagnostics"] = self._iic_diagnostics

        return EstimationResult(
            estimates=np.array(estimates, dtype=float),
            standard_errors=np.array(standard_errors, dtype=float),
            n_samples_used=n_samples_used,
            method="mrdr",
            influence_functions=self._influence_functions,
            diagnostics=diagnostics,
            metadata=metadata,
        )


=== ./cje/estimators/outcome_models.py ===

"""Outcome models for Doubly Robust estimation.

Outcome models predict E[R|X,A,S] and are used in the direct method
component of DR estimators. They must be cross-fitted to maintain orthogonality.
"""

from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Any, TYPE_CHECKING
import numpy as np
import logging

if TYPE_CHECKING:
    from ..calibration.judge import JudgeCalibrator

logger = logging.getLogger(__name__)


class BaseOutcomeModel(ABC):
    """Abstract base class for cross-fitted outcome models in DR estimation.

    All outcome models must support cross-fitted prediction where each
    sample is predicted using a model trained on other folds.
    Subclasses only need to implement the single-model training and prediction.
    """

    def __init__(self, n_folds: int = 5):
        """Initialize the outcome model.

        Args:
            n_folds: Number of folds for cross-fitting
        """
        if n_folds < 2:
            raise ValueError(f"n_folds must be at least 2, got {n_folds}")

        self.n_folds = n_folds
        self.fold_models: Dict[int, Any] = {}
        self.fold_assignments: Optional[np.ndarray] = None
        self._fitted = False

    @abstractmethod
    def _fit_single_model(
        self,
        prompts: List[str],
        responses: List[str],
        rewards: np.ndarray,
        judge_scores: np.ndarray,
    ) -> Any:
        """Fit a single model on training data.

        Args:
            prompts: Training prompts
            responses: Training responses
            rewards: Training rewards (calibrated)
            judge_scores: Training judge scores

        Returns:
            A fitted model object
        """
        pass

    @abstractmethod
    def _predict_single_model(
        self,
        model: Any,
        prompts: List[str],
        responses: List[str],
        judge_scores: np.ndarray,
    ) -> np.ndarray:
        """Make predictions using a fitted model.

        Args:
            model: A model returned by _fit_single_model
            prompts: Prompts to predict on
            responses: Responses to predict on
            judge_scores: Judge scores to predict on

        Returns:
            Predicted rewards
        """
        pass

    def fit(
        self,
        prompts: List[str],
        responses: List[str],
        rewards: np.ndarray,
        judge_scores: Optional[np.ndarray] = None,
        fold_ids: Optional[np.ndarray] = None,
    ) -> None:
        """Fit cross-fitted models on logged data."""
        if fold_ids is None:
            raise ValueError("fold_ids is required for cross-fitted outcome models")

        if judge_scores is None:
            raise ValueError("judge_scores is required for outcome models")

        # Validate inputs
        n = len(prompts)
        if (
            len(responses) != n
            or len(rewards) != n
            or len(judge_scores) != n
            or len(fold_ids) != n
        ):
            raise ValueError(
                f"Input length mismatch: prompts={len(prompts)}, responses={len(responses)}, "
                f"rewards={len(rewards)}, judge_scores={len(judge_scores)}, fold_ids={len(fold_ids)}"
            )

        # Remap fold IDs to be sequential 0..K-1 for the subset
        original_fold_ids = fold_ids.astype(int)
        unique_folds = sorted(np.unique(original_fold_ids))

        # Create mapping from original to sequential
        fold_id_map = {old_id: new_id for new_id, old_id in enumerate(unique_folds)}
        self.fold_assignments = np.vectorize(fold_id_map.__getitem__)(original_fold_ids)

        # Store reverse mapping for potential debugging
        self._fold_id_map = fold_id_map
        self._reverse_fold_map = {v: k for k, v in fold_id_map.items()}

        # Adjust n_folds to actual number of unique folds
        if len(unique_folds) != self.n_folds:
            logger.info(
                f"Adjusting n_folds from {self.n_folds} to {len(unique_folds)} based on data"
            )
            self.n_folds = len(unique_folds)

        # Train a model for each fold on the other folds (using remapped IDs)
        for fold in range(self.n_folds):
            train_mask = self.fold_assignments != fold

            if not np.any(train_mask):
                raise ValueError(f"No training data for fold {fold}")

            # Cast to numpy array for type safety
            train_mask_arr = np.asarray(train_mask)
            train_prompts = [p for i, p in enumerate(prompts) if train_mask_arr[i]]
            train_responses = [r for i, r in enumerate(responses) if train_mask_arr[i]]
            train_rewards = rewards[train_mask]
            train_scores = judge_scores[train_mask]

            # Allow subclasses to provide fold-specific kwargs (e.g., sample weights)
            extra_kwargs = {}
            if hasattr(self, "_get_fold_fit_kwargs"):
                extra_kwargs = self._get_fold_fit_kwargs(train_mask_arr)

            model = self._fit_single_model(
                train_prompts,
                train_responses,
                train_rewards,
                train_scores,
                **extra_kwargs,
            )
            self.fold_models[fold] = model

            logger.debug(
                f"Fitted model for fold {fold} on {len(train_prompts)} samples"
            )

        self._fitted = True
        logger.info(
            f"{self.__class__.__name__} fitted with {self.n_folds} cross-fitted models"
        )

    def predict(
        self,
        prompts: List[str],
        responses: List[str],
        judge_scores: Optional[np.ndarray] = None,
        fold_ids: Optional[np.ndarray] = None,
    ) -> np.ndarray:
        """Predict using cross-fitted models."""
        if not self._fitted:
            raise RuntimeError("Model must be fitted before prediction")

        if judge_scores is None:
            raise ValueError("judge_scores required for prediction")

        # Use provided fold_ids or fall back to stored ones
        if fold_ids is None:
            if self.fold_assignments is None:
                raise ValueError("fold_ids must be provided or set during fit()")
            if len(prompts) != len(self.fold_assignments):
                raise ValueError(
                    f"Using stored fold_assignments but length mismatch: "
                    f"prompts={len(prompts)}, fold_assignments={len(self.fold_assignments)}"
                )
            fold_ids = self.fold_assignments

        # Validate inputs
        n = len(prompts)
        if len(responses) != n or len(judge_scores) != n or len(fold_ids) != n:
            raise ValueError(
                f"Input length mismatch: prompts={len(prompts)}, responses={len(responses)}, "
                f"judge_scores={len(judge_scores)}, fold_ids={len(fold_ids)}"
            )

        fold_ids = fold_ids.astype(int)

        # Remap fold IDs if we have a mapping (from fit())
        if hasattr(self, "_fold_id_map") and self._fold_id_map:
            # Map incoming fold IDs to compact range
            mapped_fold_ids = np.array(
                [self._fold_id_map.get(fid, None) for fid in fold_ids]
            )

            # Check for unmapped fold IDs
            if None in mapped_fold_ids:
                unmapped = set(fid for fid in fold_ids if fid not in self._fold_id_map)
                raise ValueError(
                    f"Unmapped fold IDs: {sorted(unmapped)}. "
                    f"Known mappings: {self._fold_id_map}"
                )

            fold_ids = mapped_fold_ids.astype(int)

        # Guard against unknown fold IDs
        unknown_folds = set(np.unique(fold_ids)) - set(self.fold_models.keys())
        if unknown_folds:
            raise ValueError(
                f"Unknown fold ids in predict(): {sorted(unknown_folds)}. "
                f"Available folds: {sorted(self.fold_models.keys())}"
            )

        predictions = np.zeros(n)

        # Predict each fold using its out-of-fold model
        for fold in self.fold_models:
            fold_mask = fold_ids == fold
            if not fold_mask.any():
                continue

            fold_prompts = [p for i, p in enumerate(prompts) if fold_mask[i]]
            fold_responses = [r for i, r in enumerate(responses) if fold_mask[i]]
            fold_scores = judge_scores[fold_mask]

            fold_predictions = self._predict_single_model(
                self.fold_models[fold],
                fold_prompts,
                fold_responses,
                fold_scores,
            )

            # Validate prediction shape
            if len(fold_predictions) != fold_mask.sum():
                raise ValueError(
                    f"Model returned {len(fold_predictions)} predictions but expected {fold_mask.sum()}"
                )

            predictions[fold_mask] = fold_predictions

        return predictions


class IsotonicOutcomeModel(BaseOutcomeModel):
    """Cross-fitted isotonic outcome model for DR estimation.

    This model uses g(x,a,s) = f^(-k)(z) where z is the calibrator's index
    (either raw judge scores for monotone calibration or transformed index
    for two-stage calibration), and f^(-k) is the isotonic regression learned
    with the k-th fold held out for cross-fitting.

    The isotonic models are trained fresh during the DR fit process,
    ensuring proper cross-fitting for orthogonality.
    """

    def __init__(self, n_folds: int = 5, calibrator: Optional[Any] = None):
        """Initialize isotonic outcome model.

        Args:
            n_folds: Number of cross-fitting folds (default 5)
            calibrator: Optional calibrator with index() method for transforming scores
        """
        super().__init__(n_folds)
        self.calibrator = calibrator

    def fit(
        self,
        prompts: List[str],
        responses: List[str],
        rewards: np.ndarray,
        judge_scores: Optional[np.ndarray] = None,
        fold_ids: Optional[np.ndarray] = None,
    ) -> None:
        """Fit cross-fitted models with proper index transformation."""
        if fold_ids is None:
            raise ValueError("fold_ids is required for cross-fitted outcome models")

        if judge_scores is None:
            raise ValueError("judge_scores is required for outcome models")

        # Pre-compute transformed indices if calibrator is available
        if self.calibrator is not None and hasattr(self.calibrator, "index"):
            # Get OOF indices for all data at once
            transformed_scores = self.calibrator.index(judge_scores, fold_ids)
        else:
            transformed_scores = judge_scores

        # Store original judge_scores for later use, replace with transformed
        self._original_judge_scores = judge_scores
        judge_scores_to_use = transformed_scores

        # Call parent fit with transformed scores
        super().fit(prompts, responses, rewards, judge_scores_to_use, fold_ids)

    def _fit_single_model(
        self,
        prompts: List[str],
        responses: List[str],
        rewards: np.ndarray,
        judge_scores: np.ndarray,  # These are already transformed indices
    ) -> Any:
        """Fit an isotonic regression model on training data."""
        from sklearn.isotonic import IsotonicRegression

        # judge_scores are already transformed by fit() method
        model = IsotonicRegression(out_of_bounds="clip")
        model.fit(judge_scores, rewards)
        return model

    def predict(
        self,
        prompts: List[str],
        responses: List[str],
        judge_scores: Optional[np.ndarray] = None,
        fold_ids: Optional[np.ndarray] = None,
    ) -> np.ndarray:
        """Predict using cross-fitted models with proper index transformation."""
        if judge_scores is None:
            raise ValueError("judge_scores required for prediction")

        # Transform judge scores if calibrator is available
        if self.calibrator is not None and hasattr(self.calibrator, "index"):
            # For prediction, use the ensemble index (folds=None)
            transformed_scores = self.calibrator.index(judge_scores, folds=None)
        else:
            transformed_scores = judge_scores

        # Call parent predict with transformed scores
        return super().predict(prompts, responses, transformed_scores, fold_ids)

    def _predict_single_model(
        self,
        model: Any,
        prompts: List[str],
        responses: List[str],
        judge_scores: np.ndarray,  # These are already transformed indices
    ) -> np.ndarray:
        """Predict using the fitted isotonic model."""
        # judge_scores are already transformed by predict() method
        predictions: np.ndarray = model.predict(judge_scores)
        return predictions


class LinearOutcomeModel(BaseOutcomeModel):
    """Example custom outcome model using linear regression.

    This demonstrates how users can implement their own outcome models
    by extending BaseOutcomeModel.
    """

    def __init__(self, n_folds: int = 5, alpha: float = 1.0):
        """Initialize linear outcome model.

        Args:
            n_folds: Number of cross-fitting folds
            alpha: Regularization strength for Ridge regression
        """
        super().__init__(n_folds)
        self.alpha = alpha

    def _fit_single_model(
        self,
        prompts: List[str],
        responses: List[str],
        rewards: np.ndarray,
        judge_scores: np.ndarray,
    ) -> Any:
        """Fit a Ridge regression model on features."""
        from sklearn.linear_model import Ridge

        features = self._extract_features(prompts, responses, judge_scores)
        # Use fit_intercept=False since we add bias column manually
        model = Ridge(alpha=self.alpha, fit_intercept=False)
        model.fit(features, rewards)
        return model

    def _predict_single_model(
        self,
        model: Any,
        prompts: List[str],
        responses: List[str],
        judge_scores: np.ndarray,
    ) -> np.ndarray:
        """Predict using the fitted Ridge model."""
        features = self._extract_features(prompts, responses, judge_scores)
        predictions = model.predict(features)
        clipped: np.ndarray = np.clip(predictions, 0, 1)
        return clipped

    def _extract_features(
        self,
        prompts: List[str],
        responses: List[str],
        judge_scores: np.ndarray,
    ) -> np.ndarray:
        """Extract simple features from inputs."""
        # Length features
        prompt_lengths = np.array([len(p.split()) for p in prompts]).reshape(-1, 1)
        response_lengths = np.array([len(r.split()) for r in responses]).reshape(-1, 1)

        # Judge scores
        scores = judge_scores.reshape(-1, 1)

        # Combine features
        feature_matrix = np.hstack([prompt_lengths, response_lengths, scores])

        # Add bias term
        bias = np.ones((len(prompts), 1))
        final_features: np.ndarray = np.hstack([feature_matrix, bias])

        return final_features


class CalibratorBackedOutcomeModel(BaseOutcomeModel):
    """Outcome model that reuses cross-fitted reward calibrators.

    Instead of refitting a model on (S, R=f_all(S)), this model reuses
    the cross-fitted calibrators f^(-k) that were already trained during
    reward calibration. This preserves orthogonality and avoids redundant
    computation.

    This is the recommended default for DR estimation when using isotonic
    calibration for rewards.
    """

    def __init__(self, reward_calibrator: "JudgeCalibrator", n_folds: int = 5):
        """Initialize with a fitted reward calibrator.

        Args:
            reward_calibrator: A fitted JudgeCalibrator with cross-fitted models
            n_folds: Number of folds (should match calibrator's n_folds)
        """
        super().__init__(n_folds)
        self.calibrator = reward_calibrator

        # Verify calibrator has cross-fitted models
        if (
            not hasattr(reward_calibrator, "_fold_models")
            or not reward_calibrator._fold_models
        ):
            raise ValueError(
                "CalibratorBackedOutcomeModel requires a calibrator fitted with "
                "fit_cv(). Use enable_cross_fit=True in calibrate_dataset()."
            )

        if reward_calibrator._n_folds != n_folds:
            logger.warning(
                f"Calibrator has {reward_calibrator._n_folds} folds but outcome model "
                f"requested {n_folds}. Using calibrator's fold count."
            )
            self.n_folds = reward_calibrator._n_folds

    def _fit_single_model(
        self,
        prompts: List[str],
        responses: List[str],
        rewards: np.ndarray,
        judge_scores: np.ndarray,
    ) -> Any:
        """No training needed - reuse calibrator's models."""
        # Just return a reference to the calibrator
        return self.calibrator

    def _predict_single_model(
        self,
        model: Any,
        prompts: List[str],
        responses: List[str],
        judge_scores: np.ndarray,
    ) -> np.ndarray:
        """Predict using the calibrator's cross-fitted models.

        This should never be called directly since we override fit() and predict()
        to use the calibrator's predict_oof() method directly.
        """
        # This is a fallback that shouldn't be reached
        return judge_scores

    def fit(
        self,
        prompts: List[str],
        responses: List[str],
        rewards: np.ndarray,
        judge_scores: Optional[np.ndarray] = None,
        fold_ids: Optional[np.ndarray] = None,
    ) -> None:
        """Fit by storing fold assignments (no model training needed).

        Args:
            prompts: Training prompts
            responses: Training responses
            rewards: Training rewards (not used)
            judge_scores: Training judge scores
            fold_ids: Pre-assigned fold IDs from calibration
        """
        n_samples = len(prompts)

        if fold_ids is not None:
            # Use provided fold assignments
            self.fold_assignments = np.asarray(fold_ids)
        else:
            # Try to get from calibrator
            if (
                hasattr(self.calibrator, "_fold_ids")
                and self.calibrator._fold_ids is not None
            ):
                if len(self.calibrator._fold_ids) == n_samples:
                    self.fold_assignments = self.calibrator._fold_ids
                else:
                    # Strict error to avoid accidental in-fold leakage
                    raise ValueError(
                        f"CalibratorBackedOutcomeModel requires exact fold_ids when "
                        f"calibrator's stored fold_ids don't match the data subset. "
                        f"Calibrator has {len(self.calibrator._fold_ids)} fold IDs but "
                        f"we have {n_samples} samples. Pass explicit fold_ids from the "
                        f"'cv_fold' metadata to avoid accidental in-fold predictions."
                    )
            else:
                # No fold IDs available - require explicit ones
                raise ValueError(
                    "CalibratorBackedOutcomeModel requires fold_ids to be provided. "
                    "Either pass them explicitly or ensure the calibrator was fitted "
                    "with fit_cv() and has matching data size."
                )

        self._fitted = True
        logger.info(
            f"CalibratorBackedOutcomeModel ready: {n_samples} samples, "
            f"{self.n_folds} folds (reusing calibrator models)"
        )

    def predict(
        self,
        prompts: List[str],
        responses: List[str],
        judge_scores: Optional[np.ndarray] = None,
        fold_ids: Optional[np.ndarray] = None,
    ) -> np.ndarray:
        """Predict using cross-fitted calibration models.

        Args:
            prompts: Prompts to predict on
            responses: Responses to predict on
            judge_scores: Judge scores to calibrate
            fold_ids: Fold assignments for each sample

        Returns:
            Cross-fitted predictions using f^(-k)
        """
        if not self._fitted:
            raise RuntimeError("Must call fit() before predict()")

        if fold_ids is None:
            # Use stored fold assignments if they match
            if self.fold_assignments is not None and len(self.fold_assignments) == len(
                prompts
            ):
                fold_ids = self.fold_assignments
            else:
                # For new data, require explicit fold assignments
                raise ValueError(
                    "fold_ids required for CalibratorBackedOutcomeModel.predict() "
                    "when predicting on new data to avoid accidental in-fold predictions. "
                    "Provide fold assignments from the calibration phase."
                )

        if judge_scores is None:
            raise ValueError("judge_scores required for prediction")

        # Use calibrator's out-of-fold predictions
        predictions = self.calibrator.predict_oof(judge_scores, fold_ids)

        return predictions


=== ./cje/estimators/stacking.py ===

"""Estimator stacking via influence function variance minimization.

This module implements stacking of DR-family estimators (DR-CPO, TMLE, MRDR)
by forming an optimal convex combination that minimizes the variance of the
combined influence function.
"""

import numpy as np
import logging
from typing import List, Dict, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from cje.estimators.base_estimator import BaseCJEEstimator
from cje.data.models import EstimationResult
from cje.data.precomputed_sampler import PrecomputedSampler

logger = logging.getLogger(__name__)


class StackedDREstimator(BaseCJEEstimator):
    """
    Stacks DR estimators via influence function variance minimization.

    This implements the estimator stacking approach from the CJE paper,
    forming an optimal convex combination of DR-CPO, TMLE, and MRDR
    by minimizing the empirical variance of the combined influence function.

    Key features:
    - Runs multiple DR estimators with shared resources (folds, fresh draws)
    - Computes optimal weights by minimizing IF variance
    - Uses outer split for honest inference (default)
    - Robust covariance estimation via Ledoit-Wolf shrinkage
    - Parallel execution of component estimators

    Example:
        >>> sampler = PrecomputedSampler.from_jsonl("data.jsonl")
        >>> stacked = StackedDREstimator(sampler)
        >>> result = stacked.fit_and_estimate()
        >>> print(f"Estimate: {result.estimates[0]:.3f} ± {result.standard_errors[0]:.3f}")
    """

    def __init__(
        self,
        sampler: PrecomputedSampler,
        estimators: Optional[List[str]] = None,
        use_outer_split: bool = True,
        V_folds: int = 5,
        robust_cov: bool = True,
        shrinkage_intensity: float = 0.1,
        parallel: bool = True,
        min_weight: float = 0.0,
        fallback_on_failure: bool = True,
        seed: int = 42,
        **kwargs: Any,
    ):
        """Initialize the stacked estimator.

        Args:
            sampler: PrecomputedSampler with calibrated data
            estimators: List of estimator names to stack.
                Default: ["dr-cpo", "tmle", "mrdr"]
            use_outer_split: If True, use V-fold outer split for honest inference
            V_folds: Number of outer folds for honest stacking
            robust_cov: If True, use Ledoit-Wolf shrinkage for covariance
            shrinkage_intensity: Shrinkage parameter for Ledoit-Wolf (0 to 1)
            parallel: If True, run component estimators in parallel
            min_weight: Minimum weight for any estimator (for stability)
            fallback_on_failure: If True, fall back to best single estimator on failure
            seed: Random seed for reproducibility
            **kwargs: Additional arguments passed to base class
        """
        # Extract calibrator before passing to base class (base doesn't accept it)
        self.calibrator = kwargs.pop("calibrator", None)
        # Extract oracle_slice_config to pass to base class
        oracle_slice_config = kwargs.pop("oracle_slice_config", True)

        # BaseCJEEstimator only accepts specific params, not arbitrary kwargs
        # So we don't pass **kwargs
        super().__init__(sampler, oracle_slice_config=oracle_slice_config)

        # Configuration
        self.estimators = estimators or ["dr-cpo", "tmle", "mrdr"]
        self.use_outer_split = use_outer_split
        self.V_folds = V_folds
        self.robust_cov = robust_cov
        self.shrinkage_intensity = shrinkage_intensity
        self.parallel = parallel
        self.min_weight = min_weight
        self.fallback_on_failure = fallback_on_failure
        self.seed = seed
        self.oracle_slice_config = oracle_slice_config  # Store the extracted value

        # Storage for results
        self.component_results: Dict[str, EstimationResult] = {}
        self.component_estimators: Dict[str, Any] = (
            {}
        )  # Store estimators for weights/diagnostics
        self.weights_per_policy: Dict[str, np.ndarray] = {}
        self.stacking_diagnostics: Dict[str, Any] = {}
        self._fresh_draws: Dict[str, Any] = (
            {}
        )  # Store fresh draws to pass to components

        # Set up shared resources
        self._setup_shared_resources()

    def _setup_shared_resources(self) -> None:
        """Set up resources shared across all component estimators."""
        np.random.seed(self.seed)

        # Generate fold assignments once using unified system (all estimators use same folds)
        from ..data.folds import get_folds_for_dataset

        # Use the unified fold system if we have a real dataset
        if hasattr(self.sampler, "dataset"):
            self.shared_fold_ids = get_folds_for_dataset(
                self.sampler.dataset,
                n_folds=5,  # Could make this configurable
                seed=self.seed,
            )
        else:
            # Fallback for mock objects in tests
            try:
                n = len(self.sampler)
            except TypeError:
                n = (
                    self.sampler.n_valid_samples
                    if hasattr(self.sampler, "n_valid_samples")
                    else 100
                )
            # Simple assignment for mocks
            self.shared_fold_ids = np.arange(n) % 5

        # Note: Fresh draws are already in the sampler if available
        # Each estimator will detect and use them automatically

        logger.info(f"Set up shared resources for {len(self.estimators)} estimators")

    def add_fresh_draws(self, policy: str, fresh_draws: Any) -> None:
        """Store fresh draws to pass to component estimators.

        Args:
            policy: Target policy name
            fresh_draws: Fresh draw dataset for this policy
        """
        self._fresh_draws[policy] = fresh_draws
        logger.debug(f"Added fresh draws for policy {policy}")

    def fit(self) -> None:
        """Fit is a no-op for stacking (component estimators handle their own fitting)."""
        self._fitted = True

    def estimate(self) -> EstimationResult:
        """Run all component estimators and stack them optimally."""
        if not self._fitted:
            self.fit()

        # Step 1: Run all component estimators
        self._run_all_estimators()

        # Check for failures
        valid_estimators = self._get_valid_estimators()

        if len(valid_estimators) == 0:
            raise RuntimeError("All component estimators failed")

        if len(valid_estimators) == 1:
            # Only one valid estimator, pass through its results
            logger.warning(
                f"Only one valid estimator ({valid_estimators[0]}), using it directly"
            )
            return self._create_passthrough_result(valid_estimators[0])

        # Step 2: Stack estimates for each policy
        stacked_estimates = []
        stacked_ses = []
        stacked_ifs = {}

        for policy_idx, policy in enumerate(self.sampler.target_policies):
            # Collect influence functions from valid estimators
            IF_matrix = self._collect_influence_functions(policy, valid_estimators)

            if IF_matrix is None or IF_matrix.shape[1] == 0:
                # No valid IFs for this policy
                logger.warning(f"No valid influence functions for policy {policy}")
                stacked_estimates.append(np.nan)
                stacked_ses.append(np.nan)
                continue

            # Compute stacked influence function
            if self.use_outer_split:
                stacked_if, weights = self._stack_with_outer_split(IF_matrix, policy)
            else:
                weights = self._compute_optimal_weights(IF_matrix)
                stacked_if = IF_matrix @ weights

            self.weights_per_policy[policy] = weights

            # Apply IIC for variance reduction (if enabled)
            stacked_if = self._apply_iic(stacked_if, policy)

            # Store influence function
            stacked_ifs[policy] = stacked_if
            self._influence_functions[policy] = stacked_if

            # Compute stacked estimate as weighted average of component estimates
            component_estimates = []
            for est_name in valid_estimators:
                result = self.component_results[est_name]
                if result:
                    component_estimates.append(result.estimates[policy_idx])

            if component_estimates:
                estimate = np.dot(weights, component_estimates)
            else:
                estimate = np.nan

            # Compute SE from the stacked influence function
            se = np.std(stacked_if, ddof=1) / np.sqrt(len(stacked_if))

            stacked_estimates.append(estimate)
            stacked_ses.append(se)

        # Step 3: Build diagnostics
        diagnostics = self._build_stacking_diagnostics(valid_estimators)

        # Step 4: Create and return result
        metadata = {
            "stacking_weights": self.weights_per_policy,
            "valid_estimators": valid_estimators,
            "failed_estimators": [
                e for e in self.estimators if e not in valid_estimators
            ],
            "used_outer_split": self.use_outer_split,
            "V_folds": self.V_folds if self.use_outer_split else None,
            "stacking_diagnostics": diagnostics,  # Add the detailed diagnostics to metadata
            "iic_diagnostics": self._iic_diagnostics if self.use_iic else None,
        }

        # Get n_samples_used from one of the component estimators
        n_samples_used = {}
        if valid_estimators and self.component_results[valid_estimators[0]]:
            first_result = self.component_results[valid_estimators[0]]
            n_samples_used = first_result.n_samples_used
        else:
            # Fallback: use sampler info
            for policy in self.sampler.target_policies:
                n_samples_used[policy] = len(self.sampler)

        result = EstimationResult(
            estimates=np.array(stacked_estimates),
            standard_errors=np.array(stacked_ses),
            n_samples_used=n_samples_used,
            method=f"StackedDR({', '.join(valid_estimators)})",
            influence_functions=stacked_ifs,
            diagnostics=None,  # Use None for now to avoid validation issues
            metadata=metadata,
        )

        self._results = result
        return result

    def fit_and_estimate(self) -> EstimationResult:
        """Convenience method to fit and estimate in one call."""
        self.fit()
        return self.estimate()

    def _run_all_estimators(self) -> None:
        """Run all component estimators, either in parallel or sequentially."""
        logger.info(f"Running {len(self.estimators)} component estimators")

        if self.parallel:
            with ThreadPoolExecutor(max_workers=len(self.estimators)) as executor:
                futures = {
                    executor.submit(self._run_single_estimator, name): name
                    for name in self.estimators
                }

                for future in as_completed(futures):
                    name = futures[future]
                    try:
                        result = future.result()
                        self.component_results[name] = result
                        logger.info(f"Completed {name}")
                    except Exception as e:
                        logger.warning(f"Estimator {name} failed: {e}")
                        self.component_results[name] = None
        else:
            for name in self.estimators:
                try:
                    result = self._run_single_estimator(name)
                    self.component_results[name] = result
                    logger.info(f"Completed {name}")
                except Exception as e:
                    logger.warning(f"Estimator {name} failed: {e}")
                    self.component_results[name] = None

    def _run_single_estimator(self, name: str) -> EstimationResult:
        """Run a single component estimator with shared resources."""
        # Import here to avoid circular imports
        from cje.estimators.dr_base import DRCPOEstimator
        from cje.estimators.tmle import TMLEEstimator
        from cje.estimators.mrdr import MRDREstimator

        estimator_classes = {
            "dr-cpo": DRCPOEstimator,
            "tmle": TMLEEstimator,
            "mrdr": MRDREstimator,
        }

        if name not in estimator_classes:
            raise ValueError(f"Unknown estimator: {name}")

        estimator_class = estimator_classes[name]

        # Create estimator with shared fold assignments and calibrator
        # Note: We can't directly pass fold_ids to most estimators,
        # but they will use the same seed which helps
        # Pass calibrator as a named parameter for DR estimators
        if name in ["dr-cpo", "tmle", "mrdr"]:
            # If we have a calibrator, components should use calibrated weights
            # If no calibrator, components should use raw weights
            estimator = estimator_class(
                self.sampler,
                calibrator=self.calibrator,
                use_calibrated_weights=(self.calibrator is not None),
                oracle_slice_config=self.oracle_slice_config,
            )
        else:
            # For non-DR estimators (shouldn't happen with default config)
            estimator = estimator_class(
                self.sampler, oracle_slice_config=self.oracle_slice_config
            )

        # Add fresh draws if available
        if self._fresh_draws:
            for policy, fresh_draws in self._fresh_draws.items():
                estimator.add_fresh_draws(policy, fresh_draws)
                logger.debug(f"Added fresh draws for {policy} to {name}")

        # Store the estimator for later access
        self.component_estimators[name] = estimator

        # Run estimation
        result: EstimationResult = estimator.fit_and_estimate()
        return result

    def _get_valid_estimators(self) -> List[str]:
        """Get list of estimators that ran successfully."""
        valid = []
        for name in self.estimators:
            result = self.component_results.get(name)
            if result is not None and not np.all(np.isnan(result.estimates)):
                valid.append(name)
        return valid

    def _collect_influence_functions(
        self, policy: str, valid_estimators: List[str]
    ) -> Optional[np.ndarray]:
        """Collect influence functions from valid estimators for a given policy.

        Returns:
            IF_matrix: n x K matrix where K is the number of valid estimators
        """
        ifs = []

        for name in valid_estimators:
            result = self.component_results[name]
            if result and result.influence_functions:
                if_for_policy = result.influence_functions.get(policy)
                if if_for_policy is not None:
                    ifs.append(if_for_policy)

        if not ifs:
            return None

        # Stack into matrix (each column is one estimator's IF)
        return np.column_stack(ifs)

    def _compute_optimal_weights(self, IF_matrix: np.ndarray) -> np.ndarray:
        """Compute optimal stacking weights by minimizing IF variance.

        Solves: min_α α^T Σ α  s.t.  α ≥ 0, Σα = 1

        Args:
            IF_matrix: n x K matrix of influence functions

        Returns:
            weights: K-dimensional weight vector
        """
        K = IF_matrix.shape[1]

        # Compute covariance matrix
        if self.robust_cov:
            Sigma = self._ledoit_wolf_covariance(IF_matrix)
        else:
            # Center the IFs first
            centered_IF = IF_matrix - IF_matrix.mean(axis=0, keepdims=True)
            Sigma = np.cov(centered_IF.T)

        # Add small ridge for numerical stability
        Sigma = Sigma + 1e-8 * np.eye(K)

        # Closed-form solution: w ∝ Σ^{-1} 1
        ones = np.ones(K)

        try:
            # Solve Σw = 1
            weights = np.linalg.solve(Sigma, ones)

            # Normalize to sum to 1
            weights = weights / weights.sum()

            # Project to simplex (ensure non-negative)
            weights = self._project_to_simplex(weights)

            # Apply minimum weight if specified
            if self.min_weight > 0:
                weights = np.maximum(weights, self.min_weight)
                weights = weights / weights.sum()

        except np.linalg.LinAlgError:
            logger.warning("Covariance matrix is singular, using equal weights")
            weights = ones / K

        return weights

    def _project_to_simplex(self, v: np.ndarray) -> np.ndarray:
        """Project vector v onto the probability simplex."""
        n = len(v)
        u = np.sort(v)[::-1]
        cssv = np.cumsum(u) - 1
        ind = np.arange(1, n + 1)
        cond = u - cssv / ind > 0
        rho = np.nonzero(cond)[0][-1] if np.any(cond) else n - 1
        theta = cssv[rho] / (rho + 1)
        w = np.maximum(v - theta, 0)
        return w / w.sum() if w.sum() > 0 else np.ones(n) / n

    def _ledoit_wolf_covariance(self, IF_matrix: np.ndarray) -> np.ndarray:
        """Compute shrinkage covariance estimator (Ledoit-Wolf).

        Shrinks the sample covariance toward a diagonal target.
        """
        # Center the data
        X = IF_matrix - IF_matrix.mean(axis=0, keepdims=True)
        n, p = X.shape

        # Sample covariance
        S = (X.T @ X) / n

        # Shrinkage target (diagonal with sample variances)
        target = np.diag(np.diag(S))

        # Use specified shrinkage intensity
        shrinkage = self.shrinkage_intensity

        # Shrink toward target
        return (1 - shrinkage) * S + shrinkage * target

    def _stack_with_outer_split(
        self, IF_matrix: np.ndarray, policy: str
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Stack with V-fold outer split for honest inference.

        Returns:
            stacked_if: The stacked influence function
            avg_weights: Average weights across folds (for reporting)
        """
        n = IF_matrix.shape[0]
        stacked_if = np.zeros(n)
        fold_weights = []

        # Create V folds
        indices = np.arange(n)
        np.random.shuffle(indices)
        folds = np.array_split(indices, self.V_folds)

        for v, test_idx in enumerate(folds):
            # Training indices are all except test fold
            train_idx = np.setdiff1d(indices, test_idx)

            if len(train_idx) < 2:
                continue

            # Learn weights on training folds
            train_IF = IF_matrix[train_idx]
            weights = self._compute_optimal_weights(train_IF)
            fold_weights.append(weights)

            # Apply to test fold
            stacked_if[test_idx] = IF_matrix[test_idx] @ weights

        # Average weights across folds for reporting
        avg_weights = (
            np.mean(fold_weights, axis=0)
            if fold_weights
            else np.ones(IF_matrix.shape[1]) / IF_matrix.shape[1]
        )

        return stacked_if, avg_weights

    def _create_passthrough_result(self, estimator_name: str) -> EstimationResult:
        """Create a result that passes through a single estimator's results."""
        result = self.component_results[estimator_name]

        # Add metadata about the passthrough
        metadata = result.metadata.copy() if result.metadata else {}
        metadata.update(
            {
                "stacking_fallback": True,
                "reason": "Only one valid estimator",
                "selected_estimator": estimator_name,
                "attempted_estimators": self.estimators,
            }
        )

        return EstimationResult(
            estimates=result.estimates,
            standard_errors=result.standard_errors,
            n_samples_used=result.n_samples_used,
            method=f"StackedDR(fallback->{estimator_name})",
            influence_functions=result.influence_functions,
            diagnostics=result.diagnostics,
            metadata=metadata,
        )

    def _build_stacking_diagnostics(
        self, valid_estimators: List[str]
    ) -> Dict[str, Any]:
        """Build comprehensive diagnostics for the stacking procedure."""
        diagnostics: Dict[str, Any] = {
            "estimator_type": "StackedDR",
            "valid_estimators": valid_estimators,
            "failed_estimators": [
                e for e in self.estimators if e not in valid_estimators
            ],
            "n_components": len(valid_estimators),
            "used_outer_split": self.use_outer_split,
        }

        # Add weight diagnostics per policy
        diagnostics["weights_per_policy"] = {}
        for policy, weights in self.weights_per_policy.items():
            diagnostics["weights_per_policy"][policy] = {
                name: float(w) for name, w in zip(valid_estimators, weights)
            }

        # Add variance reduction metrics
        if len(valid_estimators) > 1:
            diagnostics["variance_reduction"] = self._compute_variance_reduction(
                valid_estimators
            )

        # Add IF correlation matrix for first policy (as example)
        if self.sampler.target_policies:
            first_policy = self.sampler.target_policies[0]
            IF_matrix = self._collect_influence_functions(
                first_policy, valid_estimators
            )
            if IF_matrix is not None and IF_matrix.shape[1] > 1:
                # Compute correlation matrix
                centered_IF = IF_matrix - IF_matrix.mean(axis=0, keepdims=True)
                corr_matrix = np.corrcoef(centered_IF.T)
                diagnostics["if_correlation_matrix"] = corr_matrix.tolist()

        return diagnostics

    def _compute_variance_reduction(self, valid_estimators: List[str]) -> Dict:
        """Compute variance reduction compared to individual estimators."""
        reduction = {}

        for policy_idx, policy in enumerate(self.sampler.target_policies):
            if policy not in self.weights_per_policy:
                continue

            # Get stacked SE
            if self._results:
                stacked_se = self._results.standard_errors[policy_idx]

                # Compare to each component
                for name in valid_estimators:
                    result = self.component_results.get(name)
                    if result and not np.isnan(result.standard_errors[policy_idx]):
                        component_se = result.standard_errors[policy_idx]
                        pct_reduction = 100 * (1 - stacked_se**2 / component_se**2)
                        reduction[f"{policy}_vs_{name}"] = float(pct_reduction)

        return reduction

    def get_weights(self, policy: str) -> Optional[np.ndarray]:
        """Get importance weights for a given policy.

        For stacked estimator, returns the weights from the first successful component
        (all DR estimators use the same IPS weights).

        Args:
            policy: Target policy name

        Returns:
            Importance weights or None if not available
        """
        # Try to get weights from first successful component estimator
        # (all DR estimators use the same IPS weights)
        for name in ["dr-cpo", "tmle", "mrdr"]:
            if name in self.component_estimators:
                estimator = self.component_estimators[name]
                if estimator:
                    # DR estimators store weights in their internal ips_estimator
                    if hasattr(estimator, "ips_estimator") and hasattr(
                        estimator.ips_estimator, "get_weights"
                    ):
                        return estimator.ips_estimator.get_weights(policy)
                    elif hasattr(estimator, "get_weights"):
                        return estimator.get_weights(policy)

        return None

    def get_diagnostics(self) -> Optional[Any]:
        """Get diagnostics from the stacked estimator.

        Returns diagnostics from the first successful component.

        Returns:
            Diagnostics object or None
        """
        # Try to get diagnostics from the first successful DR component
        for name in ["dr-cpo", "tmle", "mrdr"]:
            if name in self.component_estimators:
                estimator = self.component_estimators[name]
                if estimator and hasattr(estimator, "get_diagnostics"):
                    diag = estimator.get_diagnostics()
                    if diag is not None:
                        return diag

        # If no component has diagnostics, return None
        return None


=== ./cje/estimators/tmle.py ===

# cje/core/tmle.py
"""
TMLE estimator for policy evaluation with cross-fitted monotone outcome models.

This estimator properly inherits from DREstimator, eliminating code duplication
and ensuring consistent diagnostics with other DR methods.
"""

from __future__ import annotations
from typing import Dict, Optional, Any, List, Tuple, Union
import logging
import numpy as np

from .dr_base import DREstimator
from .outcome_models import IsotonicOutcomeModel
from ..data.precomputed_sampler import PrecomputedSampler
from ..data.models import EstimationResult
from ..data.fresh_draws import FreshDrawDataset

logger = logging.getLogger(__name__)

_EPS = 1e-7  # numerical guard for logits/probabilities


def _expit(x: np.ndarray) -> np.ndarray:
    result = 1.0 / (1.0 + np.exp(-x))
    return np.asarray(result)


def _logit(p: np.ndarray) -> np.ndarray:
    p = np.clip(p, _EPS, 1.0 - _EPS)
    result = np.log(p) - np.log(1.0 - p)
    return np.asarray(result)


class TMLEEstimator(DREstimator):
    """TMLE with cross-fitted isotonic outcome models.

    Now properly inherits from DREstimator for consistency with other DR methods.

    Args:
        sampler: PrecomputedSampler with calibrated rewards
        n_folds: Number of cross-fitting folds (default 5)
        link: 'logit' (default, for rewards in [0,1]) or 'identity'
        max_iter: Max Newton steps for logistic targeting
        tol: Convergence tolerance on the (weighted) score
        use_calibrated_weights: Use CalibratedIPS (default True)
        **kwargs: Passed through to DREstimator
    """

    def __init__(
        self,
        sampler: PrecomputedSampler,
        n_folds: int = 5,
        link: str = "logit",
        max_iter: int = 50,
        tol: float = 1e-8,
        use_calibrated_weights: bool = True,
        calibrator: Optional[Any] = None,
        **kwargs: Any,
    ):
        # Initialize DR base with standard isotonic outcome model
        # Pass calibrator for proper index transformation with two-stage calibration
        outcome_model = IsotonicOutcomeModel(n_folds=n_folds, calibrator=calibrator)

        super().__init__(
            sampler=sampler,
            outcome_model=outcome_model,
            n_folds=n_folds,
            use_calibrated_weights=use_calibrated_weights,
            calibrator=calibrator,
            **kwargs,
        )

        if link not in {"logit", "identity"}:
            raise ValueError(f"link must be one of ['logit','identity'], got {link}")

        self.link = link
        self.max_iter = int(max_iter)
        self.tol = float(tol)

        # Per-policy epsilon/diagnostics
        self._tmle_info: Dict[str, Dict[str, Any]] = {}

        # Initialize storage for diagnostics (inherited from DREstimator but ensure they exist)
        if not hasattr(self, "_dm_component"):
            self._dm_component: Dict[str, np.ndarray] = {}
        if not hasattr(self, "_ips_correction"):
            self._ips_correction: Dict[str, np.ndarray] = {}
        if not hasattr(self, "_fresh_rewards"):
            self._fresh_rewards: Dict[str, np.ndarray] = {}
        if not hasattr(self, "_outcome_predictions"):
            self._outcome_predictions: Dict[str, np.ndarray] = {}

    def estimate(self) -> EstimationResult:
        """Compute TMLE estimates for all target policies.

        Extends the base DR estimate by adding a targeting step.
        """
        self._validate_fitted()

        # Auto-load fresh draws if not already loaded
        self._auto_load_fresh_draws()

        estimates: List[float] = []
        standard_errors: List[float] = []
        n_samples_used: Dict[str, int] = {}
        self._tmle_info = {}

        for policy in self.sampler.target_policies:
            # Ensure fresh draws available
            if policy not in self._fresh_draws:
                raise ValueError(
                    f"No fresh draws registered for '{policy}'. "
                    f"Call add_fresh_draws(policy, fresh_draws) before estimate()."
                )

            # Get data and weights using base class methods
            data = self.sampler.get_data_for_policy(policy)
            if not data:
                logger.warning(f"No valid data for policy '{policy}'. Skipping.")
                estimates.append(np.nan)
                standard_errors.append(np.nan)
                n_samples_used[policy] = 0
                continue

            weights = self.get_weights(policy)
            if weights is None or len(weights) != len(data):
                raise ValueError(
                    f"Weight/data mismatch for policy '{policy}': "
                    f"weights={None if weights is None else len(weights)}, data={len(data)}"
                )

            # Extract arrays
            rewards = np.array([d["reward"] for d in data], dtype=float)
            scores = np.array([d.get("judge_score") for d in data], dtype=float)
            prompt_ids = [str(d.get("prompt_id")) for d in data]
            prompts = [d["prompt"] for d in data]
            responses = [d["response"] for d in data]

            # Get fold assignments - strict mode (no fallback)
            unknown_pids = [
                pid for pid in prompt_ids if pid not in self._promptid_to_fold
            ]
            if unknown_pids:
                raise ValueError(
                    f"Missing fold assignments for {len(unknown_pids)} samples in policy '{policy}'. "
                    f"Example prompt_ids: {unknown_pids[:3]}. "
                    f"Ensure calibration was done with enable_cross_fit=True."
                )
            fold_ids = np.array(
                [self._promptid_to_fold[pid] for pid in prompt_ids], dtype=int
            )

            # 1) Get initial cross-fitted predictions on logged data
            g_logged0 = self.outcome_model.predict(prompts, responses, scores, fold_ids)

            # 2) Get initial predictions on fresh draws
            fresh_dataset = self._fresh_draws[policy]
            g_fresh0_all = []
            fresh_var_all = []

            for i, prompt_id in enumerate(prompt_ids):
                fresh_scores = fresh_dataset.get_scores_for_prompt_id(prompt_id)
                fresh_prompts = [prompts[i]] * len(fresh_scores)
                fresh_responses = [""] * len(fresh_scores)
                fresh_fold_ids = np.full(len(fresh_scores), fold_ids[i])

                g_fresh_prompt = self.outcome_model.predict(
                    fresh_prompts, fresh_responses, fresh_scores, fresh_fold_ids
                )
                g_fresh0_all.append(g_fresh_prompt.mean())

                if len(g_fresh_prompt) > 1:
                    fresh_var_all.append(g_fresh_prompt.var())
                else:
                    fresh_var_all.append(0.0)

            g_fresh0 = np.array(g_fresh0_all)
            fresh_var = np.array(fresh_var_all)

            # 3) Targeting step: solve for ε and update Q0 → Q*
            if self.link == "logit":
                eps, info = self._solve_logistic_fluctuation(
                    g_logged0, rewards, weights
                )
                # Clever covariate is W, so update is ε·W
                g_logged_star = _expit(_logit(g_logged0) + eps * weights)
                # DO NOT shift the fresh draw predictions - only the logged term
                g_fresh_star = g_fresh0
            else:  # identity link
                eps, info = self._solve_identity_fluctuation(
                    g_logged0, rewards, weights
                )
                # Update with ε·W for identity link too
                g_logged_star = np.clip(g_logged0 + eps * weights, 0.0, 1.0)
                # DO NOT shift the fresh draw predictions
                g_fresh_star = g_fresh0

            # Fit m̂(S) = E[W|S] for oracle augmentation if not already fitted
            if policy not in self.oracle_augmentation._m_hat_cache:
                # Use the existing fold_ids for cross-fitting consistency
                self.oracle_augmentation.fit_m_hat(
                    weights, scores, policy, cv_folds=fold_ids
                )

            # Add oracle slice augmentation for honest CIs
            aug_vector, aug_diagnostics = self.oracle_augmentation.compute_augmentation(
                policy,
                rewards,  # calibrated rewards
                data,
                self.sampler.dataset.samples,
            )
            self._aug_diagnostics[policy] = aug_diagnostics

            # 4) TMLE estimate = DM + IPS correction (using targeted predictions)
            dm_term = float(g_fresh_star.mean())
            # IPS correction now includes augmentation
            ips_corr_base = weights * (rewards - g_logged_star)
            ips_corr_total = ips_corr_base + aug_vector
            ips_corr = float(np.mean(ips_corr_total))
            psi = dm_term + ips_corr

            # 5) Standard error via empirical IF (include augmentation)
            if_contrib = g_fresh_star + ips_corr_total - psi
            se = (
                float(np.std(if_contrib, ddof=1) / np.sqrt(len(if_contrib)))
                if len(if_contrib) > 1
                else 0.0
            )

            # Store components for diagnostics (like parent DR does)
            self._dm_component[policy] = g_fresh0
            self._ips_correction[policy] = ips_corr_total  # With augmentation
            self._fresh_rewards[policy] = rewards  # Actually logged rewards
            self._outcome_predictions[policy] = g_logged0

            # Store influence functions (always needed for proper inference)
            self._influence_functions[policy] = if_contrib

            estimates.append(psi)
            standard_errors.append(se)
            n_samples_used[policy] = len(rewards)

            # Keep diagnostics
            info.update(
                dict(
                    link=self.link,
                    epsilon=float(info.get("epsilon", 0.0)),
                    dm=float(dm_term),
                    ips_correction=float(ips_corr),
                    n=len(rewards),
                    mean_weight=float(weights.mean()),
                )
            )
            self._tmle_info[policy] = info

            logger.info(
                f"TMLE[{policy}]: {psi:.4f} ± {se:.4f} "
                f"(ε={info.get('epsilon', 0.0):+.4f}, DM={dm_term:.4f}, IPS_corr={ips_corr:.4f})"
            )

        # Use base class to compute DR diagnostics (with our modifications)
        # We'll override _compute_dr_diagnostics to use original predictions
        base_result = self._create_base_result(
            estimates, standard_errors, n_samples_used
        )

        # Add TMLE-specific metadata
        base_result.method = "tmle"
        if base_result.metadata is None:
            base_result.metadata = {}

        base_result.metadata.update(
            {
                "link": self.link,
                "targeting": self._tmle_info,
            }
        )

        return base_result

    def _create_base_result(
        self,
        estimates: List[float],
        standard_errors: List[float],
        n_samples_used: Dict[str, int],
    ) -> EstimationResult:
        """Create base result with DR diagnostics computed on ORIGINAL predictions."""
        from ..diagnostics.dr import compute_dr_policy_diagnostics

        dr_diagnostics_per_policy = {}
        dr_calibration_data = {}

        for idx, policy in enumerate(self.sampler.target_policies):
            if policy not in self._fresh_draws or np.isnan(estimates[idx]):
                continue

            # Get data and components
            data = self.sampler.get_data_for_policy(policy)
            if not data:
                continue

            weights = self.get_weights(policy)
            if weights is None:
                continue
            rewards = np.array([d["reward"] for d in data], dtype=float)
            scores = np.array([d.get("judge_score") for d in data], dtype=float)
            prompt_ids = [str(d.get("prompt_id")) for d in data]
            prompts = [d["prompt"] for d in data]
            responses = [d["response"] for d in data]

            # Get fold assignments - strict mode (no fallback)
            unknown_pids = [
                pid for pid in prompt_ids if pid not in self._promptid_to_fold
            ]
            if unknown_pids:
                raise ValueError(
                    f"Missing fold assignments for {len(unknown_pids)} samples in policy '{policy}'. "
                    f"Example prompt_ids: {unknown_pids[:3]}. "
                    f"Ensure calibration was done with enable_cross_fit=True."
                )
            fold_ids = np.array(
                [self._promptid_to_fold[pid] for pid in prompt_ids], dtype=int
            )

            # Get ORIGINAL predictions (not targeted) for honest R²
            g_logged0 = self.outcome_model.predict(prompts, responses, scores, fold_ids)

            # Get fresh predictions
            fresh_dataset = self._fresh_draws[policy]
            g_fresh0_all = []
            fresh_var_all = []

            for i, prompt_id in enumerate(prompt_ids):
                fresh_scores = fresh_dataset.get_scores_for_prompt_id(prompt_id)
                fresh_prompts = [prompts[i]] * len(fresh_scores)
                fresh_responses = [""] * len(fresh_scores)
                fresh_fold_ids = np.full(len(fresh_scores), fold_ids[i])

                g_fresh_prompt = self.outcome_model.predict(
                    fresh_prompts, fresh_responses, fresh_scores, fresh_fold_ids
                )
                g_fresh0_all.append(g_fresh_prompt.mean())

                if len(g_fresh_prompt) > 1:
                    fresh_var_all.append(g_fresh_prompt.var())
                else:
                    fresh_var_all.append(0.0)

            g_fresh0 = np.array(g_fresh0_all)
            fresh_var = np.array(fresh_var_all)

            # Use base class helper for consistent diagnostic computation
            dr_diagnostics_per_policy[policy] = self._compute_policy_diagnostics(
                policy, estimates[idx]
            )

            # Store calibration data
            dr_calibration_data[policy] = {
                "g_logged": g_logged0,  # Original predictions
                "rewards": rewards,
            }

        # Create overview
        dr_overview = {}
        if dr_diagnostics_per_policy:
            dr_overview = {
                "policies": list(dr_diagnostics_per_policy.keys()),
                "dm_vs_ips": {
                    p: (d["dm_mean"], d["ips_corr_mean"])
                    for p, d in dr_diagnostics_per_policy.items()
                },
                "worst_if_tail_ratio_99_5": max(
                    d.get("if_tail_ratio_99_5", 0.0)
                    for d in dr_diagnostics_per_policy.values()
                ),
                "tmle_score_abs_mean": {
                    p: abs(d.get("score_mean", 0.0))
                    for p, d in dr_diagnostics_per_policy.items()
                },
                "tmle_max_score_z": max(
                    abs(d.get("score_z", 0.0))
                    for d in dr_diagnostics_per_policy.values()
                ),
            }

        # Get IPS diagnostics from the base estimator
        ips_diag = None
        if hasattr(self.ips_estimator, "get_diagnostics"):
            ips_diag = self.ips_estimator.get_diagnostics()

        # Build DRDiagnostics object
        diagnostics = self._build_dr_diagnostics(
            estimates=estimates,
            standard_errors=standard_errors,
            n_samples_used=n_samples_used,
            dr_diagnostics_per_policy=dr_diagnostics_per_policy,
            ips_diagnostics=ips_diag,
        )

        # Create metadata without influence functions (they're first-class now)
        metadata = {
            "cross_fitted": True,
            "n_folds": self.n_folds,
            "fresh_draws_policies": list(self._fresh_draws.keys()),
            "dr_diagnostics": dr_diagnostics_per_policy,  # Keep for backward compatibility
            "dr_overview": dr_overview,
            "dr_calibration_data": dr_calibration_data,
        }

        return EstimationResult(
            estimates=np.array(estimates, dtype=float),
            standard_errors=np.array(standard_errors, dtype=float),
            n_samples_used=n_samples_used,
            method="tmle",
            influence_functions=self._influence_functions,
            diagnostics=diagnostics,  # Add the DRDiagnostics object
            metadata=metadata,
        )

    def _solve_logistic_fluctuation(
        self, q0_logged: np.ndarray, rewards: np.ndarray, weights: np.ndarray
    ) -> Tuple[float, Dict[str, Any]]:
        """Solve for ε in logit(Q*) = logit(Q0) + ε·W using weighted logistic MLE.

        The clever covariate is W, so the fluctuation is ε·W, not just ε.
        Uses scale-aware convergence: |score| / sqrt(Fisher) < tol
        """
        # Guards
        q0 = np.clip(q0_logged, _EPS, 1.0 - _EPS)
        eta0 = _logit(q0)

        eps = 0.0
        converged = False
        score_val = None
        fisher_val = None
        normalized_score = None

        # If all weights ~0, skip targeting
        if float(np.sum(weights)) <= 0:
            return 0.0, dict(
                epsilon=0.0,
                converged=True,
                iters=0,
                score=0.0,
                fisher=0.0,
                normalized_score=0.0,
            )

        for t in range(self.max_iter):
            # CRITICAL FIX: clever covariate is W, so fluctuation is ε·W
            mu = _expit(eta0 + eps * weights)
            # weighted score (sum w*(y-mu))
            score = float(np.sum(weights * (rewards - mu)))
            # Fisher information for ε with clever covariate W
            fisher = float(np.sum((weights**2) * mu * (1.0 - mu)))

            score_val = score
            fisher_val = fisher

            # Scale-aware convergence: |score| / sqrt(Fisher)
            # This is the normalized score that accounts for parameter scale
            if fisher > 1e-12:
                normalized_score = abs(score) / np.sqrt(fisher)
                # Scale-aware tolerance (much more stringent and meaningful)
                if normalized_score <= self.tol:
                    converged = True
                    break
            else:
                # Fisher too small - declare converged if score is small
                if abs(score) <= self.tol:
                    converged = True
                    normalized_score = abs(score)  # Use raw score as fallback
                    break
                logger.warning(
                    "TMLE logistic fluctuation: near-singular Fisher; stopping early."
                )
                break

            # Newton update; cap step to avoid giant jumps
            step = score / fisher
            if abs(step) > 5.0:
                step = np.sign(step) * 5.0
            eps += step

        if not converged:
            logger.info(
                f"TMLE logistic fluctuation did not fully converge: "
                f"iters={self.max_iter}, normalized_score={normalized_score:.3e}, "
                f"|score|={abs(score_val) if score_val is not None else 0:.3e}, fisher={fisher_val:.3e}"
            )

        return float(eps), dict(
            epsilon=float(eps),
            converged=bool(converged),
            iters=int(t + 1),
            score=float(score_val if score_val is not None else 0.0),
            fisher=float(fisher_val if fisher_val is not None else 0.0),
            normalized_score=float(
                normalized_score if normalized_score is not None else 0.0
            ),
        )

    def _solve_identity_fluctuation(
        self, q0_logged: np.ndarray, rewards: np.ndarray, weights: np.ndarray
    ) -> Tuple[float, Dict[str, Any]]:
        """Solve for ε in Q* = Q0 + ε·W via the EIF score equation.

        The clever covariate is W, so the update is ε·W.
        """
        num = float(np.sum(weights * (rewards - q0_logged)))
        den = float(np.sum(weights**2))  # CRITICAL FIX: denominator is sum(W²)
        if den <= 1e-12:
            return 0.0, dict(
                epsilon=0.0, converged=True, iters=1, score=num, fisher=den
            )
        eps = num / den
        return float(eps), dict(
            epsilon=float(eps),
            converged=True,
            iters=1,
            score=float(num),
            fisher=den,
        )


=== ./cje/experiments/arena_10k_simplified/ablations/analyze_results.py ===

#!/usr/bin/env python3
"""Analyze ablation results and generate summary tables.

This script reads the results from all ablations and creates:
1. Summary statistics table
2. Key findings
3. Figures if requested
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns


def load_results(results_dir: Path) -> List[Dict[str, Any]]:
    """Load all results from a directory."""
    results = []
    results_file = results_dir / "results.jsonl"

    if results_file.exists():
        with open(results_file, "r") as f:
            for line in f:
                try:
                    results.append(json.loads(line))
                except:
                    pass

    return results


def analyze_oracle_coverage(results: List[Dict[str, Any]]) -> pd.DataFrame:
    """Analyze oracle coverage ablation results."""

    rows = []
    for r in results:
        if r.get("success", False):
            spec = r["spec"]
            rows.append(
                {
                    "Oracle Coverage": f"{spec['oracle_coverage']:.0%}",
                    "RMSE": r.get("rmse_vs_oracle", np.nan),
                    "CI Width": r.get("mean_ci_width", np.nan),
                    "Calibration RMSE": r.get("calibration_rmse", np.nan),
                    "Runtime (s)": r.get("runtime_s", np.nan),
                    "Seed": r.get("seed", 0),
                }
            )

    if not rows:
        return pd.DataFrame()

    df = pd.DataFrame(rows)

    # Aggregate by coverage level
    summary = (
        df.groupby("Oracle Coverage")
        .agg(
            {
                "RMSE": ["mean", "std"],
                "CI Width": "mean",
                "Calibration RMSE": "mean",
                "Runtime (s)": "mean",
            }
        )
        .round(4)
    )

    return summary


def analyze_sample_size(results: List[Dict[str, Any]]) -> pd.DataFrame:
    """Analyze sample size ablation results."""

    rows = []
    for r in results:
        if r.get("success", False):
            spec = r["spec"]

            # Get mean ESS across policies
            mean_ess = np.nan
            if "ess_absolute" in r:
                mean_ess = np.mean(list(r["ess_absolute"].values()))

            rows.append(
                {
                    "Estimator": spec["estimator"],
                    "Sample Size": spec.get("sample_size", r.get("n_samples", 0)),
                    "RMSE": r.get("rmse_vs_oracle", np.nan),
                    "Mean SE": (
                        np.mean(list(r.get("standard_errors", {}).values()))
                        if r.get("standard_errors")
                        else np.nan
                    ),
                    "ESS": mean_ess,
                    "ESS %": (
                        100 * mean_ess / spec.get("sample_size", 1)
                        if not np.isnan(mean_ess)
                        else np.nan
                    ),
                    "Runtime (s)": r.get("runtime_s", np.nan),
                    "Seed": r.get("seed", 0),
                }
            )

    if not rows:
        return pd.DataFrame()

    df = pd.DataFrame(rows)

    # Aggregate by estimator and sample size
    summary = (
        df.groupby(["Estimator", "Sample Size"])
        .agg(
            {
                "RMSE": ["mean", "std"],
                "Mean SE": "mean",
                "ESS": "mean",
                "ESS %": "mean",
                "Runtime (s)": "mean",
            }
        )
        .round(4)
    )

    return summary


def analyze_interaction(results: List[Dict[str, Any]]) -> pd.DataFrame:
    """Analyze interaction ablation results."""

    rows = []
    for r in results:
        if r.get("success", False):
            spec = r["spec"]
            rows.append(
                {
                    "Oracle Coverage": f"{spec['oracle_coverage']:.0%}",
                    "Sample Size": spec.get("sample_size", r.get("n_samples", 0)),
                    "N Oracle": int(
                        spec["oracle_coverage"] * spec.get("sample_size", 0)
                    ),
                    "RMSE": r.get("rmse_vs_oracle", np.nan),
                    "CI Width": r.get("mean_ci_width", np.nan),
                    "Runtime (s)": r.get("runtime_s", np.nan),
                    "Seed": r.get("seed", 0),
                }
            )

    if not rows:
        return pd.DataFrame()

    df = pd.DataFrame(rows)

    # Create pivot table
    pivot = df.pivot_table(
        values="RMSE", index="Oracle Coverage", columns="Sample Size", aggfunc="mean"
    ).round(4)

    return pivot


def compute_key_findings(
    oracle_df: pd.DataFrame, sample_df: pd.DataFrame, interaction_df: pd.DataFrame
):
    """Extract key findings from results."""

    findings = []

    # Oracle coverage findings
    if not oracle_df.empty:
        rmse_col = ("RMSE", "mean")
        if rmse_col in oracle_df.columns:
            rmse_values = oracle_df[rmse_col].values
            coverages = oracle_df.index

            # Find where RMSE stabilizes (< 5% improvement)
            if len(rmse_values) > 1:
                improvements = -np.diff(rmse_values)
                rel_improvements = improvements / rmse_values[:-1]

                stable_idx = np.where(rel_improvements < 0.05)[0]
                if len(stable_idx) > 0:
                    sweet_spot = coverages[stable_idx[0] + 1]
                    findings.append(
                        f"Oracle coverage sweet spot: {sweet_spot} (diminishing returns beyond)"
                    )

                # Overall improvement from min to max coverage
                total_improvement = (rmse_values[0] - rmse_values[-1]) / rmse_values[0]
                findings.append(
                    f"RMSE reduction from 1% to 100% oracle: {total_improvement:.1%}"
                )

    # Sample size findings
    if not sample_df.empty:
        for estimator in sample_df.index.get_level_values(0).unique():
            est_data = sample_df.loc[estimator]

            if ("RMSE", "mean") in est_data.columns:
                rmse_values = est_data[("RMSE", "mean")].values
                sample_sizes = est_data.index

                if len(rmse_values) >= 2:
                    # Check √n scaling
                    log_n = np.log(sample_sizes)
                    log_rmse = np.log(rmse_values + 1e-10)

                    # Linear regression in log space
                    from scipy import stats

                    slope, _, r_value, _, _ = stats.linregress(log_n, log_rmse)

                    if abs(slope + 0.5) < 0.1:  # Close to -0.5
                        findings.append(
                            f"{estimator}: Follows √n scaling (slope={slope:.2f}, R²={r_value**2:.3f})"
                        )
                    else:
                        findings.append(
                            f"{estimator}: Deviates from √n scaling (slope={slope:.2f})"
                        )

    # Interaction findings
    if not interaction_df.empty:
        # Find most efficient configurations (low RMSE × oracle cost)
        best_configs = []
        for oracle in interaction_df.index:
            for n_samples in interaction_df.columns:
                rmse = interaction_df.loc[oracle, n_samples]
                if not np.isnan(rmse):
                    oracle_pct = float(oracle.strip("%")) / 100
                    n_oracle = oracle_pct * n_samples
                    efficiency = 1.0 / (n_oracle * rmse)
                    best_configs.append((oracle, n_samples, rmse, efficiency))

        if best_configs:
            best_configs.sort(key=lambda x: x[3], reverse=True)
            top = best_configs[0]
            findings.append(
                f"Most efficient config: {top[0]} oracle, n={top[1]} (RMSE={top[2]:.3f})"
            )

    return findings


def generate_summary_table(results_dir: Path = Path("ablations/results")):
    """Generate comprehensive summary table."""

    print("=" * 70)
    print("ABLATION RESULTS SUMMARY")
    print("=" * 70)
    print()

    # Load results
    oracle_results = load_results(results_dir / "oracle_coverage")
    sample_results = load_results(results_dir / "sample_size")
    interaction_results = load_results(results_dir / "interaction")
    estimator_results = load_results(results_dir / "estimator_comparison")

    print(f"Loaded results:")
    print(f"  Oracle coverage: {len(oracle_results)} experiments")
    print(f"  Sample size: {len(sample_results)} experiments")
    print(f"  Interaction: {len(interaction_results)} experiments")
    print(f"  Estimator comparison: {len(estimator_results)} experiments")
    print()

    # Analyze each ablation
    oracle_df = analyze_oracle_coverage(oracle_results)
    sample_df = analyze_sample_size(sample_results)
    interaction_df = analyze_interaction(interaction_results)

    # Print oracle coverage results
    if not oracle_df.empty:
        print("ORACLE COVERAGE RESULTS")
        print("-" * 40)
        print(oracle_df)
        print()

    # Print sample size results
    if not sample_df.empty:
        print("SAMPLE SIZE RESULTS")
        print("-" * 40)
        print(sample_df)
        print()

    # Print interaction results
    if not interaction_df.empty:
        print("INTERACTION RESULTS (RMSE)")
        print("-" * 40)
        print(interaction_df)
        print()

    # Note: Policy heterogeneity analysis is now done within estimator_comparison.py
    # It saves results to results/estimator_comparison/policy_heterogeneity.png

    # Compute key findings
    findings = compute_key_findings(oracle_df, sample_df, interaction_df)

    if findings:
        print("KEY FINDINGS")
        print("-" * 40)
        for i, finding in enumerate(findings, 1):
            print(f"{i}. {finding}")
        print()

    # Save to CSV
    if not oracle_df.empty:
        oracle_df.to_csv(results_dir / "oracle_coverage_summary.csv")
        print(f"Saved: {results_dir}/oracle_coverage_summary.csv")

    if not sample_df.empty:
        sample_df.to_csv(results_dir / "sample_size_summary.csv")
        print(f"Saved: {results_dir}/sample_size_summary.csv")

    if not interaction_df.empty:
        interaction_df.to_csv(results_dir / "interaction_summary.csv")
        print(f"Saved: {results_dir}/interaction_summary.csv")

    print()
    print("=" * 70)
    print("ANALYSIS COMPLETE")
    print("=" * 70)


def main():
    """Run full analysis."""
    import argparse

    parser = argparse.ArgumentParser(description="Analyze ablation results")
    parser.add_argument(
        "--results-dir",
        type=Path,
        default=Path("ablations/results"),
        help="Directory containing ablation results",
    )
    parser.add_argument("--figures", action="store_true", help="Also generate figures")

    args = parser.parse_args()

    # Generate summary table
    generate_summary_table(args.results_dir)

    # Generate figures if requested
    if args.figures:
        print("\nGenerating figures...")
        # Import and run figure generation from each ablation
        import sys

        sys.path.append("experiments")

        try:
            from oracle_coverage import OracleCoverageAblation
            from sample_size import SampleSizeAblation
            from interaction import InteractionAblation

            # Load and create figures
            oracle_results = load_results(args.results_dir / "oracle_coverage")
            if oracle_results:
                ablation = OracleCoverageAblation()
                ablation.create_figure(
                    oracle_results,
                    args.results_dir
                    / "oracle_coverage"
                    / "figure_1_oracle_coverage.png",
                )
                print("Created: oracle_coverage/figure_1_oracle_coverage.png")

            sample_results = load_results(args.results_dir / "sample_size")
            if sample_results:
                ablation = SampleSizeAblation()
                ablation.create_figure(
                    sample_results,
                    args.results_dir / "sample_size" / "figure_2_sample_scaling.png",
                )
                print("Created: sample_size/figure_2_sample_scaling.png")

            interaction_results = load_results(args.results_dir / "interaction")
            if interaction_results:
                ablation = InteractionAblation()
                ablation.create_figure(
                    interaction_results,
                    args.results_dir / "interaction" / "figure_3_interaction.png",
                )
                print("Created: interaction/figure_3_interaction.png")

        except Exception as e:
            print(f"Error generating figures: {e}")


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/ablations/core/__init__.py ===

"""Core infrastructure for CJE ablations."""

from .schemas import ExperimentSpec, create_result
from .diagnostics import (
    effective_sample_size,
    hill_alpha,
    simcal_distortion,
    weight_cv,
    compute_rmse,
)

__all__ = [
    "ExperimentSpec",
    "create_result",
    "effective_sample_size",
    "hill_alpha",
    "simcal_distortion",
    "weight_cv",
    "compute_rmse",
]


=== ./cje/experiments/arena_10k_simplified/ablations/core/base.py ===

"""Base class for ablation experiments."""

import json
import logging
import random
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import numpy as np

from .schemas import ExperimentSpec, create_result
from .diagnostics import (
    effective_sample_size,
    hill_alpha,
    weight_cv,
    compute_rmse,
    simcal_distortion,
)

# Add parent directories to path for imports
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from cje import load_dataset_from_jsonl
from cje.calibration import calibrate_dataset
from cje.data.precomputed_sampler import PrecomputedSampler
from cje.estimators import CalibratedIPS, StackedDREstimator
from cje.estimators.dr_base import DRCPOEstimator
from cje.estimators.mrdr import MRDREstimator
from cje.estimators.tmle import TMLEEstimator
from cje.data.fresh_draws import load_fresh_draws_auto

logger = logging.getLogger(__name__)


class BaseAblation:
    """Base class for all ablation experiments.

    Provides common functionality:
    - Data loading and preparation
    - Oracle masking and calibration
    - Estimator creation and execution
    - Diagnostic computation
    """

    def __init__(self, name: str):
        """Initialize ablation.

        Args:
            name: Name of this ablation (e.g., "oracle_coverage")
        """
        self.name = name
        self.results: List[Dict[str, Any]] = []

    def prepare_dataset(
        self, spec: ExperimentSpec, seed: int
    ) -> Tuple[Any, int, Dict[int, Any]]:
        """Load and prepare dataset with oracle masking.

        Args:
            spec: Experiment specification
            seed: Random seed

        Returns:
            (dataset, n_oracle, original_oracle_labels)
        """
        # Set seed for reproducibility
        random.seed(seed)
        np.random.seed(seed)

        # Load dataset
        dataset = load_dataset_from_jsonl(spec.dataset_path)

        # Subsample if requested
        if spec.sample_size is not None:
            n_samples = min(spec.sample_size, len(dataset.samples))
            indices = sorted(random.sample(range(len(dataset.samples)), n_samples))
            dataset.samples = [dataset.samples[i] for i in indices]
        elif spec.sample_fraction is not None:
            n_samples = int(len(dataset.samples) * spec.sample_fraction)
            indices = sorted(random.sample(range(len(dataset.samples)), n_samples))
            dataset.samples = [dataset.samples[i] for i in indices]

        # Mask oracle labels if coverage < 1
        original_oracle_labels = {}
        n_oracle = len(dataset.samples)  # Default: all have oracle

        if spec.oracle_coverage is not None and spec.oracle_coverage < 1.0:
            # Find samples with oracle labels
            oracle_indices = [
                i
                for i, s in enumerate(dataset.samples)
                if s.metadata.get("oracle_label") is not None
            ]

            # Determine how many to keep
            n_keep = max(2, int(len(oracle_indices) * spec.oracle_coverage))
            keep_indices = set(
                random.sample(oracle_indices, min(n_keep, len(oracle_indices)))
            )

            # Mask labels not in keep set
            for i, sample in enumerate(dataset.samples):
                if i not in keep_indices and "oracle_label" in sample.metadata:
                    original_oracle_labels[i] = sample.metadata["oracle_label"]
                    sample.metadata = sample.metadata.copy()
                    sample.metadata["oracle_label"] = None

            n_oracle = len(keep_indices)

        return dataset, n_oracle, original_oracle_labels

    def create_estimator(
        self, spec: ExperimentSpec, sampler: PrecomputedSampler, cal_result: Any
    ) -> Any:
        """Create estimator based on specification.

        Args:
            spec: Experiment specification
            sampler: PrecomputedSampler with data
            cal_result: Calibration result

        Returns:
            Estimator instance
        """
        estimator_map = {
            "raw-ips": lambda s: CalibratedIPS(s, calibrate=False),
            "calibrated-ips": lambda s: CalibratedIPS(s),
            "dr-cpo": lambda s: DRCPOEstimator(
                s,
                calibrator=cal_result.calibrator if cal_result else None,
                n_folds=5,
                oracle_slice_config=(
                    spec.oracle_coverage < 1.0 if spec.oracle_coverage else False
                ),
                use_calibrated_weights=False,  # No weight calibration
            ),
            "calibrated-dr-cpo": lambda s: DRCPOEstimator(
                s,
                calibrator=cal_result.calibrator if cal_result else None,
                n_folds=5,
                oracle_slice_config=(
                    spec.oracle_coverage < 1.0 if spec.oracle_coverage else False
                ),
                use_calibrated_weights=True,  # Use SIMCal calibrated weights
            ),
            "mrdr": lambda s: MRDREstimator(
                s,
                calibrator=cal_result.calibrator if cal_result else None,
                n_folds=5,
                oracle_slice_config=(
                    spec.oracle_coverage < 1.0 if spec.oracle_coverage else False
                ),
                use_calibrated_weights=True,  # Default: use calibrated weights
            ),
            "tmle": lambda s: TMLEEstimator(
                s,
                calibrator=cal_result.calibrator if cal_result else None,
                n_folds=5,
                oracle_slice_config=(
                    spec.oracle_coverage < 1.0 if spec.oracle_coverage else False
                ),
                use_calibrated_weights=True,  # Default: use calibrated weights
            ),
            "stacked-dr": lambda s: StackedDREstimator(
                s,
                calibrator=cal_result.calibrator if cal_result else None,
                n_folds=5,
                oracle_slice_config=(
                    spec.oracle_coverage < 1.0 if spec.oracle_coverage else False
                ),
                use_calibrated_weights=False,  # No weight calibration
            ),
            "cal-stacked-dr": lambda s: StackedDREstimator(
                s,
                calibrator=cal_result.calibrator if cal_result else None,
                n_folds=5,
                oracle_slice_config=(
                    spec.oracle_coverage < 1.0 if spec.oracle_coverage else False
                ),
                use_calibrated_weights=True,  # Use SIMCal calibrated weights
            ),
        }

        if spec.estimator not in estimator_map:
            raise ValueError(f"Unknown estimator: {spec.estimator}")

        return estimator_map[spec.estimator](sampler)

    def _load_oracle_ground_truth(
        self, dataset_path: str, dataset: Any, target_policies: List[str]
    ) -> Dict[str, float]:
        """Load oracle ground truth values for comparison.

        Args:
            dataset_path: Path to dataset file
            dataset: Dataset object
            target_policies: List of target policies

        Returns:
            Dictionary mapping policy names to oracle mean values
        """
        oracle_means = {}
        data_dir = Path(dataset_path).parent
        responses_dir = data_dir / "responses"

        # Load base policy oracle labels from dataset
        base_oracle_values = []
        for sample in dataset.samples:
            if hasattr(sample, "metadata") and sample.metadata:
                oracle_val = sample.metadata.get("oracle_label")
                if oracle_val is not None:
                    base_oracle_values.append(oracle_val)

        if base_oracle_values:
            oracle_means["base"] = float(np.mean(base_oracle_values))

        # Load oracle labels for each target policy from response files
        for policy in target_policies:
            response_file = responses_dir / f"{policy}_responses.jsonl"
            if response_file.exists():
                oracle_values = []
                with open(response_file, "r") as f:
                    for line in f:
                        try:
                            data = json.loads(line)
                            if (
                                "metadata" in data
                                and "oracle_label" in data["metadata"]
                            ):
                                oracle_val = data["metadata"]["oracle_label"]
                                if oracle_val is not None:
                                    oracle_values.append(oracle_val)
                        except json.JSONDecodeError:
                            continue

                if oracle_values:
                    oracle_means[policy] = float(np.mean(oracle_values))

        return oracle_means

    def compute_diagnostics(
        self, estimator: Any, result: Dict[str, Any], n_total: int
    ) -> None:
        """Compute and add diagnostics to result.

        Args:
            estimator: Fitted estimator
            result: Result dictionary to update
            n_total: Total number of samples
        """
        # Get target policies
        policies = estimator.sampler.target_policies

        for policy in policies:
            try:
                # Get weights (method may vary by estimator)
                if hasattr(estimator, "get_weights"):
                    weights = estimator.get_weights(policy)
                elif hasattr(estimator, "_weights_cache"):
                    weights = estimator._weights_cache.get(policy)
                else:
                    weights = estimator.sampler.compute_importance_weights(policy)

                if weights is not None and len(weights) > 0:
                    # Compute diagnostics
                    ess = effective_sample_size(weights)
                    result["ess_absolute"][policy] = ess
                    result["ess_relative"][policy] = (
                        100.0 * ess / n_total if n_total > 0 else 0
                    )
                    result["tail_alpha"][policy] = hill_alpha(weights)
                    result["weight_cv"][policy] = weight_cv(weights)

                    # Max weight (normalized)
                    weights_norm = weights / np.sum(weights)
                    result["max_weight"][policy] = np.max(weights_norm)

            except Exception as e:
                logger.warning(f"Failed to compute diagnostics for {policy}: {e}")

    def run_single(self, spec: ExperimentSpec, seed: int) -> Dict[str, Any]:
        """Run single experiment with given seed.

        Args:
            spec: Experiment specification
            seed: Random seed

        Returns:
            Result dictionary
        """
        # Create result
        result = create_result(spec, seed)

        try:
            # Prepare data
            dataset, n_oracle, original_oracle_labels = self.prepare_dataset(spec, seed)
            result["n_samples"] = len(dataset.samples)
            result["n_oracle"] = n_oracle

            # Calibrate
            calibrated_dataset, cal_result = calibrate_dataset(
                dataset,
                judge_field="judge_score",
                oracle_field="oracle_label",
                enable_cross_fit=True,
                n_folds=5 if n_oracle >= 50 else 3,
            )

            if cal_result:
                result["calibration_rmse"] = cal_result.calibration_rmse

            # Create sampler and estimator
            sampler = PrecomputedSampler(calibrated_dataset)
            estimator = self.create_estimator(spec, sampler, cal_result)

            # Add fresh draws for DR methods
            if spec.estimator in [
                "dr-cpo",
                "calibrated-dr-cpo",
                "mrdr",
                "tmle",
                "stacked-dr",
                "cal-stacked-dr",
            ]:
                data_dir = Path(spec.dataset_path).parent

                # Get prompt IDs from the subsampled dataset
                dataset_prompt_ids = set()
                for sample in calibrated_dataset.samples:
                    if hasattr(sample, "prompt_id") and sample.prompt_id:
                        dataset_prompt_ids.add(sample.prompt_id)

                for policy in sampler.target_policies:
                    try:
                        # Load ALL fresh draws
                        all_fresh_draws = load_fresh_draws_auto(
                            data_dir, policy, verbose=False
                        )

                        # Filter to only include fresh draws matching our subsampled prompts
                        if dataset_prompt_ids:
                            filtered_samples = []
                            for fd_sample in all_fresh_draws.samples:
                                if (
                                    hasattr(fd_sample, "prompt_id")
                                    and fd_sample.prompt_id in dataset_prompt_ids
                                ):
                                    filtered_samples.append(fd_sample)

                            # Create filtered fresh draws dataset with required fields
                            from cje.data.fresh_draws import FreshDrawDataset

                            # Count draws per prompt
                            draws_per_prompt_dict = {}
                            for sample in filtered_samples:
                                prompt_id = (
                                    sample.prompt_id
                                    if hasattr(sample, "prompt_id")
                                    else None
                                )
                                if prompt_id:
                                    draws_per_prompt_dict[prompt_id] = (
                                        draws_per_prompt_dict.get(prompt_id, 0) + 1
                                    )

                            # Get the most common draws per prompt value
                            draws_per_prompt = (
                                max(
                                    set(draws_per_prompt_dict.values()),
                                    key=list(draws_per_prompt_dict.values()).count,
                                )
                                if draws_per_prompt_dict
                                else 10
                            )

                            filtered_fresh_draws = FreshDrawDataset(
                                samples=filtered_samples,
                                target_policy=policy,  # Use the policy we're processing
                                draws_per_prompt=draws_per_prompt,
                            )

                            estimator.add_fresh_draws(policy, filtered_fresh_draws)
                            logger.info(
                                f"Added {len(filtered_samples)}/{len(all_fresh_draws.samples)} fresh draws for {policy}"
                            )
                        else:
                            # If no prompt IDs, use all fresh draws (fallback)
                            estimator.add_fresh_draws(policy, all_fresh_draws)

                    except FileNotFoundError:
                        logger.warning(f"No fresh draws for {policy}")

            # Run estimation
            estimation_result = estimator.fit_and_estimate()

            # Extract results
            for i, policy in enumerate(sampler.target_policies):
                result["estimates"][policy] = float(estimation_result.estimates[i])
                if estimation_result.standard_errors is not None:
                    result["standard_errors"][policy] = float(
                        estimation_result.standard_errors[i]
                    )
                    # Compute confidence interval from SE
                    est = estimation_result.estimates[i]
                    se = estimation_result.standard_errors[i]
                    result["confidence_intervals"][policy] = (
                        float(est - 1.96 * se),
                        float(est + 1.96 * se),
                    )

            # Compute diagnostics
            self.compute_diagnostics(estimator, result, len(dataset.samples))

            # Restore oracle labels for ground truth computation
            if original_oracle_labels:
                for idx, oracle_label in original_oracle_labels.items():
                    dataset.samples[idx].metadata["oracle_label"] = oracle_label

            # Compute oracle truths
            oracle_truths = self._load_oracle_ground_truth(
                spec.dataset_path,
                dataset,
                list(sampler.target_policies),
            )
            result["oracle_truths"] = oracle_truths

            # Compute RMSE
            result["rmse_vs_oracle"] = compute_rmse(result["estimates"], oracle_truths)

            # Mean CI width
            if result["confidence_intervals"]:
                widths = [
                    ci[1] - ci[0] for ci in result["confidence_intervals"].values()
                ]
                result["mean_ci_width"] = np.mean(widths)

            result["success"] = True

        except Exception as e:
            logger.error(f"Experiment failed: {e}")
            result["error"] = str(e)
            result["success"] = False

        result["runtime_s"] = time.time() - result["start_ts"]

        return result

    def run_with_seeds(self, spec: ExperimentSpec) -> List[Dict[str, Any]]:
        """Run experiment with multiple seeds.

        Args:
            spec: Experiment specification

        Returns:
            List of results (one per seed)
        """
        results = []
        for i in range(spec.n_seeds):
            seed = spec.seed_base + i
            logger.info(f"Running {self.name} with seed {seed} ({i+1}/{spec.n_seeds})")
            result = self.run_single(spec, seed)
            results.append(result)

            # Log progress
            if result["success"]:
                logger.info(f"  ✓ RMSE: {result.get('rmse_vs_oracle', 'N/A'):.4f}")
            else:
                logger.warning(f"  ✗ Failed: {result.get('error', 'Unknown')}")

        return results

    def run_ablation(self) -> List[Dict[str, Any]]:
        """Run the complete ablation.

        Override this in subclasses to define the experiment grid.
        """
        raise NotImplementedError("Subclasses must implement run_ablation()")


=== ./cje/experiments/arena_10k_simplified/ablations/core/diagnostics.py ===

"""Diagnostic functions for ablation experiments."""

import numpy as np
from typing import Optional, Dict, Any, Tuple


def effective_sample_size(weights: np.ndarray) -> float:
    """Compute effective sample size (ESS).

    ESS = (sum w)^2 / sum(w^2)

    This measures how many "effective" samples we have after importance weighting.
    An ESS of 100 means the weights are as variable as if we had 100 equal samples.

    Args:
        weights: Importance weights (should sum to n for Hajek, or be normalized)

    Returns:
        ESS (absolute number)
    """
    if len(weights) == 0:
        return 0.0

    weights = np.asarray(weights)
    weights = weights[np.isfinite(weights)]  # Remove NaN/inf

    if len(weights) == 0:
        return 0.0

    sum_w = np.sum(weights)
    sum_w2 = np.sum(weights**2)

    if sum_w2 == 0:
        return 0.0

    return sum_w**2 / sum_w2


def hill_alpha(weights: np.ndarray, k: Optional[int] = None) -> float:
    """Compute Hill estimator of tail index α.

    The tail index measures how heavy the tail is:
    - α > 2: Light tails, finite variance (good)
    - α ≤ 2: Heavy tails, infinite variance (bad)
    - α ≤ 1: Very heavy tails, infinite mean (very bad)

    Args:
        weights: Importance weights
        k: Number of tail observations to use (default: 5% or 50, whichever is larger)

    Returns:
        Hill estimator α̂
    """
    weights = np.asarray(weights)
    weights = weights[weights > 0]  # Only positive weights
    weights = weights[np.isfinite(weights)]  # Remove NaN/inf

    if len(weights) < 10:
        return np.nan  # Not enough data

    # Default k: 5% of data or 50, whichever is larger
    if k is None:
        k = max(50, int(0.05 * len(weights)))
        k = min(k, len(weights) // 2)  # But at most half the data

    # Sort and get tail
    x_sorted = np.sort(weights)
    x_tail = x_sorted[-k:]  # k largest values
    x_k = x_sorted[-k]  # k-th largest (threshold)

    # Hill estimator: 1 / mean(log(X_i / X_k)) for X_i > X_k
    log_ratios = np.log(x_tail / x_k)

    # Avoid division by zero
    mean_log_ratio = np.mean(log_ratios)
    if mean_log_ratio == 0:
        return np.inf

    return 1.0 / mean_log_ratio


def simcal_distortion(W_original: np.ndarray, W_calibrated: np.ndarray) -> float:
    """Compute relative L2 distortion from SIMCal calibration.

    δ = ||W_cal - W_orig||_2 / ||W_orig||_2

    This measures how much the weights were changed by calibration.
    Lower is better (less distortion).

    Args:
        W_original: Original importance weights
        W_calibrated: Calibrated weights

    Returns:
        Relative distortion δ
    """
    W_original = np.asarray(W_original)
    W_calibrated = np.asarray(W_calibrated)

    # Handle shape mismatch
    if W_original.shape != W_calibrated.shape:
        return np.nan

    # Remove NaN/inf
    mask = np.isfinite(W_original) & np.isfinite(W_calibrated)
    W_original = W_original[mask]
    W_calibrated = W_calibrated[mask]

    if len(W_original) == 0:
        return np.nan

    norm_orig = np.linalg.norm(W_original)
    if norm_orig == 0:
        return np.nan

    return np.linalg.norm(W_calibrated - W_original) / norm_orig


def weight_cv(weights: np.ndarray) -> float:
    """Compute coefficient of variation of weights.

    CV = std(w) / mean(w)

    This is a normalized measure of weight variability.
    Lower is better (more uniform weights).

    Args:
        weights: Importance weights

    Returns:
        Coefficient of variation
    """
    weights = np.asarray(weights)
    weights = weights[np.isfinite(weights)]

    if len(weights) == 0:
        return np.nan

    mean_w = np.mean(weights)
    if mean_w == 0:
        return np.nan

    return np.std(weights) / mean_w


def compute_rmse(estimates: Dict[str, float], truths: Dict[str, float]) -> float:
    """Compute RMSE between estimates and oracle truths.
    
    Note: The 'unhelpful' policy is excluded from RMSE calculation because
    it has a very different reward distribution (mean ~0.14) compared to
    other policies (mean ~0.76). This causes systematic calibration bias
    when the reward calibrator (trained on base policy data) is applied
    to unhelpful policy responses.

    Args:
        estimates: Policy -> estimate
        truths: Policy -> oracle truth

    Returns:
        Root mean squared error
    """
    if not estimates or not truths:
        return np.nan

    squared_errors = []
    for policy in estimates:
        # Skip unhelpful policy - it has very different reward distribution
        # and causes calibration bias issues
        if policy == "unhelpful":
            continue
            
        if policy in truths:
            est = estimates[policy]
            truth = truths[policy]
            if np.isfinite(est) and np.isfinite(truth):
                squared_errors.append((est - truth) ** 2)

    if not squared_errors:
        return np.nan

    return np.sqrt(np.mean(squared_errors))


def compute_calibration_metrics(
    judge_scores: np.ndarray,
    oracle_labels: np.ndarray,
    calibrated_scores: Optional[np.ndarray] = None,
) -> Dict[str, float]:
    """Compute calibration quality metrics.

    Args:
        judge_scores: Original judge scores
        oracle_labels: Oracle ground truth labels
        calibrated_scores: Calibrated predictions (optional)

    Returns:
        Dictionary of calibration metrics
    """
    # Remove missing values
    mask = np.isfinite(judge_scores) & np.isfinite(oracle_labels)
    judge_scores = judge_scores[mask]
    oracle_labels = oracle_labels[mask]

    if len(judge_scores) == 0:
        return {"calibration_rmse": np.nan, "kendall_tau": np.nan}

    metrics = {}

    # RMSE of judge vs oracle
    metrics["judge_oracle_rmse"] = np.sqrt(np.mean((judge_scores - oracle_labels) ** 2))

    # Kendall's tau (rank correlation)
    from scipy import stats

    tau, _ = stats.kendalltau(judge_scores, oracle_labels)
    metrics["kendall_tau"] = tau

    # If calibrated scores provided, compute improvement
    if calibrated_scores is not None:
        calibrated_scores = calibrated_scores[mask]
        metrics["calibrated_rmse"] = np.sqrt(
            np.mean((calibrated_scores - oracle_labels) ** 2)
        )
        metrics["calibration_improvement"] = (
            metrics["judge_oracle_rmse"] - metrics["calibrated_rmse"]
        ) / metrics["judge_oracle_rmse"]

    return metrics


def compute_paired_delta(
    results_baseline: Dict[str, Any],
    results_treatment: Dict[str, Any],
    metric: str = "rmse_vs_oracle",
) -> Tuple[float, float]:
    """Compute paired difference with standard error.

    Used for comparing parameter settings (e.g., ρ=1.0 vs ρ=0.6)
    with the same seeds and oracle slices.

    Args:
        results_baseline: Baseline results
        results_treatment: Treatment results
        metric: Which metric to compare

    Returns:
        (delta, se) where delta = treatment - baseline
    """
    # This would need the raw paired differences, not just aggregates
    # For now, return simple difference
    baseline_val = results_baseline.get(metric, np.nan)
    treatment_val = results_treatment.get(metric, np.nan)

    if not np.isfinite(baseline_val) or not np.isfinite(treatment_val):
        return np.nan, np.nan

    delta = treatment_val - baseline_val
    # SE would require access to paired seed-level differences
    se = np.nan  # Placeholder

    return delta, se


=== ./cje/experiments/arena_10k_simplified/ablations/core/schemas.py ===

"""Standardized schemas for ablation experiments."""

import json
import time
from dataclasses import dataclass, asdict, field
from typing import Optional, Dict, Any, List
import subprocess


@dataclass(frozen=True)
class ExperimentSpec:
    """Standardized input specification for ablation experiments.

    This ensures every ablation speaks the same language and enables
    caching, reproducibility, and easy aggregation.
    """

    # Required fields
    ablation: str  # e.g., "oracle_coverage"
    dataset_path: str  # Path to CJE dataset
    estimator: str  # e.g., "calibrated-ips", "dr-cpo"

    # Data configuration
    oracle_coverage: Optional[float] = None  # Fraction of oracle labels (0-1)
    sample_size: Optional[int] = None  # Number of samples to use
    sample_fraction: Optional[float] = None  # Alternative: fraction of dataset

    # Method parameters
    rho: Optional[float] = None  # Variance cap for SIMCal
    lambda_tr: Optional[float] = None  # Trust region tempering
    ordering: Optional[str] = None  # "S", "R", "S_bucket_shuffle"
    draws_per_prompt: Optional[int] = None  # Fresh draws for DR (K)

    # Temporal analysis
    temporal_block: Optional[int] = None  # Block size for bootstrap

    # Experiment configuration
    n_seeds: int = 5  # Number of random seeds
    seed_base: int = 42  # Base seed for reproducibility

    # Additional parameters
    extra: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return asdict(self)


def get_git_commit() -> Optional[str]:
    """Get current git commit hash."""
    try:
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"], capture_output=True, text=True, check=True
        )
        return result.stdout.strip()[:8]
    except (subprocess.CalledProcessError, FileNotFoundError):
        return None


def create_result(spec: ExperimentSpec, seed: int) -> Dict[str, Any]:
    """Create standardized result dictionary.

    Args:
        spec: Experiment specification
        seed: Random seed for this run

    Returns:
        Dictionary with all fields initialized
    """
    return {
        # Identity and provenance
        "seed": seed,
        "spec": spec.to_dict(),
        "git_commit": get_git_commit(),
        # Execution metadata
        "start_ts": time.time(),
        "runtime_s": None,
        "success": False,
        "error": None,
        # Core estimation results (per policy)
        "estimates": {},  # Policy -> estimate
        "standard_errors": {},  # Policy -> SE
        "confidence_intervals": {},  # Policy -> (lower, upper)
        "oracle_truths": {},  # Policy -> oracle ground truth
        # Weight diagnostics (per policy)
        "ess_absolute": {},  # Effective sample size (absolute)
        "ess_relative": {},  # ESS / n (percentage)
        "max_weight": {},  # Maximum single weight
        "tail_alpha": {},  # Hill estimator of tail index
        "weight_cv": {},  # Coefficient of variation
        # Calibration metrics
        "calibration_rmse": None,  # RMSE of judge→oracle calibration
        "simcal_distortion": {},  # Per-policy SIMCal distortion δ
        "rho_used": {},  # Actual ρ used per policy
        "blend_alpha": {},  # SIMCal blend parameter α
        # Oracle augmentation metrics
        "augmentation_share": {},  # Fraction of variance from augmentation
        "oracle_slice_size": None,  # Number of oracle labels used
        # DR-specific metrics
        "mc_variance_share": {},  # Monte Carlo share of variance
        "draws_per_prompt": None,  # K value used
        "policies_skipped": [],  # Policies that couldn't be estimated
        # Overall metrics
        "rmse_vs_oracle": None,  # RMSE across all policies
        "mean_ci_width": None,  # Average CI width
        "n_samples": None,  # Actual number of samples used
        "n_oracle": None,  # Actual number of oracle labels
    }


def aggregate_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Aggregate results across seeds.

    Args:
        results: List of result dictionaries from same spec

    Returns:
        Aggregated statistics
    """
    import numpy as np

    if not results:
        return {}

    # Extract successful results
    successful = [r for r in results if r.get("success", False)]
    if not successful:
        return {
            "n_seeds_total": len(results),
            "n_seeds_successful": 0,
            "all_failed": True,
            "errors": [r.get("error", "Unknown") for r in results],
        }

    # Aggregate estimates across seeds
    aggregated = {
        "spec": results[0]["spec"],
        "n_seeds_total": len(results),
        "n_seeds_successful": len(successful),
        "estimates_mean": {},
        "estimates_se": {},
        "estimates_ci": {},
        "oracle_truths": successful[0].get("oracle_truths", {}),
    }

    # Get all policies
    policies = set()
    for r in successful:
        if "estimates" in r:
            policies.update(r["estimates"].keys())

    # Aggregate each policy
    for policy in policies:
        estimates = []
        ses = []

        for r in successful:
            if policy in r.get("estimates", {}):
                est = r["estimates"][policy]
                if est == est:  # Not NaN
                    estimates.append(est)

                    if "standard_errors" in r and policy in r["standard_errors"]:
                        se = r["standard_errors"][policy]
                        if se == se:  # Not NaN
                            ses.append(se)

        if estimates:
            mean_est = np.mean(estimates)

            # Combined SE (within + between seed variance)
            if ses and len(ses) == len(estimates):
                within_var = np.mean([s**2 for s in ses])
                between_var = np.var(estimates, ddof=1) if len(estimates) > 1 else 0
                combined_se = np.sqrt(within_var + between_var)
            else:
                combined_se = (
                    np.std(estimates, ddof=1) / np.sqrt(len(estimates))
                    if len(estimates) > 1
                    else 0
                )

            aggregated["estimates_mean"][policy] = mean_est
            aggregated["estimates_se"][policy] = combined_se
            aggregated["estimates_ci"][policy] = (
                mean_est - 1.96 * combined_se,
                mean_est + 1.96 * combined_se,
            )

    # Add diagnostic aggregates
    aggregated["mean_ess"] = np.mean(
        [
            np.mean(list(r.get("ess_absolute", {}).values()))
            for r in successful
            if r.get("ess_absolute")
        ]
    )

    aggregated["policies_skipped"] = list(
        set().union(*[set(r.get("policies_skipped", [])) for r in successful])
    )

    return aggregated


=== ./cje/experiments/arena_10k_simplified/ablations/estimator_comparison.py ===

#!/usr/bin/env python3
"""Systematic estimator comparison ablation.

This ablation compares estimators systematically to show:
1. Impact of self-normalization (IPS vs SNIPS)
2. Impact of calibration (SNIPS vs Cal-IPS, DR vs Cal-DR)
3. Impact of stacking (individual DR vs stacked)

Estimators compared:
- IPS: Raw importance sampling (unnormalized)
- SNIPS: Self-normalized IPS (Hajek estimator)
- Cal-IPS: Calibrated IPS (SIMCal)
- DR-CPO: Basic doubly robust
- Cal-DR-CPO: Calibrated DR (uses Cal-IPS weights)
- Stacked-DR: Optimal combination of DR methods
- Cal-Stacked-DR: Calibrated stacked DR
"""

import json
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass

import sys

sys.path.append(str(Path(__file__).parent.parent))

from core import ExperimentSpec
from core.base import BaseAblation
from core.schemas import aggregate_results

# Import CJE components directly for custom configurations
sys.path.append(str(Path(__file__).parent.parent.parent.parent))
from cje import load_dataset_from_jsonl
from cje.calibration import calibrate_dataset
from cje.data.precomputed_sampler import PrecomputedSampler
from cje.estimators import CalibratedIPS
from cje.estimators.dr_base import DRCPOEstimator
from cje.estimators.stacking import StackedDREstimator
from cje.data.fresh_draws import load_fresh_draws_auto

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)


@dataclass
class EstimatorConfig:
    """Configuration for an estimator variant."""

    name: str
    display_name: str
    estimator_class: str
    use_calibration: bool
    weight_mode: str = "hajek"  # "raw" or "hajek"
    is_dr: bool = False
    is_stacked: bool = False


class EstimatorComparison(BaseAblation):
    """Comprehensive comparison of all CJE estimators across varying conditions.

    This ablation evaluates 20 estimator variants across 16 experimental scenarios
    to understand their relative strengths, weaknesses, and optimal use cases.

    Estimator Variants (20 total):
    - IPS family: RawIPS, SNIPS, CalibratedIPS
    - DR family: DRCPO, MRDR, TMLE, StackedDR (each with ±IIC, ±Calibration)
    - Each DR estimator has 4 variants: base, +IIC, Calibrated, Calibrated+IIC

    Experimental Grid (16 scenarios):
    - Sample sizes: 500, 1000, 2500, 5000
    - Oracle coverage: 5%, 10%, 25%, 100%
    - Creates a 4×4 grid testing data scarcity vs calibration quality

    Key Analyses:
    1. Method comparison: How estimators rank across scenarios
    2. Policy heterogeneity: Which methods work best for which policies
    3. Oracle efficiency: Performance vs oracle labeling cost
    4. Convergence rates: How methods scale with data

    Outputs:
    - Main comparison figure: 16-panel grid showing all scenarios
    - Policy heterogeneity heatmaps: Per-scenario analysis of method×policy performance
    - Statistical summaries: RMSE, SE, ESS, CI coverage vs oracle ground truth

    The results inform practitioners about:
    - When to use simple (IPS) vs complex (DR) methods
    - How much oracle data is really needed
    - Which policies benefit from which estimators
    - Whether added complexity (IIC, calibration) is worth it
    """

    def __init__(self) -> None:
        super().__init__(name="estimator_comparison")

        # Define estimators to compare
        self.estimator_configs = [
            EstimatorConfig("ips", "IPS", "ips", False, "raw"),
            EstimatorConfig("snips", "SNIPS", "ips", False, "hajek"),
            EstimatorConfig("cal-ips", "Cal-IPS", "calibrated-ips", True, "hajek"),
            EstimatorConfig("dr-cpo", "DR-CPO", "dr-cpo", False, "hajek", is_dr=True),
            EstimatorConfig(
                "cal-dr-cpo", "Cal-DR-CPO", "dr-cpo", True, "hajek", is_dr=True
            ),
            EstimatorConfig(
                "stacked-dr",
                "Stacked-DR",
                "stacked",
                False,
                "hajek",
                is_dr=True,
                is_stacked=True,
            ),
            EstimatorConfig(
                "cal-stacked-dr",
                "Cal-Stacked-DR",
                "stacked",
                True,
                "hajek",
                is_dr=True,
                is_stacked=True,
            ),
        ]

    def create_custom_estimator(
        self, config: EstimatorConfig, sampler: PrecomputedSampler, cal_result: Any
    ) -> Union[CalibratedIPS, DRCPOEstimator, StackedDREstimator]:
        """Create estimator with specific configuration."""

        if config.estimator_class == "ips":
            # Raw or self-normalized IPS
            return CalibratedIPS(sampler, calibrate=False)

        elif config.estimator_class == "calibrated-ips":
            # Calibrated IPS (always uses SIMCal)
            return CalibratedIPS(sampler, calibrate=True)

        elif config.estimator_class == "dr-cpo":
            # DR-CPO with or without calibrated weights
            estimator = DRCPOEstimator(
                sampler,
                calibrator=cal_result.calibrator if cal_result else None,
                n_folds=5,
                use_calibrated_weights=config.use_calibration,
            )
            return estimator

        elif config.estimator_class == "stacked":
            # Stacked DR with or without calibration
            # For non-calibrated Stacked-DR, we should NOT pass calibrator
            # For Cal-Stacked-DR, we pass calibrator and it will be used
            if config.use_calibration and cal_result:
                # Cal-Stacked-DR: pass calibrator for component estimators to use
                estimator = StackedDREstimator(
                    sampler,
                    estimators=["dr-cpo", "tmle", "mrdr"],
                    V_folds=5,
                    parallel=True,
                    calibrator=cal_result.calibrator,
                )
            else:
                # Stacked-DR: no calibrator, components will use raw weights
                estimator = StackedDREstimator(
                    sampler,
                    estimators=["dr-cpo", "tmle", "mrdr"],
                    V_folds=5,
                    parallel=True,
                    # Explicitly no calibrator - components will use raw weights
                )
            return estimator

        else:
            raise ValueError(f"Unknown estimator class: {config.estimator_class}")

    def run_single_comparison(
        self, spec: ExperimentSpec, config: EstimatorConfig, seed: int
    ) -> Dict[str, Any]:
        """Run a single estimator configuration."""

        # Initialize result
        result = {
            "spec": spec if isinstance(spec, dict) else spec.__dict__,
            "config": config.name,
            "display_name": config.display_name,
            "seed": seed,
            "success": False,
        }

        try:
            # Load and prepare data
            np.random.seed(seed)
            dataset_path = (
                spec["dataset_path"] if isinstance(spec, dict) else spec.dataset_path
            )
            dataset = load_dataset_from_jsonl(dataset_path)

            # Subsample if requested
            sample_size = (
                spec.get("sample_size") if isinstance(spec, dict) else spec.sample_size
            )
            if sample_size:
                n = min(sample_size, len(dataset.samples))
                indices = sorted(
                    np.random.choice(len(dataset.samples), n, replace=False)
                )
                dataset.samples = [dataset.samples[i] for i in indices]

            # Mask oracle labels for calibration
            n_samples = len(dataset.samples)
            oracle_coverage = (
                spec.get("oracle_coverage")
                if isinstance(spec, dict)
                else spec.oracle_coverage
            )
            if oracle_coverage and oracle_coverage < 1.0:
                oracle_indices = [
                    i
                    for i, s in enumerate(dataset.samples)
                    if s.metadata.get("oracle_label") is not None
                ]
                n_keep = max(2, int(len(oracle_indices) * oracle_coverage))
                keep_indices = set(
                    np.random.choice(oracle_indices, n_keep, replace=False)
                )

                for i, sample in enumerate(dataset.samples):
                    if i not in keep_indices and "oracle_label" in sample.metadata:
                        sample.metadata["oracle_label"] = None

            # Calibrate dataset
            calibrated_dataset, cal_result = calibrate_dataset(
                dataset,
                judge_field="judge_score",
                oracle_field="oracle_label",
                enable_cross_fit=True,
                n_folds=5,
            )

            # Create sampler with appropriate weight mode
            sampler = PrecomputedSampler(calibrated_dataset)

            # Override weight computation for IPS vs SNIPS
            if config.weight_mode == "raw" and config.estimator_class == "ips":
                # Monkey-patch to use raw weights instead of Hajek
                # Note: CalibratedIPS.get_raw_weights already passes mode="raw"
                # So we need to intercept and handle that case
                original_method = sampler.compute_importance_weights

                def raw_weights(policy: str, **kwargs: Any) -> Any:
                    # Remove mode from kwargs to avoid duplicate since we'll set it
                    kwargs.pop("mode", None)
                    # Always use raw mode for IPS
                    return original_method(policy, mode="raw", **kwargs)

                sampler.compute_importance_weights = raw_weights  # type: ignore[assignment]

            # Create and run estimator
            estimator = self.create_custom_estimator(config, sampler, cal_result)

            # Add fresh draws for DR methods
            if config.is_dr:
                data_dir = Path(dataset_path).parent
                for policy in sampler.target_policies:
                    try:
                        fresh_draws = load_fresh_draws_auto(
                            data_dir, policy, verbose=False
                        )
                        estimator.add_fresh_draws(policy, fresh_draws)
                    except:
                        pass

            # Run estimation
            import time

            start_time = time.time()
            estimation_result = estimator.fit_and_estimate()
            runtime = time.time() - start_time

            # Extract results
            result["estimates"] = {
                policy: float(estimation_result.estimates[i])
                for i, policy in enumerate(sampler.target_policies)
            }
            result["standard_errors"] = {
                policy: float(estimation_result.standard_errors[i])
                for i, policy in enumerate(sampler.target_policies)
            }
            result["runtime"] = runtime
            result["n_samples"] = n_samples
            result["success"] = True

            # Add estimator-specific diagnostics
            if hasattr(estimator, "weights_per_policy") and config.is_stacked:
                # Stacking weights
                result["stacking_weights"] = {
                    policy: weights.tolist()
                    for policy, weights in estimator.weights_per_policy.items()
                }

            # Compute ESS for all methods (IPS and DR both use importance weights)
            ess_values = {}
            for policy in sampler.target_policies:
                try:
                    weights = None

                    # Try to get the actual weights used by the estimator
                    # For IPS-based estimators, get weights directly
                    if hasattr(estimator, "get_weights"):
                        weights = estimator.get_weights(policy)
                    
                    # For DR estimators, get weights from their internal IPS estimator
                    if weights is None and hasattr(estimator, "ips_estimator"):
                        if hasattr(estimator.ips_estimator, "get_weights"):
                            weights = estimator.ips_estimator.get_weights(policy)
                    
                    # For stacked estimators, try to get from component
                    if weights is None and hasattr(estimator, "_component_estimators"):
                        for comp_est in estimator._component_estimators:
                            if hasattr(comp_est, "get_weights"):
                                comp_weights = comp_est.get_weights(policy)
                                if comp_weights is not None:
                                    weights = comp_weights
                                    break
                            elif hasattr(comp_est, "ips_estimator") and hasattr(
                                comp_est.ips_estimator, "get_weights"
                            ):
                                comp_weights = comp_est.ips_estimator.get_weights(policy)
                                if comp_weights is not None:
                                    weights = comp_weights
                                    break

                    # IMPORTANT: Do NOT fall back to raw weights for calibrated methods
                    # If we can't get the calibrated weights, ESS should be NaN
                    # This prevents misleading ESS values that show baseline overlap
                    # instead of post-calibration effective sample size
                    
                    if weights is not None:
                        ess = np.sum(weights) ** 2 / np.sum(weights**2)
                        ess_values[policy] = float(ess)
                    else:
                        # For calibrated methods without accessible weights, set NaN
                        if "Cal-" in config.display_name:
                            ess_values[policy] = np.nan
                        else:
                            # For non-calibrated methods, raw weights are appropriate
                            weights = sampler.compute_importance_weights(policy)
                            if weights is not None:
                                ess = np.sum(weights) ** 2 / np.sum(weights**2)
                                ess_values[policy] = float(ess)
                except Exception as e:
                    # Log the error for debugging but continue
                    logger.debug(f"ESS calculation failed for {policy}: {e}")
                    pass
            result["ess"] = ess_values

        except Exception as e:
            result["error"] = str(e)
            logger.warning(f"Failed {config.name}: {e}")

        return result

    def create_all_scenario_plots(self, results: List[Dict[str, Any]], color_by: str = "se") -> None:
        """Generate separate policy heterogeneity plots for each scenario.
        
        Args:
            results: All estimator comparison results
            color_by: "se" for standard error or "error" for absolute error
        """
        # Extract unique scenarios from successful results
        scenarios = sorted({
            r.get("scenario") 
            for r in results 
            if r.get("success", False) and r.get("scenario")
        })
        
        if not scenarios:
            logger.warning("No scenarios found in results")
            return
            
        logger.info(f"\nGenerating {len(scenarios)} scenario plots...")
        
        for scenario in scenarios:
            logger.info(f"\nProcessing scenario: {scenario}")
            
            # Filter results for this scenario
            scenario_results = [
                r for r in results 
                if r.get("scenario") == scenario
            ]
            
            # Generate filename from scenario
            # Convert "n=1000, oracle=10%" to "n1000_oracle10pct"
            safe_scenario = scenario.replace("=", "").replace(", ", "_").replace("%", "pct")
            output_path = Path(f"results/estimator_comparison/policy_heterogeneity_{safe_scenario}_{color_by}.png")
            
            # Create figure for this scenario
            self.create_policy_heterogeneity_figure(
                scenario_results,
                output_path=output_path,
                scenario_label=scenario,
                color_by=color_by
            )
        
        logger.info(f"\nGenerated {len(scenarios)} policy heterogeneity plots")

    def run_ablation(self) -> List[Dict[str, Any]]:
        """Run systematic comparison across all estimators."""

        # Test scenarios: sample size x oracle coverage
        sample_sizes = [500, 1000, 2500, 5000]
        oracle_coverages = [0.05, 0.10, 0.25, 1.00]

        scenarios = []
        for n in sample_sizes:
            for oracle in oracle_coverages:
                scenarios.append(
                    {"n": n, "oracle": oracle, "label": f"n={n}, oracle={oracle:.0%}"}
                )

        n_seeds = 3  # Multiple seeds for stability

        logger.info("=" * 70)
        logger.info("ESTIMATOR COMPARISON ABLATION")
        logger.info("=" * 70)
        logger.info(f"Estimators: {[c.display_name for c in self.estimator_configs]}")
        logger.info(f"Scenarios: {len(scenarios)}")
        logger.info(f"Seeds: {n_seeds}")
        logger.info("")

        all_results = []

        for scenario in scenarios:
            logger.info(f"\n{'='*60}")
            logger.info(
                f"Scenario: {scenario['label']} (n={scenario['n']}, oracle={scenario['oracle']:.0%})"
            )
            logger.info(f"{'='*60}")

            for config in self.estimator_configs:
                logger.info(f"\n{config.display_name}:")

                # Determine correct data path - use absolute path to be safe
                base_dir = Path(__file__).parent.parent
                data_path = base_dir / "data" / "cje_dataset.jsonl"
                if not data_path.exists():
                    # Fallback to relative path from current directory
                    data_path = Path("../data/cje_dataset.jsonl").resolve()

                # Create spec with n_seeds instead of individual seed
                spec = ExperimentSpec(
                    ablation="estimator_comparison",
                    dataset_path=str(data_path),
                    estimator=config.name,
                    oracle_coverage=scenario["oracle"],
                    sample_size=scenario["n"],
                    n_seeds=n_seeds,
                    seed_base=42,
                )

                # Run with multiple seeds using the base class method
                for seed_offset in range(n_seeds):
                    seed = 42 + seed_offset

                    result = self.run_single_comparison(spec, config, seed)
                    result["scenario"] = scenario["label"]
                    all_results.append(result)

                    if result["success"]:
                        # Show mean estimate and SE
                        mean_est = np.mean(list(result["estimates"].values()))
                        mean_se = np.mean(list(result["standard_errors"].values()))
                        logger.info(
                            f"  Seed {seed}: {mean_est:.3f} ± {mean_se:.3f} ({result['runtime']:.1f}s)"
                        )
                    else:
                        logger.info(f"  Seed {seed}: FAILED")

        # Save all results
        output_dir = Path("results/estimator_comparison")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Convert numpy types for JSON serialization
        def convert_numpy(obj):
            """Convert numpy types to Python types for JSON."""
            import numpy as np

            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.bool_, np.bool8)):
                return bool(obj)
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(v) for v in obj]
            elif hasattr(obj, "item"):
                return obj.item()
            return obj

        with open(output_dir / "results.jsonl", "w") as f:
            for result in all_results:
                f.write(json.dumps(convert_numpy(result)) + "\n")

        logger.info(f"\nSaved {len(all_results)} results to {output_dir}")

        return all_results

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze comparison results."""

        # Load oracle means for RMSE calculation
        oracle_means = self.load_oracle_means()

        # Group by scenario and estimator
        by_scenario: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}
        for r in results:
            if r.get("success", False):
                scenario = r["scenario"]
                estimator = r["display_name"]

                if scenario not in by_scenario:
                    by_scenario[scenario] = {}
                if estimator not in by_scenario[scenario]:
                    by_scenario[scenario][estimator] = []

                # Compute RMSE across all policies
                squared_errors = []
                for policy, estimate in r["estimates"].items():
                    
                    oracle_truth = oracle_means.get(policy, np.nan)
                    if not np.isnan(oracle_truth) and not np.isnan(estimate):
                        squared_error = (estimate - oracle_truth) ** 2
                        squared_errors.append(squared_error)
                
                # Calculate RMSE if we have any valid squared errors
                if squared_errors:
                    rmse = np.sqrt(np.mean(squared_errors))
                else:
                    # Fallback to SE if no oracle truth available
                    se_values = [
                        r["standard_errors"][p] 
                        for p in r["standard_errors"] 
                        if not np.isnan(r["standard_errors"][p])
                    ]
                    rmse = np.mean(se_values) if se_values else np.nan
                
                by_scenario[scenario][estimator].append(
                    {
                        "rmse": rmse,
                        "runtime": r["runtime"],
                        "ess": (
                            np.mean([
                                r.get("ess", {}).get(p, np.nan) 
                                for p in r.get("ess", {}) 
                                if p != "unhelpful"
                            ])
                            if "ess" in r
                            else None
                        ),
                    }
                )

        # Compute rankings
        rankings = {}
        for scenario, estimators in by_scenario.items():
            scenario_rankings = []

            for est_name, runs in estimators.items():
                if runs:
                    mean_rmse = np.mean([r["rmse"] for r in runs if not np.isnan(r["rmse"])])
                    mean_runtime = np.mean([r["runtime"] for r in runs])
                    mean_ess = np.mean([r["ess"] for r in runs if r["ess"] is not None])

                    scenario_rankings.append(
                        {
                            "estimator": est_name,
                            "mean_rmse": mean_rmse,
                            "mean_runtime": mean_runtime,
                            "mean_ess": mean_ess if not np.isnan(mean_ess) else None,
                        }
                    )

            # Sort by RMSE (lower is better)
            scenario_rankings.sort(key=lambda x: x["mean_rmse"])
            rankings[scenario] = scenario_rankings

        return rankings

    def create_figure(
        self, results: List[Dict[str, Any]], output_path: Optional[Path] = None
    ) -> Any:
        """Create comparison figure."""

        rankings = self.analyze_results(results)

        if not rankings:
            logger.warning("No results to plot")
            return

        # Create figure with dynamic grid to fit all scenarios
        n_scenarios = len(rankings)
        n_cols = 4  # 4 columns for better aspect ratio
        n_rows = (n_scenarios + n_cols - 1) // n_cols  # Ceiling division

        fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4 * n_rows))

        # Flatten axes and handle case where we might have fewer scenarios than subplots
        if n_rows == 1 and n_cols == 1:
            axes = [axes]
        elif n_rows == 1 or n_cols == 1:
            axes = axes.flatten()
        else:
            axes = axes.flatten()

        for idx, (scenario, ranking) in enumerate(rankings.items()):

            ax = axes[idx]

            # Extract data
            estimators = [r["estimator"] for r in ranking]
            rmses = [r["mean_rmse"] for r in ranking]

            # Color by type
            colors = []
            for est in estimators:
                if "Cal-" in est:
                    colors.append("green")  # Calibrated
                elif "Stacked" in est:
                    colors.append("purple")  # Stacked
                elif "DR" in est:
                    colors.append("orange")  # DR
                else:
                    colors.append("blue")  # IPS variants

            # Create bar chart
            bars = ax.barh(range(len(estimators)), rmses, color=colors)
            ax.set_yticks(range(len(estimators)))
            ax.set_yticklabels(estimators)
            ax.set_xlabel("RMSE (log scale)")
            ax.set_xscale('log')  # Set log scale for x-axis
            ax.set_title(scenario)
            ax.invert_yaxis()  # Best at top

            # Add values on bars
            for i, (bar, rmse) in enumerate(zip(bars, rmses)):
                ax.text(rmse, i, f" {rmse:.4f}", va="center")

        # Hide unused subplots
        for idx in range(n_scenarios, len(axes)):
            axes[idx].set_visible(False)

        plt.suptitle(
            "Estimator Comparison: RMSE by Scenario",
            fontsize=16,
            fontweight="bold",
            y=0.98  # Move title up slightly
        )
        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space at top for title

        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(output_path, dpi=150, bbox_inches="tight")
            logger.info(f"Saved figure to {output_path}")

        # Don't show interactively - let caller decide
        # plt.show()

        return fig

    def load_oracle_means(self) -> Dict[str, float]:
        """Load oracle truth means for each policy from response files.

        Returns:
            Dictionary mapping policy name to oracle mean
        """
        import json
        from pathlib import Path

        oracle_means = {}

        # Policy name mapping from file names
        response_files = {
            "clone": "../data/responses/clone_responses.jsonl",
            "parallel_universe_prompt": "../data/responses/parallel_universe_prompt_responses.jsonl",
            "premium": "../data/responses/premium_responses.jsonl",
            "unhelpful": "../data/responses/unhelpful_responses.jsonl",
        }

        for policy, file_path in response_files.items():
            file_path = Path(file_path)
            if file_path.exists():
                oracle_values = []
                try:
                    with open(file_path, "r") as f:
                        for line in f:
                            data = json.loads(line)
                            oracle_label = data.get("metadata", {}).get("oracle_label")
                            if oracle_label is not None:
                                oracle_values.append(oracle_label)

                    if oracle_values:
                        oracle_means[policy] = np.mean(oracle_values)
                        logger.info(
                            f"Loaded {len(oracle_values)} oracle values for {policy}, mean: {oracle_means[policy]:.3f}"
                        )
                    else:
                        logger.warning(f"No oracle values found in {file_path}")
                except Exception as e:
                    logger.warning(f"Error reading {file_path}: {e}")
            else:
                logger.warning(f"Response file not found: {file_path}")

        return oracle_means

    def analyze_by_policy(self, results: List[Dict[str, Any]], scenario: Optional[str] = None) -> "pd.DataFrame":
        """Extract per-policy performance metrics from estimator comparison results.

        Creates a DataFrame with one row per (method, policy) combination showing:
        - Standard error
        - ESS (effective sample size)
        - Estimates and oracle truth values if available

        Args:
            results: List of estimator comparison results
            scenario: Optional scenario filter (e.g., "n=1000, oracle=10%")

        Returns:
            DataFrame with columns: Method, Policy, Estimate, SE, ESS_%, Oracle_Truth, Error
        """
        import pandas as pd
        
        # Filter by scenario if specified
        if scenario is not None:
            results = [r for r in results if r.get("scenario") == scenario]

        # Load oracle truth means
        oracle_means = self.load_oracle_means()

        rows = []
        for r in results:
            if r.get("success", False) and "estimates" in r:
                method = r.get("display_name", r.get("config", "unknown"))

                for policy in r.get("estimates", {}).keys():
                    # Get absolute ESS and calculate percentage
                    ess_abs = r.get("ess", {}).get(policy, np.nan)
                    n_samples = r.get("n_samples", 1000)  # Default fallback
                    ess_pct = (
                        (ess_abs / n_samples * 100) if not np.isnan(ess_abs) else np.nan
                    )

                    # Get oracle truth from loaded means
                    oracle_truth = oracle_means.get(policy, np.nan)
                    estimate = r["estimates"].get(policy, np.nan)

                    row = {
                        "Method": method,
                        "Policy": policy,
                        "Estimate": estimate,
                        "SE": r.get("standard_errors", {}).get(policy, np.nan),
                        "ESS_abs": ess_abs,
                        "ESS_%": ess_pct,
                        "Oracle_Truth": oracle_truth,
                    }

                    # Calculate absolute error if both values available
                    if not np.isnan(oracle_truth) and not np.isnan(estimate):
                        row["Abs_Error"] = abs(estimate - oracle_truth)
                    else:
                        row["Abs_Error"] = np.nan

                    rows.append(row)

        return pd.DataFrame(rows)

    def create_policy_heterogeneity_figure(
        self,
        results: List[Dict[str, Any]],
        output_path: Optional[Path] = None,
        scenario_label: Optional[str] = None,
        color_by: str = "se",
    ) -> Any:
        """Create heatmap showing SE or Abs Error by (method × policy) to demonstrate heterogeneity.

        Shows how different policies require different estimation methods based on
        their distribution shift from the base policy. Can color by either standard
        error or absolute error.

        Args:
            results: List of estimator comparison results
            output_path: Optional path to save figure
            scenario_label: Optional label describing the scenario (filters results)
            color_by: "se" for standard error (default) or "error" for absolute error

        Returns:
            matplotlib figure
        """
        import pandas as pd

        # Use scenario_label to filter data if provided
        df = self.analyze_by_policy(results, scenario=scenario_label)

        if df.empty:
            logger.warning("No policy-specific data to visualize")
            return None

        # Create matrices for different metrics
        se_matrix = df.pivot_table(
            index="Method", columns="Policy", values="SE", aggfunc="mean"
        )

        ess_matrix = df.pivot_table(
            index="Method", columns="Policy", values="ESS_%", aggfunc="mean"
        )

        # Get absolute error matrix
        abs_error_matrix = df.pivot_table(
            index="Method", columns="Policy", values="Abs_Error", aggfunc="mean"
        )

        # Reorder methods to the desired order
        method_order = [
            "IPS",
            "SNIPS",
            "Cal-IPS",
            "DR-CPO",
            "Cal-DR-CPO",
            "Stacked-DR",
            "Cal-Stacked-DR",
        ]
        # Keep only methods that exist in the data
        existing_methods = [m for m in method_order if m in se_matrix.index]

        # Reindex all matrices with the desired order
        se_matrix = se_matrix.reindex(existing_methods)
        ess_matrix = ess_matrix.reindex(existing_methods)
        abs_error_matrix = abs_error_matrix.reindex(existing_methods)

        # Create figure
        fig, ax = plt.subplots(figsize=(12, 8))

        # Choose the matrix and scale based on color_by parameter
        if color_by.lower() == "error":
            # Use absolute error for coloring
            color_matrix = abs_error_matrix
            color_vals = color_matrix.values
            color_vals_clean = np.where(
                np.isnan(color_vals) | (color_vals < 0), 0, color_vals
            )  # Replace NaN/negative with 0

            # Adaptive scale based on data, with reasonable maximum
            vmin = 0.0
            # Use 95th percentile or 0.05, whichever is larger (but cap at 0.1)
            if color_vals_clean.size > 0:
                percentile_95 = (
                    np.percentile(color_vals_clean[color_vals_clean > 0], 95)
                    if np.any(color_vals_clean > 0)
                    else 0.05
                )
                vmax = min(0.1, max(0.05, percentile_95))
            else:
                vmax = 0.05

            color_label = "Absolute Error"
            logger.info(
                f"Color scale: Absolute Error from {vmin} to {vmax:.3f} (adaptive)"
            )
        else:
            # Default: use standard error for coloring
            color_matrix = se_matrix
            color_vals = color_matrix.values
            color_vals_clean = np.where(
                np.isnan(color_vals) | (color_vals <= 0), 0, color_vals
            )  # Replace NaN/negative with 0

            # Adaptive scale based on data
            vmin = 0.0
            # Use 95th percentile or 0.1, whichever is smaller (but at least 0.01)
            if color_vals_clean.size > 0:
                percentile_95 = (
                    np.percentile(color_vals_clean[color_vals_clean > 0], 95)
                    if np.any(color_vals_clean > 0)
                    else 0.1
                )
                vmax = max(0.01, min(0.2, percentile_95))
            else:
                vmax = 0.1

            color_label = "Standard Error"
            logger.info(
                f"Color scale: Standard Error from {vmin} to {vmax:.3f} (adaptive)"
            )

        # Clip values to the fixed range
        vals_for_color = np.clip(color_vals_clean, vmin, vmax)

        # Create heatmap with viridis colormap (colorblind-friendly)
        im = ax.imshow(
            vals_for_color, cmap="viridis", aspect="auto", vmin=vmin, vmax=vmax
        )

        # Set ticks and labels
        ax.set_xticks(np.arange(len(se_matrix.columns)))
        ax.set_yticks(np.arange(len(se_matrix.index)))
        ax.set_xticklabels(se_matrix.columns, rotation=45, ha="right")
        ax.set_yticklabels(se_matrix.index)

        # Add colorbar with fixed linear scale
        cbar = plt.colorbar(im, ax=ax)
        if color_by.lower() == "error":
            cbar.set_label(f"{color_label} (0.0 to {vmax})", rotation=270, labelpad=20)
        else:
            cbar.set_label(f"{color_label} (0.0 to {vmax})", rotation=270, labelpad=20)

        # Add text annotations showing SE and ESS%
        for i in range(len(se_matrix.index)):
            for j in range(len(se_matrix.columns)):
                method = se_matrix.index[i]
                policy = se_matrix.columns[j]

                se_val = se_matrix.loc[method, policy]

                # Safely get ESS and error values using loc to avoid index errors
                ess_pct_val = np.nan
                abs_error_val = np.nan

                if (
                    not ess_matrix.empty
                    and method in ess_matrix.index
                    and policy in ess_matrix.columns
                ):
                    ess_pct_val = ess_matrix.loc[method, policy]

                if (
                    not abs_error_matrix.empty
                    and method in abs_error_matrix.index
                    and policy in abs_error_matrix.columns
                ):
                    abs_error_val = abs_error_matrix.loc[method, policy]

                if not np.isnan(se_val):
                    # Handle zero/near-zero SEs (likely numerical precision issues)
                    if se_val < 1e-6:
                        se_display = "<0.001"
                    else:
                        se_display = f"{se_val:.3f}"

                    # Build annotation text with SE, MDE, ESS%, and absolute error
                    text_lines = [f"SE: {se_display}"]
                    
                    # Add MDE for two-policy comparison (3.96 * SE)
                    if se_val >= 1e-6:
                        mde_two = 3.96 * se_val
                        if mde_two < 0.01:
                            text_lines.append(f"MDE: <1%")
                        elif mde_two < 0.02:
                            text_lines.append(f"MDE: {mde_two:.1%}")
                        else:
                            text_lines.append(f"MDE: {mde_two:.1%}")

                    if not np.isnan(ess_pct_val):
                        # Add warning marker if ESS is very low
                        if ess_pct_val < 10:
                            text_lines.append(f"ESS: {ess_pct_val:.1f}% ⚠")
                        else:
                            text_lines.append(f"ESS: {ess_pct_val:.1f}%")

                    if not np.isnan(abs_error_val):
                        text_lines.append(f"Err: {abs_error_val:.3f}")

                    text = "\n".join(text_lines)

                    # Choose text color based on background brightness
                    # Get the actual value used for coloring
                    if color_by.lower() == "error":
                        color_val = abs_error_val if not np.isnan(abs_error_val) else 0
                    else:
                        color_val = se_val

                    # For viridis colormap: dark (low values) need white text, bright (high values) need black
                    # Use 40% threshold - values below 40% of range are dark enough for white text
                    threshold = vmin + 0.4 * (vmax - vmin)
                    color = "white" if color_val < threshold else "black"

                    ax.text(
                        j,
                        i,
                        text,
                        ha="center",
                        va="center",
                        color=color,
                        fontsize=8,
                        fontweight="bold",
                    )

        # Extract scenario info from results if not provided
        if scenario_label is None and results:
            # Try to extract from first result
            first_result = results[0]
            if "spec" in first_result:
                sample_size = first_result["spec"].get("sample_size", "unknown")
                oracle_coverage = first_result["spec"].get("oracle_coverage", "unknown")
                if oracle_coverage != "unknown":
                    oracle_pct = (
                        f"{oracle_coverage:.0%}"
                        if oracle_coverage >= 1
                        else f"{oracle_coverage*100:.0f}%"
                    )
                    scenario_label = f"n={sample_size}, oracle={oracle_pct}"

        if scenario_label:
            metric_name = (
                "Absolute Error" if color_by.lower() == "error" else "Standard Error"
            )
            title = f"Policy Heterogeneity ({metric_name}): {scenario_label}"
        else:
            metric_name = (
                "Absolute Error" if color_by.lower() == "error" else "Standard Error"
            )
            title = f"Policy Heterogeneity: {metric_name} by Method and Policy"

        ax.set_title(title, fontsize=12, fontweight="bold")
        ax.set_xlabel("Target Policy", fontsize=12)
        ax.set_ylabel("Estimation Method", fontsize=12)

        plt.tight_layout()

        # Generate default output path if not provided
        if output_path is None:
            # Use consistent naming: _by_se for standard error, _by_abs_error for absolute error
            suffix = "_by_abs_error" if color_by.lower() == "error" else "_by_se"
            if results and "spec" in results[0]:
                sample_size = results[0]["spec"].get("sample_size", "unknown")
                oracle_coverage = results[0]["spec"].get("oracle_coverage", "unknown")
                if oracle_coverage != "unknown":
                    oracle_pct = (
                        int(oracle_coverage * 100)
                        if oracle_coverage < 1
                        else int(oracle_coverage)
                    )
                    output_path = Path(
                        f"results/estimator_comparison/policy_heterogeneity_n{sample_size}_oracle{oracle_pct}pct{suffix}.png"
                    )
                else:
                    output_path = Path(
                        f"results/estimator_comparison/policy_heterogeneity{suffix}.png"
                    )
            else:
                output_path = Path(
                    f"results/estimator_comparison/policy_heterogeneity{suffix}.png"
                )

        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(output_path, dpi=150, bbox_inches="tight")
            logger.info(f"Saved policy heterogeneity figure to {output_path}")

        # Print insights
        metric_name = (
            "Absolute Error" if color_by.lower() == "error" else "Standard Error"
        )
        metric_abbrev = "Err" if color_by.lower() == "error" else "SE"
        logger.info(f"\nPolicy Heterogeneity Insights ({metric_name}):")
        logger.info("-" * 40)

        # Find best method per policy based on the selected metric
        metric_matrix = abs_error_matrix if color_by.lower() == "error" else se_matrix
        for policy in metric_matrix.columns:
            policy_vals = metric_matrix[policy].dropna()
            if not policy_vals.empty:
                best_method = policy_vals.idxmin()
                best_val = policy_vals.min()
                worst_val = policy_vals.max()
                improvement = (
                    (worst_val - best_val) / worst_val * 100 if worst_val > 0 else 0
                )
                logger.info(f"{policy}:")
                logger.info(
                    f"  Best method: {best_method} ({metric_abbrev}={best_val:.3f})"
                )
                logger.info(f"  Improvement over worst: {improvement:.0f}%")

        return fig


def main() -> List[Dict[str, Any]]:
    """Run estimator comparison."""

    comparison = EstimatorComparison()
    results = comparison.run_ablation()

    # Analyze
    rankings = comparison.analyze_results(results)

    logger.info("\n" + "=" * 70)
    logger.info("ANALYSIS SUMMARY")
    logger.info("=" * 70)

    for scenario, ranking in rankings.items():
        logger.info(f"\n{scenario}:")
        logger.info("-" * 40)
        for i, r in enumerate(ranking, 1):
            logger.info(
                f"{i}. {r['estimator']:15s}: SE={r['mean_se']:.4f}, Runtime={r['mean_runtime']:.1f}s"
            )

    # Create figures
    figure_path = Path("results/estimator_comparison/comparison_figure.png")
    comparison.create_figure(results, figure_path)

    # Create policy heterogeneity analysis - separate plots per scenario
    logger.info("\nGenerating policy heterogeneity plots per scenario...")
    comparison.create_all_scenario_plots(results, color_by="se")
    # Also generate error-colored version for comparison
    comparison.create_all_scenario_plots(results, color_by="error")

    logger.info("\n" + "=" * 70)
    logger.info("ESTIMATOR COMPARISON COMPLETE")
    logger.info("=" * 70)

    return results


if __name__ == "__main__":
    results = main()


=== ./cje/experiments/arena_10k_simplified/ablations/fresh_draws.py ===

#!/usr/bin/env python3
"""
Fresh draws (K) ablation for DR estimators.

Tests how many fresh draws per prompt improve DR estimates.
More draws reduce Monte Carlo variance but increase compute cost.
"""

import logging
from pathlib import Path
from typing import Dict, List, Any
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from .core.base import BaseAblation
from .core.schemas import ExperimentSpec

logger = logging.getLogger(__name__)


class FreshDrawsAblation(BaseAblation):
    """Ablation for number of fresh draws in DR estimation."""

    def __init__(self) -> None:
        super().__init__("fresh_draws")

    def run_ablation(self) -> List[Dict[str, Any]]:
        """Test different numbers of fresh draws."""
        results = []

        # Test different K values
        k_values = [1, 3, 5, 10, 20]

        for k in k_values:
            for estimator in ["dr-cpo", "tmle"]:
                spec = ExperimentSpec(
                    ablation="fresh_draws",
                    dataset_path="../../data/cje_dataset.jsonl",
                    estimator=estimator,
                    oracle_coverage=0.2,
                    n_seeds=3,  # Fewer seeds since this is expensive
                    draws_per_prompt=k,
                )

                logger.info(f"Running {estimator} with K={k} draws")
                seed_results = self.run_with_seeds(spec)
                results.extend(seed_results)

        return results

    def analyze_results(self, results: List[Dict[str, Any]]) -> None:
        """Analyze fresh draws sweep."""
        output_dir = Path("ablations/results/fresh_draws")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Collect data
        draw_data = []
        for r in results:
            if r.get("success"):
                k = r["spec"]["draws_per_prompt"]
                estimator = r["spec"]["estimator"]

                # Extract metrics
                for policy in r.get("estimates", {}).keys():
                    se = r.get("standard_errors", {}).get(policy, 0)

                    draw_data.append(
                        {
                            "k": k,
                            "estimator": estimator,
                            "policy": policy,
                            "se": se,
                            "runtime": r.get("runtime_s", 0),
                        }
                    )

        if not draw_data:
            logger.warning("No successful results")
            return

        df = pd.DataFrame(draw_data)

        # Plot: 1x3 grid
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        # Panel A: SE vs K
        ax = axes[0]
        for estimator in df["estimator"].unique():
            est_df = df[df["estimator"] == estimator]
            grouped = est_df.groupby("k")["se"].agg(["mean", "std"])
            ax.errorbar(
                grouped.index,
                grouped["mean"],
                yerr=grouped["std"],
                marker="o",
                label=estimator,
                capsize=5,
            )

        ax.set_xlabel("K (Fresh Draws per Prompt)")
        ax.set_ylabel("Standard Error")
        ax.set_title("(A) SE vs Fresh Draws")
        ax.set_xscale("log")
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Panel B: Runtime vs K
        ax = axes[1]
        runtime_grouped = df.groupby("k")["runtime"].agg(["mean", "std"])
        ax.errorbar(
            runtime_grouped.index,
            runtime_grouped["mean"],
            yerr=runtime_grouped["std"],
            marker="s",
            color="orange",
            capsize=5,
        )
        ax.set_xlabel("K (Fresh Draws per Prompt)")
        ax.set_ylabel("Runtime (seconds)")
        ax.set_title("(B) Computational Cost")
        ax.set_xscale("log")
        ax.grid(True, alpha=0.3)

        # Panel C: Efficiency (SE reduction per unit time)
        ax = axes[2]
        # Compute efficiency: 1/SE normalized by runtime
        efficiency_data = []
        for k in df["k"].unique():
            k_df = df[df["k"] == k]
            mean_se = k_df["se"].mean()
            mean_runtime = k_df["runtime"].mean()
            efficiency = (1 / mean_se) / mean_runtime if mean_runtime > 0 else 0
            efficiency_data.append({"k": k, "efficiency": efficiency})

        eff_df = pd.DataFrame(efficiency_data)
        ax.plot(eff_df["k"], eff_df["efficiency"], "o-", color="green")
        ax.set_xlabel("K (Fresh Draws per Prompt)")
        ax.set_ylabel("Efficiency (1/SE per second)")
        ax.set_title("(C) Efficiency: Precision per Unit Time")
        ax.set_xscale("log")
        ax.grid(True, alpha=0.3)

        # Mark optimal K
        optimal_k = eff_df.loc[eff_df["efficiency"].idxmax(), "k"]
        ax.axvline(x=optimal_k, color="red", linestyle="--", alpha=0.5)
        ax.text(
            optimal_k * 1.2,
            ax.get_ylim()[1] * 0.9,
            f"Optimal K≈{optimal_k}",
            color="red",
        )

        plt.suptitle("Fresh Draws (K) Ablation for DR Estimators", fontsize=14, y=1.02)
        plt.tight_layout()
        plt.savefig(output_dir / "fresh_draws_ablation.png", dpi=150)
        plt.close()

        # Summary table
        summary = (
            df.groupby(["k", "estimator"])
            .agg({"se": ["mean", "std"], "runtime": "mean"})
            .round(4)
        )

        print("\n" + "=" * 60)
        print("FRESH DRAWS ABLATION SUMMARY")
        print("=" * 60)
        print(summary)
        print(f"\nOptimal K (best efficiency): {optimal_k}")
        print("Recommendation: K=5-10 balances precision and compute cost")

        logger.info(f"Results saved to {output_dir}")


def main() -> None:
    """Run fresh draws ablation."""
    ablation = FreshDrawsAblation()
    results = ablation.run_ablation()
    ablation.analyze_results(results)


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/ablations/iic_effect_minimal.py ===

#!/usr/bin/env python3
"""
Minimal IIC ablation that leverages existing diagnostics.

Since IIC diagnostics are already computed and stored in metadata,
we just need to extract and visualize them properly.
"""

import logging
from pathlib import Path
from typing import Dict, List, Any
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from core.base import BaseAblation
from core.schemas import ExperimentSpec

logger = logging.getLogger(__name__)


class IICEffectMinimal(BaseAblation):
    """Minimal IIC ablation using existing diagnostics."""

    def __init__(self) -> None:
        super().__init__("iic_effect_minimal")

    def run_ablation(self) -> List[Dict[str, Any]]:
        """Run experiments with IIC enabled (default) to collect diagnostics."""
        results = []

        # IIC is enabled by default in all DR estimators
        # We just need to run them and extract the diagnostics

        for estimator in ["calibrated-ips", "dr-cpo", "tmle"]:
            for oracle_coverage in [0.1, 0.2, 0.5]:
                spec = ExperimentSpec(
                    ablation="iic_effect",
                    dataset_path="../data/cje_dataset.jsonl",
                    estimator=estimator,
                    oracle_coverage=oracle_coverage,
                    n_seeds=5,
                )

                logger.info(f"Running {estimator} (oracle={oracle_coverage*100:.0f}%)")
                seed_results = self.run_with_seeds(spec)

                # Extract IIC diagnostics from metadata
                for result in seed_results:
                    if result.get("success") and "estimates" in result:
                        # Check if IIC diagnostics are in metadata
                        # They should be at result["metadata"]["iic_diagnostics"]
                        result["iic_enabled"] = True
                        result["iic_diagnostics_found"] = False

                        # The diagnostics might be in different places depending on estimator
                        # Let's be flexible in extraction
                        if "iic_diagnostics" in result.get("metadata", {}):
                            result["iic_diagnostics_found"] = True

                results.extend(seed_results)

        return results

    def analyze_results(self, results: List[Dict[str, Any]]) -> None:
        """Extract and analyze IIC diagnostics from results."""

        output_dir = Path("results/iic_effect")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Collect IIC metrics
        iic_data = []

        for r in results:
            if not r.get("success"):
                continue

            estimator = r["spec"]["estimator"]
            oracle_coverage = r["spec"]["oracle_coverage"]

            # Look for IIC diagnostics in metadata
            if "metadata" in r and isinstance(r["metadata"], dict):
                iic_diag = r["metadata"].get("iic_diagnostics", {})

                for policy, diag in iic_diag.items():
                    if isinstance(diag, dict):
                        iic_data.append(
                            {
                                "estimator": estimator,
                                "policy": policy,
                                "oracle_coverage": oracle_coverage,
                                "r_squared": diag.get("r_squared", 0),
                                "se_reduction": diag.get("se_reduction", 0),
                                "var_reduction": diag.get("var_reduction", 0),
                                "ess_gain": diag.get("ess_gain", 1),
                                "direction": diag.get("direction", "unknown"),
                            }
                        )

        if not iic_data:
            logger.warning("No IIC diagnostics found in results")
            return

        df = pd.DataFrame(iic_data)

        # Generate main plot: SE reduction vs R²
        fig, ax = plt.subplots(figsize=(10, 8))

        for estimator in df["estimator"].unique():
            est_df = df[df["estimator"] == estimator]
            scatter = ax.scatter(
                est_df["r_squared"],
                est_df["se_reduction"] * 100,
                label=estimator,
                alpha=0.7,
                s=50,
            )

        ax.set_xlabel("R² (Judge Explainability)", fontsize=12)
        ax.set_ylabel("SE Reduction (%)", fontsize=12)
        ax.set_title(
            "IIC: Variance Reduction from Judge Score Predictability", fontsize=14
        )
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Add summary statistics
        if len(df) > 0:
            mean_r2 = df["r_squared"].mean()
            mean_se_reduction = df["se_reduction"].mean() * 100
            max_se_reduction = df["se_reduction"].max() * 100

            summary_text = (
                f"Mean R²: {mean_r2:.3f}\n"
                f"Mean SE reduction: {mean_se_reduction:.1f}%\n"
                f"Max SE reduction: {max_se_reduction:.1f}%"
            )
            ax.text(
                0.02,
                0.98,
                summary_text,
                transform=ax.transAxes,
                verticalalignment="top",
                bbox=dict(boxstyle="round", facecolor="wheat"),
            )

        plt.tight_layout()
        plt.savefig(output_dir / "iic_se_reduction_vs_r2.png", dpi=150)
        plt.close()

        # Summary table
        summary = (
            df.groupby(["estimator", "policy"])
            .agg(
                {
                    "r_squared": "mean",
                    "se_reduction": "mean",
                    "var_reduction": "mean",
                    "ess_gain": "mean",
                }
            )
            .round(3)
        )

        print("\n" + "=" * 60)
        print("IIC SUMMARY")
        print("=" * 60)
        print(summary)

        # Save summary
        summary.to_csv(output_dir / "iic_summary.csv")
        logger.info(f"Results saved to {output_dir}")


def main() -> None:
    """Run minimal IIC ablation."""
    ablation = IICEffectMinimal()
    results = ablation.run_ablation()
    ablation.analyze_results(results)


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/ablations/interaction.py ===

#!/usr/bin/env python3
"""Interaction ablation - 2D grid of oracle coverage × sample size.

This ablation explores the joint effects of oracle coverage and sample size.

Key findings we expect:
- More oracle data helps more when sample size is large
- Small samples need higher oracle coverage for stability
- Trade-off frontier: you can compensate for less oracle with more samples
"""

import json
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns

import sys

sys.path.append(str(Path(__file__).parent.parent))

from core import ExperimentSpec
from core.base import BaseAblation
from core.schemas import aggregate_results

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)


class InteractionAblation(BaseAblation):
    """Ablation to study oracle×sample interaction."""

    def __init__(self):
        super().__init__(name="interaction")

    def run_ablation(self) -> List[Dict[str, Any]]:
        """Run 2D grid of oracle coverage × sample size."""

        # Define grid
        oracle_coverages = [0.02, 0.05, 0.10, 0.20, 0.50]  # 5 levels
        sample_sizes = [100, 250, 500, 1000, 2500, 5000]  # 6 levels

        # Fixed parameters
        estimator = "stacked-dr"  # Use best method
        n_seeds = 3  # Fewer seeds since we have many configs

        logger.info("=" * 70)
        logger.info("INTERACTION ABLATION (Oracle × Sample Size)")
        logger.info("=" * 70)
        logger.info(f"Oracle coverages: {oracle_coverages}")
        logger.info(f"Sample sizes: {sample_sizes}")
        logger.info(f"Estimator: {estimator}")
        logger.info(
            f"Total configurations: {len(oracle_coverages) * len(sample_sizes)}"
        )
        logger.info(f"Seeds per config: {n_seeds}")
        logger.info("")

        all_results = []
        config_count = 0
        total_configs = len(oracle_coverages) * len(sample_sizes)

        for oracle_coverage in oracle_coverages:
            for sample_size in sample_sizes:
                config_count += 1

                logger.info(f"\n{'='*60}")
                logger.info(
                    f"Config {config_count}/{total_configs}: "
                    f"Oracle={oracle_coverage:.0%}, n={sample_size}"
                )
                logger.info(f"{'='*60}")

                # Determine correct data path based on current directory
                data_path = Path("../data/cje_dataset.jsonl")
                if not data_path.exists():
                    data_path = Path("../../data/cje_dataset.jsonl")

                spec = ExperimentSpec(
                    ablation="interaction",
                    dataset_path=str(data_path),
                    estimator=estimator,
                    oracle_coverage=oracle_coverage,
                    sample_size=sample_size,
                    n_seeds=n_seeds,
                    seed_base=42,
                )

                # Run with multiple seeds
                results = self.run_with_seeds(spec)
                all_results.extend(results)

                # Show summary
                agg = aggregate_results(results)
                if agg.get("n_seeds_successful", 0) > 0:
                    successful = [r for r in results if r.get("success", False)]
                    mean_rmse = np.mean(
                        [r.get("rmse_vs_oracle", np.nan) for r in successful]
                    )

                    logger.info(
                        f"Results ({agg['n_seeds_successful']}/{agg['n_seeds_total']} successful):"
                    )
                    logger.info(f"  Mean RMSE: {mean_rmse:.4f}")
                    logger.info(f"  N oracle: {oracle_coverage * sample_size:.0f}")
                else:
                    logger.warning(f"All {agg['n_seeds_total']} seeds failed!")

        # Save all results
        output_dir = Path("results/interaction")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Convert numpy types before saving
        def convert_numpy(obj):
            import numpy as np

            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.bool_, np.bool8)):
                return bool(obj)
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(v) for v in obj]
            elif hasattr(obj, "item"):
                return obj.item()
            return obj

        with open(output_dir / "results.jsonl", "w") as f:
            for result in all_results:
                f.write(json.dumps(convert_numpy(result)) + "\n")

        logger.info(f"\nSaved {len(all_results)} results to {output_dir}")

        return all_results

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze interaction effects with MDE-based sweet spots."""

        # Create 2D grids for RMSE and SE values
        rmse_grid = {}
        se_grid = {}
        success_grid = {}

        for r in results:
            if r.get("success", False):
                oracle = r["spec"]["oracle_coverage"]
                n_samples = r["spec"]["sample_size"]
                key = (oracle, n_samples)

                if key not in rmse_grid:
                    rmse_grid[key] = []
                    se_grid[key] = []
                    success_grid[key] = 0

                rmse_grid[key].append(r.get("rmse_vs_oracle", np.nan))
                
                # Collect standard errors (proper IC-based SEs)
                if "standard_errors" in r and r["standard_errors"]:
                    # Average SE across policies for this configuration
                    avg_se = np.nanmean(list(r["standard_errors"].values()))
                    se_grid[key].append(avg_se)
                else:
                    # Fallback to RMSE if SE not available
                    se_grid[key].append(r.get("rmse_vs_oracle", np.nan))
                    
                success_grid[key] += 1

        # Average across seeds
        mean_rmse_grid = {}
        mean_se_grid = {}
        for key, rmses in rmse_grid.items():
            mean_rmse_grid[key] = np.nanmean(rmses)
            mean_se_grid[key] = np.nanmean(se_grid[key])

        # Extract unique values for axes
        oracle_values = sorted(set(k[0] for k in mean_rmse_grid.keys()))
        sample_values = sorted(set(k[1] for k in mean_rmse_grid.keys()))

        # Create matrices for heatmap
        rmse_matrix = np.full((len(oracle_values), len(sample_values)), np.nan)
        se_matrix = np.full((len(oracle_values), len(sample_values)), np.nan)
        for i, oracle in enumerate(oracle_values):
            for j, n_samples in enumerate(sample_values):
                if (oracle, n_samples) in mean_rmse_grid:
                    rmse_matrix[i, j] = mean_rmse_grid[(oracle, n_samples)]
                    se_matrix[i, j] = mean_se_grid[(oracle, n_samples)]

        # Find sweet spots based on MDE thresholds using proper SEs
        alpha = 0.05
        power = 0.80
        z_alpha = 1.96
        z_power = 0.84
        k = z_alpha + z_power  # ≈ 2.80
        
        sweet_spots = []
        target_mdes = [0.01, 0.02]  # 1% and 2% effect sizes
        
        for i, oracle in enumerate(oracle_values):
            for j, n_samples in enumerate(sample_values):
                if np.isfinite(se_matrix[i, j]):
                    n_oracle = oracle * n_samples
                    
                    # MDE for two-policy comparison using proper SE
                    mde_two = k * np.sqrt(2.0) * se_matrix[i, j]
                    
                    # Check which MDE targets this config can achieve
                    achievable_targets = [t for t in target_mdes if mde_two <= t]
                    
                    if achievable_targets:
                        # Compute cost-efficiency (lower is better)
                        # Assuming unit cost per oracle label for simplicity
                        cost_per_percent_mde = n_oracle / min(achievable_targets)
                        
                        sweet_spots.append(
                            {
                                "oracle_coverage": oracle,
                                "sample_size": n_samples,
                                "n_oracle": n_oracle,
                                "rmse": rmse_matrix[i, j],
                                "mde_two": mde_two,
                                "achievable_mde": min(achievable_targets),
                                "cost_efficiency": cost_per_percent_mde,
                            }
                        )

        # Sort by cost efficiency (lower is better)
        sweet_spots.sort(key=lambda x: x["cost_efficiency"])

        return {
            "oracle_values": oracle_values,
            "sample_values": sample_values,
            "rmse_matrix": rmse_matrix,
            "se_matrix": se_matrix,  # Proper IC-based standard errors
            "mean_rmse_grid": mean_rmse_grid,
            "mean_se_grid": mean_se_grid,
            "success_grid": success_grid,
            "sweet_spots": sweet_spots[:5],  # Top 5 most cost-efficient
        }

    def create_figure(self, results: List[Dict[str, Any]], output_path: Path = None):
        """Create Figure 3: Interaction heatmap with MDE contours."""

        analysis = self.analyze_results(results)

        if not analysis["oracle_values"] or not analysis["sample_values"]:
            logger.warning("No successful results to plot")
            return

        # Set style
        plt.style.use("seaborn-v0_8-darkgrid")
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # Panel A: RMSE heatmap with improved formatting
        ax = axes[0]
        rmse = analysis["rmse_matrix"]
        mask = ~np.isfinite(rmse)
        
        sns.heatmap(
            rmse,
            mask=mask,
            annot=True,
            fmt=".3f",
            cmap="viridis_r",  # Sequential colormap, colorblind-friendly
            vmin=0.0,
            vmax=np.nanquantile(rmse, 0.95),  # Consistent scale across runs
            xticklabels=[str(x) for x in analysis["sample_values"]],
            yticklabels=[f"{int(100*y)}%" for y in analysis["oracle_values"]],
            cbar_kws={"label": "RMSE"},
            ax=ax,
        )
        ax.set_xlabel("Sample Size", fontsize=12)
        ax.set_ylabel("Oracle Coverage", fontsize=12)
        ax.set_title("A. RMSE vs Oracle Truth", fontsize=14, fontweight="bold")

        # Panel B: MDE contours (ability to detect 1-2% differences)
        ax = axes[1]

        # Use proper IC-based standard errors from estimators
        se_grid = np.array(analysis["se_matrix"], dtype=float)

        # Power calculation settings
        alpha = 0.05  # 5% significance level
        power = 0.80  # 80% power
        z_alpha = 1.96  # stats.norm.ppf(1 - alpha/2)
        z_power = 0.84  # stats.norm.ppf(power)
        k = z_alpha + z_power  # ≈ 2.80

        # MDE for single policy and two-policy comparison
        mde_one = k * se_grid
        mde_two = k * np.sqrt(2.0) * se_grid  # Conservative: assumes SE_Δ ≈ √2 * SE

        # Build coordinates in actual units (not indices)
        X, Y = np.meshgrid(analysis["sample_values"], analysis["oracle_values"])

        # Mask invalid cells
        mask = ~np.isfinite(mde_two)
        mde_plot = np.ma.array(mde_two, mask=mask)

        # Filled contours of MDE for two-policy comparison
        cf = ax.contourf(
            X, Y, mde_plot,
            levels=[0.005, 0.01, 0.015, 0.02, 0.03, 0.05, 0.1],
            cmap="viridis",
            antialiased=True,
        )
        cbar = plt.colorbar(cf, ax=ax, label="MDE (two-policy, 80% power, 95% CI)")

        # Key iso-lines at 1% and 2% effect sizes
        cs = ax.contour(
            X, Y, mde_plot,
            levels=[0.01, 0.02],
            colors=["white", "black"],
            linestyles=["--", "-"],
            linewidths=[2.0, 2.0],
        )
        ax.clabel(cs, fmt={0.01: "1%", 0.02: "2%"}, fontsize=10)

        # Optional: overlay iso-lines of n_oracle (labeling effort)
        n_oracle = X * Y  # X = sample size, Y = oracle coverage fraction
        cost_lines = ax.contour(
            X, Y, np.ma.array(n_oracle, mask=mask),
            levels=[50, 100, 250, 500, 1000, 2000],
            colors="gray",
            linewidths=0.8,
            alpha=0.5,
        )
        ax.clabel(cost_lines, fmt=lambda v: f"{int(v)} labels", fontsize=8)

        ax.set_xlabel("Sample Size", fontsize=12)
        ax.set_ylabel("Oracle Coverage", fontsize=12)
        ax.set_title("B. MDE (Can we detect 1-2% effects?)", fontsize=14, fontweight="bold")
        ax.set_ylim(min(analysis["oracle_values"]), max(analysis["oracle_values"]))
        ax.invert_yaxis()  # Match heatmap orientation (higher coverage at top)

        plt.suptitle(
            "Oracle × Sample Size Interaction", fontsize=16, fontweight="bold"
        )
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Leave space for suptitle

        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(output_path, dpi=150, bbox_inches="tight")
            logger.info(f"Saved figure to {output_path}")

        plt.show()

        return fig


def main():
    """Run interaction ablation."""

    ablation = InteractionAblation()

    # Run the ablation
    results = ablation.run_ablation()

    # Analyze
    analysis = ablation.analyze_results(results)

    logger.info("\n" + "=" * 70)
    logger.info("ANALYSIS SUMMARY")
    logger.info("=" * 70)

    if analysis["sweet_spots"]:
        logger.info("\nMost cost-efficient configurations:")
        logger.info("(For detecting 1-2% improvements with 80% power)")

        for i, spot in enumerate(analysis["sweet_spots"], 1):
            logger.info(
                f"\n{i}. Oracle={spot['oracle_coverage']:.0%}, n={spot['sample_size']}"
            )
            logger.info(f"   N oracle labels: {spot['n_oracle']:.0f}")
            logger.info(f"   RMSE: {spot['rmse']:.4f}")
            logger.info(f"   MDE (two-policy): {spot['mde_two']:.1%}")
            logger.info(f"   Can detect: ≥{spot['achievable_mde']:.0%} effects")
            logger.info(f"   Cost per % MDE: {spot['cost_efficiency']:.0f} labels")

    # Show overall patterns
    if len(analysis["rmse_matrix"]) > 0:
        logger.info("\nOverall patterns:")

        # Effect of doubling oracle coverage
        if len(analysis["oracle_values"]) >= 2:
            first_col = analysis["rmse_matrix"][:, 0]  # First sample size
            valid = np.isfinite(first_col)
            if np.sum(valid) >= 2:
                improvement = (first_col[valid][0] - first_col[valid][-1]) / first_col[
                    valid
                ][0]
                logger.info(
                    f"  Increasing oracle 10x: ~{improvement:.0%} RMSE reduction"
                )

        # Effect of doubling sample size
        if len(analysis["sample_values"]) >= 2:
            first_row = analysis["rmse_matrix"][0, :]  # First oracle coverage
            valid = np.isfinite(first_row)
            if np.sum(valid) >= 2:
                improvement = (first_row[valid][0] - first_row[valid][-1]) / first_row[
                    valid
                ][0]
                logger.info(
                    f"  Increasing samples 25x: ~{improvement:.0%} RMSE reduction"
                )

    # Create figure
    figure_path = Path("results/interaction/interaction_mde_analysis.png")
    ablation.create_figure(results, figure_path)

    logger.info("\n" + "=" * 70)
    logger.info("INTERACTION ABLATION COMPLETE")
    logger.info("=" * 70)

    return results


if __name__ == "__main__":
    results = main()


=== ./cje/experiments/arena_10k_simplified/ablations/oracle_coverage.py ===

#!/usr/bin/env python3
"""Oracle coverage ablation - THE fundamental result.

This ablation answers: How much oracle data do we need?

Key findings we expect:
- 5-10% oracle coverage is sufficient for reliable estimates
- Below 5% calibration becomes unreliable
- Augmentation helps when coverage is low
- Diminishing returns above 20%
"""

import json
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns

import sys

sys.path.append(str(Path(__file__).parent.parent))

from core import ExperimentSpec
from core.base import BaseAblation
from core.schemas import aggregate_results

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)


class OracleCoverageAblation(BaseAblation):
    """Ablation to study oracle coverage requirements."""

    def __init__(self):
        super().__init__(name="oracle_coverage")

    def run_ablation(self) -> List[Dict[str, Any]]:
        """Run oracle coverage sweep."""

        # Define coverage levels to test
        oracle_coverages = [0.01, 0.02, 0.05, 0.10, 0.20, 0.50, 1.00]

        # Fixed parameters for this ablation
        estimator = "mrdr"  # Use MRDR as it's our best method
        sample_fraction = 1.0  # Use full dataset
        n_seeds = 5  # Multiple seeds for stability

        logger.info("=" * 70)
        logger.info("ORACLE COVERAGE ABLATION")
        logger.info("=" * 70)
        logger.info(f"Estimator: {estimator}")
        logger.info(f"Sample fraction: {sample_fraction:.0%}")
        logger.info(f"Coverage levels: {oracle_coverages}")
        logger.info(f"Seeds per level: {n_seeds}")
        logger.info("")

        all_results = []

        for coverage in oracle_coverages:
            logger.info(f"\n{'='*50}")
            logger.info(f"Oracle Coverage: {coverage:.0%}")
            logger.info(f"{'='*50}")

            # Use stable dataset
            data_path = Path("../data/cje_dataset.jsonl")
            if not data_path.exists():
                data_path = Path("../../data/cje_dataset.jsonl")

            spec = ExperimentSpec(
                ablation="oracle_coverage",
                dataset_path=str(data_path),
                estimator=estimator,
                oracle_coverage=coverage,
                sample_fraction=sample_fraction,
                n_seeds=n_seeds,
                seed_base=42,
            )

            # Run with multiple seeds
            results = self.run_with_seeds(spec)
            all_results.extend(results)

            # Show aggregate statistics
            agg = aggregate_results(results)
            if agg.get("n_seeds_successful", 0) > 0:
                mean_rmse = np.mean(
                    [
                        r.get("rmse_vs_oracle", np.nan)
                        for r in results
                        if r.get("success", False)
                    ]
                )
                mean_ci_width = np.mean(
                    [
                        r.get("mean_ci_width", np.nan)
                        for r in results
                        if r.get("success", False)
                    ]
                )

                logger.info(
                    f"Results ({agg['n_seeds_successful']}/{agg['n_seeds_total']} successful):"
                )
                logger.info(f"  Mean RMSE: {mean_rmse:.4f}")
                logger.info(f"  Mean CI width: {mean_ci_width:.4f}")

                # Show per-policy RMSE if available
                if agg.get("estimates_mean"):
                    oracle_truths = agg.get("oracle_truths", {})
                    for policy in agg["estimates_mean"]:
                        est = agg["estimates_mean"][policy]
                        truth = oracle_truths.get(policy, np.nan)
                        if np.isfinite(est) and np.isfinite(truth):
                            policy_rmse = abs(est - truth)
                            logger.info(
                                f"  {policy}: estimate={est:.3f}, truth={truth:.3f}, error={policy_rmse:.3f}"
                            )
            else:
                logger.warning(f"All {agg['n_seeds_total']} seeds failed!")

        # Save all results
        output_dir = Path("results/oracle_coverage")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Convert numpy types before saving
        def convert_numpy(obj):
            import numpy as np

            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.bool_, np.bool8)):
                return bool(obj)
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(v) for v in obj]
            elif hasattr(obj, "item"):
                return obj.item()
            return obj

        with open(output_dir / "results.jsonl", "w") as f:
            for result in all_results:
                f.write(json.dumps(convert_numpy(result)) + "\n")

        logger.info(f"\nSaved {len(all_results)} results to {output_dir}")

        return all_results

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze oracle coverage results."""

        # Group by coverage level
        by_coverage = {}
        for r in results:
            if r.get("success", False):
                coverage = r["spec"]["oracle_coverage"]
                if coverage not in by_coverage:
                    by_coverage[coverage] = []
                by_coverage[coverage].append(r)

        analysis = {
            "coverage_levels": sorted(by_coverage.keys()),
            "mean_rmse": {},
            "mean_ci_width": {},
            "mean_calibration_rmse": {},
            "success_rate": {},
        }

        for coverage in analysis["coverage_levels"]:
            coverage_results = by_coverage[coverage]

            # RMSE
            rmses = [r.get("rmse_vs_oracle", np.nan) for r in coverage_results]
            analysis["mean_rmse"][coverage] = np.nanmean(rmses)

            # CI width
            ci_widths = [r.get("mean_ci_width", np.nan) for r in coverage_results]
            analysis["mean_ci_width"][coverage] = np.nanmean(ci_widths)

            # Calibration RMSE
            cal_rmses = [r.get("calibration_rmse", np.nan) for r in coverage_results]
            analysis["mean_calibration_rmse"][coverage] = np.nanmean(cal_rmses)

            # Success rate
            analysis["success_rate"][coverage] = len(coverage_results)

        # Find sweet spot (where RMSE stabilizes)
        rmse_values = [analysis["mean_rmse"][c] for c in analysis["coverage_levels"]]
        if len(rmse_values) > 2:
            # Find where improvement slows down
            improvements = np.diff(rmse_values)
            if len(improvements) > 0:
                # Find first point where improvement < 10% of initial
                threshold = 0.1 * abs(improvements[0]) if improvements[0] != 0 else 0.01
                sweet_spot_idx = next(
                    (i for i, imp in enumerate(improvements) if abs(imp) < threshold),
                    len(improvements) - 1,
                )
                analysis["sweet_spot_coverage"] = analysis["coverage_levels"][
                    min(sweet_spot_idx + 1, len(analysis["coverage_levels"]) - 1)
                ]
            else:
                analysis["sweet_spot_coverage"] = analysis["coverage_levels"][0]

        return analysis

    def create_figure(self, results: List[Dict[str, Any]], output_path: Path = None):
        """Create Figure 1: Oracle coverage vs RMSE."""

        analysis = self.analyze_results(results)

        if not analysis["coverage_levels"]:
            logger.warning("No successful results to plot")
            return

        # Set style
        plt.style.use("seaborn-v0_8-darkgrid")
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        coverages = np.array(analysis["coverage_levels"]) * 100  # Convert to percentage

        # Panel A: RMSE vs coverage
        ax = axes[0]
        rmse_values = [analysis["mean_rmse"][c] for c in analysis["coverage_levels"]]
        ax.plot(coverages, rmse_values, "o-", linewidth=2, markersize=8, label="MRDR")

        # Add sweet spot
        if "sweet_spot_coverage" in analysis:
            sweet_spot_pct = analysis["sweet_spot_coverage"] * 100
            ax.axvline(
                sweet_spot_pct,
                color="red",
                linestyle="--",
                alpha=0.5,
                label=f"Sweet spot ({sweet_spot_pct:.0f}%)",
            )

        ax.set_xlabel("Oracle Coverage (%)", fontsize=12)
        ax.set_ylabel("RMSE vs Oracle Truth", fontsize=12)
        ax.set_title(
            "A. Estimation Error vs Oracle Coverage", fontsize=14, fontweight="bold"
        )
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Panel B: CI Width vs coverage
        ax = axes[1]
        ci_values = [analysis["mean_ci_width"][c] for c in analysis["coverage_levels"]]
        ax.plot(coverages, ci_values, "o-", linewidth=2, markersize=8, color="orange")
        ax.set_xlabel("Oracle Coverage (%)", fontsize=12)
        ax.set_ylabel("Mean CI Width", fontsize=12)
        ax.set_title(
            "B. Uncertainty vs Oracle Coverage", fontsize=14, fontweight="bold"
        )
        ax.grid(True, alpha=0.3)

        # Panel C: Calibration quality
        ax = axes[2]
        cal_values = [
            analysis["mean_calibration_rmse"][c] for c in analysis["coverage_levels"]
        ]
        ax.plot(coverages, cal_values, "o-", linewidth=2, markersize=8, color="green")
        ax.set_xlabel("Oracle Coverage (%)", fontsize=12)
        ax.set_ylabel("Calibration RMSE", fontsize=12)
        ax.set_title(
            "C. Judge→Oracle Calibration Quality", fontsize=14, fontweight="bold"
        )
        ax.grid(True, alpha=0.3)

        plt.suptitle(
            "Oracle Coverage Requirements for CJE",
            fontsize=16,
            fontweight="bold",
            y=1.02,
        )
        plt.tight_layout()

        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(output_path, dpi=150, bbox_inches="tight")
            logger.info(f"Saved figure to {output_path}")

        plt.show()

        return fig


def main():
    """Run oracle coverage ablation."""

    ablation = OracleCoverageAblation()

    # Run the ablation
    results = ablation.run_ablation()

    # Analyze
    analysis = ablation.analyze_results(results)

    logger.info("\n" + "=" * 70)
    logger.info("ANALYSIS SUMMARY")
    logger.info("=" * 70)

    if analysis["coverage_levels"]:
        logger.info("\nRMSE by coverage:")
        for coverage in analysis["coverage_levels"]:
            rmse = analysis["mean_rmse"][coverage]
            logger.info(f"  {coverage:6.1%}: {rmse:.4f}")

        if "sweet_spot_coverage" in analysis:
            logger.info(f"\nSweet spot: {analysis['sweet_spot_coverage']:.1%} coverage")
            logger.info("(Where diminishing returns begin)")

    # Create figure
    figure_path = Path("results/oracle_coverage/figure_1_oracle_coverage.png")
    ablation.create_figure(results, figure_path)

    logger.info("\n" + "=" * 70)
    logger.info("ORACLE COVERAGE ABLATION COMPLETE")
    logger.info("=" * 70)

    return results


if __name__ == "__main__":
    results = main()


=== ./cje/experiments/arena_10k_simplified/ablations/oracle_coverage_simple.py ===

#!/usr/bin/env python3
"""Simplified oracle coverage ablation - testing how much oracle data we need.

This version removes caching complexity while keeping the core experiment logic.
"""

import json
import logging
import numpy as np
import random
from pathlib import Path
from typing import Dict, List, Any
import matplotlib.pyplot as plt

# Add parent dirs to path
import sys

sys.path.append(str(Path(__file__).parent.parent))
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from cje import load_dataset_from_jsonl
from cje.calibration import calibrate_dataset
from cje.data.precomputed_sampler import PrecomputedSampler
from cje.estimators.mrdr import MRDREstimator
from cje.data.fresh_draws import load_fresh_draws_auto

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)


def mask_oracle_labels(dataset, oracle_coverage: float, seed: int):
    """Mask oracle labels to simulate partial coverage.

    Args:
        dataset: Dataset with samples
        oracle_coverage: Fraction to keep (0-1)
        seed: Random seed for reproducibility

    Returns:
        Number of oracle labels kept
    """
    random.seed(seed)
    np.random.seed(seed)

    # Find samples with oracle labels
    oracle_indices = [
        i
        for i, s in enumerate(dataset.samples)
        if s.metadata.get("oracle_label") is not None
    ]

    # Keep only specified fraction
    n_keep = max(2, int(len(oracle_indices) * oracle_coverage))
    keep_indices = set(random.sample(oracle_indices, min(n_keep, len(oracle_indices))))

    # Mask the rest
    for i, sample in enumerate(dataset.samples):
        if i not in keep_indices and "oracle_label" in sample.metadata:
            sample.metadata["oracle_label"] = None

    return len(keep_indices)


def run_single_experiment(
    dataset_path: str, oracle_coverage: float, seed: int
) -> Dict[str, Any]:
    """Run a single oracle coverage experiment.

    Args:
        dataset_path: Path to dataset
        oracle_coverage: Fraction of oracle labels to use
        seed: Random seed

    Returns:
        Dictionary with results
    """
    result = {"oracle_coverage": oracle_coverage, "seed": seed, "success": False}

    try:
        # Load dataset
        dataset = load_dataset_from_jsonl(dataset_path)
        n_samples = len(dataset.samples)

        # Mask oracle labels
        n_oracle = mask_oracle_labels(dataset, oracle_coverage, seed)
        result["n_samples"] = n_samples
        result["n_oracle"] = n_oracle

        # Calibrate dataset
        calibrated_dataset, cal_result = calibrate_dataset(
            dataset,
            judge_field="judge_score",
            oracle_field="oracle_label",
            enable_cross_fit=True,
            n_folds=5,
        )

        if cal_result:
            result["calibration_rmse"] = float(cal_result.calibration_rmse)

        # Create estimator (MRDR)
        sampler = PrecomputedSampler(calibrated_dataset)
        estimator = MRDREstimator(
            sampler,
            calibrator=cal_result.calibrator if cal_result else None,
            n_folds=5,
            oracle_slice_config=(oracle_coverage < 1.0),
        )

        # Add fresh draws
        data_dir = Path(dataset_path).parent
        for policy in sampler.target_policies:
            try:
                fresh_draws = load_fresh_draws_auto(data_dir, policy, verbose=False)
                estimator.add_fresh_draws(policy, fresh_draws)
            except FileNotFoundError:
                logger.warning(f"No fresh draws for {policy}")

        # Run estimation
        estimation_result = estimator.fit_and_estimate()

        # Extract results
        result["estimates"] = {}
        result["standard_errors"] = {}

        for i, policy in enumerate(sampler.target_policies):
            result["estimates"][policy] = float(estimation_result.estimates[i])
            result["standard_errors"][policy] = float(
                estimation_result.standard_errors[i]
            )

        # Add diagnostics if available
        if estimation_result.diagnostics:
            diag = estimation_result.diagnostics
            result["mean_ess"] = float(diag.weight_ess) if diag.weight_ess else None

        result["success"] = True

    except Exception as e:
        logger.error(f"Experiment failed: {e}")
        result["error"] = str(e)

    return result


def compute_oracle_ground_truth(dataset_path: str) -> Dict[str, float]:
    """Load oracle ground truth values from response files.

    Args:
        dataset_path: Path to dataset file

    Returns:
        Dictionary mapping policy names to oracle means
    """
    oracle_means = {}
    data_dir = Path(dataset_path).parent
    responses_dir = data_dir / "responses"

    # Get policy names from a sample file or hardcode the known ones
    policies = ["clone", "parallel_universe_prompt", "premium", "unhelpful"]

    for policy in policies:
        response_file = responses_dir / f"{policy}_responses.jsonl"
        if response_file.exists():
            oracle_values = []
            with open(response_file, "r") as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        if "metadata" in data and "oracle_label" in data["metadata"]:
                            oracle_val = data["metadata"]["oracle_label"]
                            if oracle_val is not None:
                                oracle_values.append(oracle_val)
                    except json.JSONDecodeError:
                        continue

            if oracle_values:
                oracle_means[policy] = float(np.mean(oracle_values))

    return oracle_means


def main():
    """Run oracle coverage ablation."""

    # Configuration
    oracle_coverages = [0.05, 0.10]  # Just test 2 levels for now
    n_seeds = 1  # Just 1 seed for testing

    # Find dataset
    data_path = Path("../data/cje_dataset.jsonl")
    if not data_path.exists():
        data_path = Path("../../data/cje_dataset.jsonl")

    if not data_path.exists():
        logger.error(f"Dataset not found at {data_path}")
        return

    logger.info("=" * 70)
    logger.info("ORACLE COVERAGE ABLATION (SIMPLIFIED)")
    logger.info("=" * 70)
    logger.info(f"Dataset: {data_path}")
    logger.info(f"Coverage levels: {oracle_coverages}")
    logger.info(f"Seeds per level: {n_seeds}")
    logger.info("")

    # Get ground truth
    oracle_truth = compute_oracle_ground_truth(str(data_path))
    logger.info(f"Oracle ground truth loaded for {len(oracle_truth)} policies")
    logger.info("")

    # Run experiments
    all_results = []

    for coverage in oracle_coverages:
        logger.info(f"\nOracle Coverage: {coverage:.0%}")
        logger.info("-" * 40)

        coverage_results = []
        for seed_offset in range(n_seeds):
            seed = 42 + seed_offset

            result = run_single_experiment(str(data_path), coverage, seed)

            if result["success"]:
                # Compute RMSE vs oracle if we have ground truth
                if oracle_truth and result["estimates"]:
                    errors = []
                    for policy, estimate in result["estimates"].items():
                        if policy in oracle_truth:
                            errors.append((estimate - oracle_truth[policy]) ** 2)
                    if errors:
                        result["rmse_vs_oracle"] = float(np.sqrt(np.mean(errors)))

                logger.info(
                    f"  Seed {seed}: ✓ RMSE={result.get('rmse_vs_oracle', 0):.4f}"
                )
            else:
                logger.info(f"  Seed {seed}: ✗ {result.get('error', 'Failed')}")

            coverage_results.append(result)
            all_results.append(result)

        # Show summary for this coverage level
        successful = [r for r in coverage_results if r["success"]]
        if successful:
            mean_rmse = np.mean([r.get("rmse_vs_oracle", np.nan) for r in successful])
            logger.info(f"  Mean RMSE: {mean_rmse:.4f}")

    # Save results
    output_dir = Path("results/oracle_coverage")
    output_dir.mkdir(parents=True, exist_ok=True)

    with open(output_dir / "results_simple.jsonl", "w") as f:
        for result in all_results:
            f.write(json.dumps(result) + "\n")

    logger.info(
        f"\nSaved {len(all_results)} results to {output_dir}/results_simple.jsonl"
    )

    # Analysis
    logger.info("\n" + "=" * 70)
    logger.info("ANALYSIS SUMMARY")
    logger.info("=" * 70)

    # Group by coverage
    by_coverage = {}
    for r in all_results:
        if r["success"]:
            cov = r["oracle_coverage"]
            if cov not in by_coverage:
                by_coverage[cov] = []
            by_coverage[cov].append(r.get("rmse_vs_oracle", np.nan))

    logger.info("\nRMSE by Coverage:")
    for cov in sorted(by_coverage.keys()):
        rmses = by_coverage[cov]
        mean_rmse = np.nanmean(rmses)
        std_rmse = np.nanstd(rmses)
        logger.info(f"  {cov:6.1%}: {mean_rmse:.4f} ± {std_rmse:.4f}")

    # Find sweet spot (where improvement slows)
    coverages = sorted(by_coverage.keys())
    mean_rmses = [np.nanmean(by_coverage[c]) for c in coverages]
    if len(mean_rmses) > 2:
        improvements = -np.diff(mean_rmses)
        # Find first point where improvement < 10% of initial
        if improvements[0] != 0:
            threshold = 0.1 * abs(improvements[0])
            for i, imp in enumerate(improvements):
                if abs(imp) < threshold:
                    logger.info(f"\nSweet spot: {coverages[i]:.1%} coverage")
                    logger.info("(Diminishing returns beyond this point)")
                    break

    # Create simple visualization
    if by_coverage:
        plt.figure(figsize=(10, 6))

        coverages_pct = [c * 100 for c in coverages]
        plt.plot(coverages_pct, mean_rmses, "o-", linewidth=2, markersize=8)

        plt.xlabel("Oracle Coverage (%)", fontsize=12)
        plt.ylabel("RMSE vs Oracle Truth", fontsize=12)
        plt.title("Oracle Coverage Impact on Estimation Error", fontsize=14)
        plt.grid(True, alpha=0.3)

        figure_path = output_dir / "oracle_coverage_simple.png"
        plt.savefig(figure_path, dpi=150, bbox_inches="tight")
        logger.info(f"\nSaved figure to {figure_path}")
        plt.close()

    logger.info("\n" + "=" * 70)
    logger.info("ORACLE COVERAGE ABLATION COMPLETE")
    logger.info("=" * 70)


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/ablations/run_all_ablations.py ===

#!/usr/bin/env python3
"""
Master script to run all ablations for the CJE paper.

This runs all ablation experiments in sequence and collects results.
Results are saved to ablations/results/ directory.
"""

import subprocess
import sys
import time
from pathlib import Path
import json
from datetime import datetime


def run_ablation(script_name: str) -> bool:
    """Run a single ablation script and return success status."""
    print(f"\n{'='*60}")
    print(f"Running {script_name}...")
    print(f"{'='*60}")

    start_time = time.time()
    try:
        result = subprocess.run(
            [sys.executable, script_name], capture_output=True, text=True, check=True
        )
        elapsed = time.time() - start_time
        print(f"✓ {script_name} completed in {elapsed:.1f}s")
        if result.stdout:
            print(result.stdout)
        return True
    except subprocess.CalledProcessError as e:
        elapsed = time.time() - start_time
        print(f"✗ {script_name} failed after {elapsed:.1f}s")
        print(f"Error: {e.stderr}")
        return False


def main() -> int:
    """Run all ablations and collect results."""

    # Ablations to run in order
    ablations = [
        "oracle_coverage.py",  # Effect of oracle slice size
        "sample_size.py",  # Effect of dataset size
        "estimator_comparison.py",  # Compare all estimators
        "interaction.py",  # Oracle × sample size interaction
    ]

    print("=" * 80)
    print("CJE ABLATION EXPERIMENTS")
    print(f"Starting at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    # Track results
    from typing import Dict, Any

    results: Dict[str, Any] = {"timestamp": datetime.now().isoformat(), "ablations": {}}

    total_start = time.time()
    successful = 0
    failed = 0

    for ablation in ablations:
        success = run_ablation(ablation)
        if success:
            successful += 1
            results["ablations"][ablation] = "completed"

            # Try to load the results if they exist
            result_file = Path(f"results/{ablation.replace('.py', '_results.json')}")
            if result_file.exists():
                with open(result_file) as f:
                    results["ablations"][ablation] = json.load(f)
        else:
            failed += 1
            results["ablations"][ablation] = "failed"

    total_elapsed = time.time() - total_start

    # Summary
    print("\n" + "=" * 80)
    print("ABLATION SUMMARY")
    print("=" * 80)
    print(f"Successful: {successful}/{len(ablations)}")
    print(f"Failed: {failed}/{len(ablations)}")
    print(f"Total time: {total_elapsed/60:.1f} minutes")

    # Save master results
    results_dir = Path("results")
    results_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    master_file = results_dir / f"all_ablations_{timestamp}.json"
    with open(master_file, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\nMaster results saved to: {master_file}")

    # Print key findings
    print("\n" + "=" * 80)
    print("KEY FINDINGS")
    print("=" * 80)

    if successful > 0:
        print(
            """
Based on completed ablations:

1. Oracle Coverage: Shows how calibration quality affects estimates
2. Sample Size: Demonstrates convergence behavior  
3. Estimator Comparison: StackedDR should show best performance
4. Interaction: Reveals when DR methods are most valuable

Check individual result files in results/ for detailed analysis.
"""
        )

    return 0 if failed == 0 else 1


if __name__ == "__main__":
    sys.exit(main())


=== ./cje/experiments/arena_10k_simplified/ablations/sample_size.py ===

#!/usr/bin/env python3
"""Sample size scaling ablation.

This ablation answers: How many samples do we need?

Key findings we expect:
- Standard errors follow √n scaling
- DR methods are ~4x more sample efficient than IPS
- ESS degrades with importance weighting
- Minimum viable is ~1000 samples for production use
"""

import json
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import seaborn as sns

import sys

sys.path.append(str(Path(__file__).parent.parent))

from core import ExperimentSpec
from core.base import BaseAblation
from core.schemas import aggregate_results

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger(__name__)


class SampleSizeAblation(BaseAblation):
    """Ablation to study sample size requirements."""

    def __init__(self):
        super().__init__(name="sample_size")

    def run_ablation(self) -> List[Dict[str, Any]]:
        """Run sample size scaling experiments."""

        # Define sample sizes to test
        sample_sizes = [100, 250, 500, 1000, 2500, 5000]

        # Test key estimators in order of increasing sophistication
        # Shows progression: Basic IPS → Calibrated IPS → DR → Calibrated DR → Stacked → Calibrated Stacked
        estimators = [
            "raw-ips",  # 1. Baseline
            "calibrated-ips",  # 2. + Weight calibration
            "dr-cpo",  # 3. + Doubly robust (no weight cal)
            "calibrated-dr-cpo",  # 4. + Weight calibration on DR
            "stacked-dr",  # 5. + Ensemble (no weight cal)
            "cal-stacked-dr",  # 6. + Weight calibration on ensemble
        ]

        # Fixed parameters
        oracle_coverage = 1.00  # 100% oracle coverage
        n_seeds = 5  # Multiple seeds for stability

        logger.info("=" * 70)
        logger.info("SAMPLE SIZE SCALING ABLATION")
        logger.info("=" * 70)
        logger.info(f"Sample sizes: {sample_sizes}")
        logger.info(f"Estimators: {estimators}")
        logger.info(f"Oracle coverage: {oracle_coverage:.0%}")
        logger.info(f"Seeds per configuration: {n_seeds}")
        logger.info("")

        all_results = []

        for estimator in estimators:
            logger.info(f"\n{'='*60}")
            logger.info(f"Estimator: {estimator.upper()}")
            logger.info(f"{'='*60}")

            for n_samples in sample_sizes:
                logger.info(f"\nSample size: {n_samples}")
                logger.info("-" * 30)

                # Determine correct data path based on current directory
                data_path = Path("../data/cje_dataset.jsonl")
                if not data_path.exists():
                    data_path = Path("../../data/cje_dataset.jsonl")

                spec = ExperimentSpec(
                    ablation="sample_size",
                    dataset_path=str(data_path),
                    estimator=estimator,
                    oracle_coverage=oracle_coverage,
                    sample_size=n_samples,
                    n_seeds=n_seeds,
                    seed_base=42,
                )

                # Run with multiple seeds
                results = self.run_with_seeds(spec)
                all_results.extend(results)

                # Show aggregate statistics
                agg = aggregate_results(results)
                if agg.get("n_seeds_successful", 0) > 0:
                    # Extract successful results
                    successful_results = [r for r in results if r.get("success", False)]

                    # Compute mean metrics
                    mean_rmse = np.mean(
                        [r.get("rmse_vs_oracle", np.nan) for r in successful_results]
                    )

                    # Mean SE across policies and seeds
                    all_ses = []
                    for r in successful_results:
                        if "standard_errors" in r:
                            all_ses.extend(list(r["standard_errors"].values()))
                    mean_se = np.mean(all_ses) if all_ses else np.nan

                    # Mean ESS
                    all_ess = []
                    for r in successful_results:
                        if "ess_absolute" in r:
                            all_ess.extend(list(r["ess_absolute"].values()))
                    mean_ess = np.mean(all_ess) if all_ess else np.nan

                    logger.info(
                        f"Results ({agg['n_seeds_successful']}/{agg['n_seeds_total']} successful):"
                    )
                    logger.info(f"  Mean RMSE: {mean_rmse:.4f}")
                    logger.info(f"  Mean SE: {mean_se:.4f}")
                    logger.info(
                        f"  Mean ESS: {mean_ess:.0f} ({100*mean_ess/n_samples:.1f}%)"
                    )
                else:
                    logger.warning(f"All {agg['n_seeds_total']} seeds failed!")

        # Save all results
        output_dir = Path("results/sample_size")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Convert numpy types before saving
        def convert_numpy(obj):
            import numpy as np

            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, (np.bool_, np.bool8)):
                return bool(obj)
            elif isinstance(obj, dict):
                return {k: convert_numpy(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy(v) for v in obj]
            elif hasattr(obj, "item"):
                return obj.item()
            return obj

        with open(output_dir / "results.jsonl", "w") as f:
            for result in all_results:
                f.write(json.dumps(convert_numpy(result)) + "\n")

        logger.info(f"\nSaved {len(all_results)} results to {output_dir}")

        return all_results

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze sample size scaling."""

        # Group by estimator and sample size
        by_config = {}
        for r in results:
            if r.get("success", False):
                estimator = r["spec"]["estimator"]
                n_samples = r["spec"]["sample_size"]
                key = (estimator, n_samples)
                if key not in by_config:
                    by_config[key] = []
                by_config[key].append(r)

        # Organize analysis by estimator
        estimators = list(set(k[0] for k in by_config.keys()))
        analysis = {
            est: {
                "sample_sizes": [],
                "mean_rmse": [],
                "mean_se": [],
                "mean_ess": [],
                "ess_percent": [],
            }
            for est in estimators
        }

        for (estimator, n_samples), results_subset in sorted(by_config.items()):
            # RMSE
            rmses = [r.get("rmse_vs_oracle", np.nan) for r in results_subset]

            # Standard errors
            all_ses = []
            for r in results_subset:
                if "standard_errors" in r:
                    all_ses.extend(list(r["standard_errors"].values()))

            # ESS
            all_ess = []
            for r in results_subset:
                if "ess_absolute" in r:
                    all_ess.extend(list(r["ess_absolute"].values()))

            analysis[estimator]["sample_sizes"].append(n_samples)
            analysis[estimator]["mean_rmse"].append(np.nanmean(rmses))
            analysis[estimator]["mean_se"].append(
                np.nanmean(all_ses) if all_ses else np.nan
            )
            analysis[estimator]["mean_ess"].append(
                np.nanmean(all_ess) if all_ess else np.nan
            )
            analysis[estimator]["ess_percent"].append(
                100 * np.nanmean(all_ess) / n_samples if all_ess else np.nan
            )

        # Check √n scaling
        for estimator in estimators:
            n_array = np.array(analysis[estimator]["sample_sizes"])
            se_array = np.array(analysis[estimator]["mean_se"])

            # Filter out NaNs
            mask = np.isfinite(se_array) & (n_array > 0)
            if np.sum(mask) >= 2:
                # Fit SE = c / √n in log space
                log_n = np.log(n_array[mask])
                log_se = np.log(se_array[mask])

                # Linear regression in log space
                from scipy import stats

                slope, intercept, r_value, _, _ = stats.linregress(log_n, log_se)

                analysis[estimator]["scaling_exponent"] = slope
                analysis[estimator]["scaling_r2"] = r_value**2

                # Ideal √n scaling has slope = -0.5
                analysis[estimator]["follows_sqrt_n"] = abs(slope + 0.5) < 0.1

        return analysis

    def create_figure(self, results: List[Dict[str, Any]], output_path: Path = None):
        """Create Figure 2: Sample size scaling."""

        analysis = self.analyze_results(results)

        if not analysis:
            logger.warning("No successful results to plot")
            return

        # Set style
        plt.style.use("seaborn-v0_8-darkgrid")
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        colors = {
            "raw-ips": "red",  # Red for uncalibrated baseline
            "calibrated-ips": "blue",  # Blue for calibrated IPS
            "dr-cpo": "orange",  # Orange for uncalibrated DR
            "calibrated-dr-cpo": "green",  # Green for calibrated DR
            "stacked-dr": "purple",  # Purple for stacked DR
            "cal-stacked-dr": "darkviolet",  # Dark violet for calibrated stacked
        }

        # Panel A: RMSE vs n (Estimation Error)
        ax = axes[0]
        for estimator, data in analysis.items():
            if data["sample_sizes"]:
                ax.loglog(
                    data["sample_sizes"],
                    data["mean_rmse"],
                    "o-",
                    label=estimator.upper().replace("-", " "),
                    color=colors.get(estimator, "gray"),
                    linewidth=2,
                    markersize=8,
                )
        ax.set_xlabel("Sample Size (n)", fontsize=12)
        ax.set_ylabel("RMSE vs Oracle", fontsize=12)
        ax.set_title("Estimation Error vs Sample Size", fontsize=14, fontweight="bold")
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Panel B: SE vs n (Standard Error Scaling)
        ax = axes[1]
        for estimator, data in analysis.items():
            if data["sample_sizes"]:
                ax.loglog(
                    data["sample_sizes"],
                    data["mean_se"],
                    "o-",
                    label=estimator.upper().replace("-", " "),
                    color=colors.get(estimator, "gray"),
                    linewidth=2,
                    markersize=8,
                )

        # Add √n reference line
        n_ref = np.array([100, 5000])
        se_ref = 0.1 * np.sqrt(n_ref[0] / n_ref)  # c/√n scaling
        ax.loglog(n_ref, se_ref, "k--", alpha=0.5, label="√n scaling")

        ax.set_xlabel("Sample Size (n)", fontsize=12)
        ax.set_ylabel("Standard Error", fontsize=12)
        ax.set_title("Standard Error Scaling", fontsize=14, fontweight="bold")
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Add scaling exponent annotation
        for i, (estimator, data) in enumerate(analysis.items()):
            if "scaling_exponent" in data:
                ax.text(
                    0.05,
                    0.95 - i * 0.1,
                    f'{estimator}: slope = {data["scaling_exponent"]:.2f}',
                    transform=ax.transAxes,
                    fontsize=10,
                )

        # Removed ESS panels C and D - focusing on error and SE scaling only

        plt.suptitle(
            "Sample Size Requirements for CJE", fontsize=16, fontweight="bold", y=1.02
        )
        plt.tight_layout()

        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(output_path, dpi=150, bbox_inches="tight")
            logger.info(f"Saved figure to {output_path}")

        plt.show()

        return fig


def main():
    """Run sample size ablation."""

    ablation = SampleSizeAblation()

    # Run the ablation
    results = ablation.run_ablation()

    # Analyze
    analysis = ablation.analyze_results(results)

    logger.info("\n" + "=" * 70)
    logger.info("ANALYSIS SUMMARY")
    logger.info("=" * 70)

    for estimator, data in analysis.items():
        logger.info(f"\n{estimator.upper()}:")

        if data["sample_sizes"]:
            logger.info("\n  Sample Size -> RMSE:")
            for n, rmse in zip(data["sample_sizes"], data["mean_rmse"]):
                logger.info(f"    {n:5d}: {rmse:.4f}")

            if "scaling_exponent" in data:
                logger.info(f"\n  SE scaling exponent: {data['scaling_exponent']:.3f}")
                logger.info(f"  (Ideal √n scaling = -0.50)")
                logger.info(f"  R² of fit: {data['scaling_r2']:.3f}")

                if data["follows_sqrt_n"]:
                    logger.info("  ✓ Follows √n scaling")
                else:
                    logger.info("  ✗ Deviates from √n scaling")

    # Compare efficiency
    if len(analysis) == 2 and all(
        len(data["mean_rmse"]) > 0 for data in analysis.values()
    ):
        estimators = list(analysis.keys())

        # Find common sample sizes
        common_sizes = set(analysis[estimators[0]]["sample_sizes"]) & set(
            analysis[estimators[1]]["sample_sizes"]
        )

        if common_sizes:
            logger.info(f"\nEfficiency comparison at n={max(common_sizes)}:")
            n = max(common_sizes)

            for est in estimators:
                idx = analysis[est]["sample_sizes"].index(n)
                rmse = analysis[est]["mean_rmse"][idx]
                logger.info(f"  {est}: RMSE = {rmse:.4f}")

    # Create figure
    figure_path = Path("results/sample_size/figure_2_sample_scaling.png")
    ablation.create_figure(results, figure_path)

    logger.info("\n" + "=" * 70)
    logger.info("SAMPLE SIZE ABLATION COMPLETE")
    logger.info("=" * 70)

    return results


if __name__ == "__main__":
    results = main()


=== ./cje/experiments/arena_10k_simplified/ablations/simcal_rho.py ===

#!/usr/bin/env python3
"""
SIMCal variance cap (ρ) ablation.

Tests different variance cap values to understand the bias-variance tradeoff.
ρ = 1.0: No variance increase allowed (most conservative)
ρ = 2.0: Default, allows 2x variance for better bias
ρ = 3.0+: More permissive, less variance reduction
"""

import logging
from pathlib import Path
from typing import Any, Dict, List
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from core.base import BaseAblation
from core.schemas import ExperimentSpec

logger = logging.getLogger(__name__)


class SIMCalRhoAblation(BaseAblation):
    """Ablation for SIMCal variance cap parameter."""

    def __init__(self) -> None:
        super().__init__("simcal_rho")

    def create_estimator(self, spec: Any, sampler: Any, cal_result: Any) -> Any:
        """Override to pass var_cap to CalibratedIPS."""
        if spec.estimator == "calibrated-ips":
            # Import here to avoid circular dependency
            from cje.estimators import CalibratedIPS

            # Get var_cap from spec.extra or use default
            var_cap = spec.extra.get("var_cap", None)

            return CalibratedIPS(
                sampler,
                calibrate=True,
                var_cap=var_cap,
                calibrator=cal_result.calibrator if cal_result else None,
            )
        else:
            return super().create_estimator(spec, sampler, cal_result)

    def run_ablation(self) -> List[Dict[str, Any]]:
        """Test different var_cap values."""
        results = []

        # Test different variance caps (None means no cap)
        var_cap_values = [0.5, 1.0, 2.0, 5.0, None]

        for var_cap in var_cap_values:
            spec = ExperimentSpec(
                ablation="simcal_rho",
                dataset_path="../data/cje_dataset.jsonl",
                estimator="calibrated-ips",
                oracle_coverage=0.2,
                n_seeds=5,
                extra={"var_cap": var_cap},  # Pass via extra dict
            )

            logger.info(f"Running with var_cap={var_cap}")
            seed_results = self.run_with_seeds(spec)
            results.extend(seed_results)

        return results

    def analyze_results(self, results: List[Dict[str, Any]]) -> None:
        """Analyze rho sweep results."""
        output_dir = Path("ablations/results/simcal_rho")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Aggregate by var_cap
        cap_data = []
        for r in results:
            if r.get("success"):
                var_cap = r["spec"].get("extra", {}).get("var_cap", "None")

                # Extract relevant metrics
                mean_ess = np.mean(list(r.get("ess_relative", {}).values()))
                mean_ci_width = r.get("mean_ci_width", 0)
                rmse = r.get("rmse_vs_oracle", 0)

                cap_data.append(
                    {
                        "var_cap": var_cap if var_cap is not None else float("inf"),
                        "var_cap_label": str(var_cap),
                        "ess": mean_ess,
                        "ci_width": mean_ci_width,
                        "rmse": rmse,
                    }
                )

        if not cap_data:
            logger.warning("No successful results")
            return

        df = pd.DataFrame(cap_data)

        # Plot: 2x2 grid
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # Group by var_cap and aggregate
        grouped = df.groupby("var_cap").agg(
            {
                "ess": ["mean", "std"],
                "ci_width": ["mean", "std"],
                "rmse": ["mean", "std"],
            }
        )

        var_caps = grouped.index

        # Panel A: ESS vs var_cap
        ax = axes[0, 0]
        ax.errorbar(
            var_caps,
            grouped["ess"]["mean"],
            yerr=grouped["ess"]["std"],
            marker="o",
            capsize=5,
        )
        ax.set_xlabel("Variance Cap")
        ax.set_ylabel("ESS (%)")
        ax.set_title("(A) Effective Sample Size")
        ax.grid(True, alpha=0.3)

        # Panel B: CI width vs var_cap
        ax = axes[0, 1]
        ax.errorbar(
            var_caps,
            grouped["ci_width"]["mean"],
            yerr=grouped["ci_width"]["std"],
            marker="o",
            capsize=5,
            color="orange",
        )
        ax.set_xlabel("Variance Cap")
        ax.set_ylabel("CI Width")
        ax.set_title("(B) Confidence Interval Width")
        ax.grid(True, alpha=0.3)

        # Panel C: RMSE vs var_cap
        ax = axes[1, 0]
        ax.errorbar(
            var_caps,
            grouped["rmse"]["mean"],
            yerr=grouped["rmse"]["std"],
            marker="o",
            capsize=5,
            color="red",
        )
        ax.set_xlabel("Variance Cap")
        ax.set_ylabel("RMSE vs Oracle")
        ax.set_title("(C) Estimation Error")
        ax.grid(True, alpha=0.3)

        # Panel D: Bias-variance tradeoff
        ax = axes[1, 1]
        # Normalize to show tradeoff
        ess_norm = grouped["ess"]["mean"] / grouped["ess"]["mean"].max()
        rmse_norm = 1 - (grouped["rmse"]["mean"] / grouped["rmse"]["mean"].max())

        ax.plot(var_caps, ess_norm, "o-", label="ESS (normalized)")
        ax.plot(var_caps, rmse_norm, "s-", label="Accuracy (1-RMSE, normalized)")
        ax.axvline(
            x=2.0, color="red", linestyle="--", alpha=0.5, label="Default var_cap=2"
        )
        ax.set_xlabel("Variance Cap")
        ax.set_ylabel("Normalized Performance")
        ax.set_title("(D) Bias-Variance Tradeoff")
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.suptitle("SIMCal Variance Cap Ablation", fontsize=14, y=1.02)
        plt.tight_layout()
        plt.savefig(output_dir / "simcal_rho_ablation.png", dpi=150)
        plt.close()

        # Print summary
        print("\n" + "=" * 60)
        print("SIMCAL VARIANCE CAP ABLATION SUMMARY")
        print("=" * 60)
        print(grouped)
        print(f"\nOptimal var_cap appears to be around 2.0 (allows 2x variance)")

        logger.info(f"Results saved to {output_dir}")


def main() -> None:
    """Run SIMCal rho ablation."""
    ablation = SIMCalRhoAblation()
    results = ablation.run_ablation()
    ablation.analyze_results(results)


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/analysis/__init__.py ===

"""CJE analysis pipeline modules.

This package contains modular components for analyzing CJE datasets.
Each module has a single responsibility following the Unix philosophy.
"""

from .loading import load_data
from .calibration import handle_rewards, restore_oracle_labels
from .estimation import create_estimator, add_fresh_draws
from .results import display_results, compute_base_statistics
from .diagnostics import (
    display_weight_diagnostics,
    display_dr_diagnostics,
    display_augmentation_diagnostics,
    analyze_extreme_weights_report,
)
from .visualization import generate_visualizations
from .export import export_results

__all__ = [
    # Data loading
    "load_data",
    # Calibration
    "handle_rewards",
    "restore_oracle_labels",
    # Estimation
    "create_estimator",
    "add_fresh_draws",
    # Results
    "display_results",
    "compute_base_statistics",
    # Diagnostics
    "display_weight_diagnostics",
    "display_dr_diagnostics",
    "display_augmentation_diagnostics",
    "analyze_extreme_weights_report",
    # Visualization
    "generate_visualizations",
    # Export
    "export_results",
]


=== ./cje/experiments/arena_10k_simplified/analysis/calibration.py ===

"""Calibration and reward assignment for CJE analysis.

This module handles calibrating judge scores to oracle labels and assigning
rewards to samples. It supports partial oracle coverage and cross-fitting.

Following CLAUDE.md: Do one thing well - this module only handles calibration.
"""

import random
import numpy as np
from typing import Any, Optional, Tuple, Set
from cje.calibration.dataset import calibrate_dataset


def handle_rewards(
    dataset: Any, args: Any, analysis_config: dict, verbose: bool = True
) -> Tuple[Any, Optional[Any]]:
    """Handle reward assignment and calibration.

    This function determines whether to:
    1. Use existing rewards from the dataset
    2. Use oracle labels directly as rewards
    3. Calibrate judge scores to get rewards

    Args:
        dataset: Input dataset
        args: Command-line arguments
        analysis_config: Configuration dictionary
        verbose: Whether to print status

    Returns:
        Tuple of (calibrated_dataset, calibration_result)
    """
    if verbose:
        print("\n2. Handling rewards...")

    calibrated_dataset = None
    cal_result = None

    # Check if rewards already exist
    rewards_exist = sum(1 for s in dataset.samples if s.reward is not None)

    if rewards_exist > 0:
        # Use existing rewards
        if verbose:
            print(f"   ✓ Using {rewards_exist} pre-computed rewards from dataset")
        calibrated_dataset = dataset

        # Check if we need cross-fitted models for SIMCal
        if _needs_crossfit_models(dataset, args):
            cal_result = _fit_crossfit_models(dataset, args, analysis_config, verbose)
            _add_fold_ids(calibrated_dataset, cal_result)

    elif args.use_oracle or args.oracle_coverage == 1.0:
        # Use oracle labels directly
        calibrated_dataset = _use_oracle_as_rewards(dataset, args, verbose)

        # Still fit cross-fitted models for SIMCal ordering
        if verbose:
            print("   Fitting cross-fitted models for SIMCal ordering index...")
        _, cal_result = calibrate_dataset(
            dataset,
            judge_field=args.judge_field,
            oracle_field=args.oracle_field,
            enable_cross_fit=True,
            n_folds=analysis_config["n_folds"],
        )
        _add_fold_ids(calibrated_dataset, cal_result)

        if verbose and cal_result:
            print(
                f"   ✓ Cross-fitted models ready (RMSE: {cal_result.calibration_rmse:.3f})"
            )

    else:
        # Calibrate with partial oracle coverage
        calibrated_dataset, cal_result = _calibrate_with_coverage(
            dataset, args, analysis_config, verbose
        )

    return calibrated_dataset, cal_result


def _needs_crossfit_models(dataset: Any, args: Any) -> bool:
    """Check if we need to fit cross-fitted models."""
    has_fold_ids = all("cv_fold" in s.metadata for s in dataset.samples[:10])
    has_oracle = args.oracle_field in dataset.samples[0].metadata
    return not has_fold_ids and has_oracle


def _fit_crossfit_models(
    dataset: Any, args: Any, analysis_config: dict, verbose: bool
) -> Optional[Any]:
    """Fit cross-fitted calibration models."""
    if verbose:
        print("   Fitting cross-fitted models for SIMCal ordering index...")

    try:
        _, cal_result = calibrate_dataset(
            dataset,
            judge_field=args.judge_field,
            oracle_field=args.oracle_field,
            enable_cross_fit=True,
            n_folds=analysis_config["n_folds"],
        )
        if verbose:
            print(f"   ✓ Cross-fitted models ready")
        return cal_result
    except Exception as e:
        if verbose:
            print(f"   ⚠️  Could not fit cross-fitted models: {e}")
        return None


def _add_fold_ids(dataset: Any, cal_result: Any) -> None:
    """Legacy function - no longer adds fold IDs to metadata.

    Folds are now computed on-demand from prompt_id using the unified fold system.
    This function is kept for backward compatibility but does nothing.
    """
    # Folds are computed dynamically via cje.data.folds.get_fold()
    # No need to store in metadata anymore
    pass


def _use_oracle_as_rewards(dataset: Any, args: Any, verbose: bool) -> Any:
    """Use oracle labels directly as rewards."""
    if verbose:
        print("   Using oracle labels directly as rewards...")

    oracle_count = 0
    for sample in dataset.samples:
        if args.oracle_field in sample.metadata:
            sample.reward = float(sample.metadata[args.oracle_field])
            oracle_count += 1

    if verbose:
        print(f"   ✓ Assigned {oracle_count} oracle labels as rewards")

    return dataset


def _calibrate_with_coverage(
    dataset: Any, args: Any, analysis_config: dict, verbose: bool
) -> Tuple[Any, Any]:
    """Calibrate with partial oracle coverage."""
    if verbose:
        print(f"   Calibrating with {args.oracle_coverage:.0%} oracle coverage...")

    # Set random seed for reproducibility
    random.seed(42)
    np.random.seed(42)

    # Store original oracle labels (will be restored later for visualization)
    original_oracle_labels = {}

    # Mask some oracle labels if partial coverage
    if args.oracle_coverage < 1.0:
        original_oracle_labels = _mask_oracle_labels(
            dataset, args, args.oracle_coverage
        )

    # Calibrate with cross-fitting for DR
    calibrated_dataset, cal_result = calibrate_dataset(
        dataset,
        judge_field=args.judge_field,
        oracle_field=args.oracle_field,
        enable_cross_fit=True,
        n_folds=analysis_config["n_folds"],
    )

    if verbose:
        print(f"   ✓ Calibrated using {cal_result.n_oracle} oracle labels")
        print(f"   ✓ Calibration RMSE: {cal_result.calibration_rmse:.3f}")

    # Store original labels for later restoration
    calibrated_dataset._original_oracle_labels = original_oracle_labels
    # Also store on original dataset so it can be restored there too
    dataset._original_oracle_labels = original_oracle_labels

    return calibrated_dataset, cal_result


def _mask_oracle_labels(dataset: Any, args: Any, oracle_coverage: float) -> dict:
    """Mask oracle labels to simulate partial coverage.

    Returns:
        Dictionary mapping sample index to original oracle value
    """
    samples_with_oracle = [
        i
        for i, s in enumerate(dataset.samples)
        if args.oracle_field in s.metadata and s.metadata[args.oracle_field] is not None
    ]

    n_keep = max(2, int(len(samples_with_oracle) * oracle_coverage))
    keep_indices = set(
        random.sample(samples_with_oracle, min(n_keep, len(samples_with_oracle)))
    )

    original_oracle_labels = {}
    for i, sample in enumerate(dataset.samples):
        if i not in keep_indices and args.oracle_field in sample.metadata:
            original_oracle_labels[i] = sample.metadata[args.oracle_field]
            sample.metadata[args.oracle_field] = None

    return original_oracle_labels


def restore_oracle_labels(dataset: Any, args: Any) -> None:
    """Restore masked oracle labels for visualization.

    This should be called after estimation but before visualization.
    """
    if hasattr(dataset, "_original_oracle_labels"):
        for i, oracle_value in dataset._original_oracle_labels.items():
            dataset.samples[i].metadata[args.oracle_field] = oracle_value


=== ./cje/experiments/arena_10k_simplified/analysis/diagnostics.py ===

"""Diagnostic computation and display for CJE analysis.

This module handles displaying weight diagnostics, DR diagnostics,
and generating diagnostic reports.

Following CLAUDE.md: Do one thing well - this module only handles diagnostics display.
"""

import numpy as np
from pathlib import Path
from typing import Any, Dict, Optional
from cje.utils.extreme_weights_analysis import analyze_extreme_weights
from cje.diagnostics.weights import compute_weight_diagnostics
from cje.diagnostics.display import (
    create_weight_summary_table,
    format_dr_diagnostic_summary,
)


# Diagnostic thresholds (following CLAUDE.md: explicit > magic values)
DIAGNOSTIC_THRESHOLDS = {
    "LOW_ESS": 0.1,
    "CRITICAL_ESS": 0.05,
    "EXTREME_CONCENTRATION": 0.9,
    "NEAR_ZERO_WEIGHT": 1e-10,
    "EXTREME_ESTIMATE_DIFF": 0.3,  # >30% difference from base is suspicious
}


def display_weight_diagnostics(
    estimator: Any, sampler: Any, calibrated_dataset: Any, args: Any
) -> Dict[str, Any]:
    """Display weight diagnostics and return diagnostic data.

    Args:
        estimator: Fitted estimator
        sampler: PrecomputedSampler
        calibrated_dataset: Dataset with calibrated rewards
        args: Command-line arguments

    Returns:
        Dictionary of weight diagnostics per policy
    """
    print(f"\n5. Weight diagnostics:")

    # Try to use the estimator's diagnostics object directly if available
    if hasattr(estimator, "get_diagnostics"):
        diagnostics = estimator.get_diagnostics()
        # Check if it's an IPSDiagnostics object (not a plain dict from DR estimators)
        if diagnostics is not None and hasattr(diagnostics, "policies"):
            # Use the IPSDiagnostics object directly for display
            print("\n" + create_weight_summary_table(diagnostics))

            # Still need to return the dictionary format for downstream code
            all_weight_diagnostics = {}

            # Add base policy manually
            base_rewards = [
                s.reward for s in calibrated_dataset.samples if s.reward is not None
            ]
            base_diag = compute_weight_diagnostics(
                np.ones(len(base_rewards)),
                "base",
            )
            all_weight_diagnostics["base"] = base_diag

            # Add target policies from diagnostics
            for policy in diagnostics.policies:
                all_weight_diagnostics[policy] = {
                    "ess_fraction": diagnostics.ess_per_policy.get(policy, 0.0),
                    "max_weight": diagnostics.max_weight_per_policy.get(policy, 1.0),
                    "status": (
                        diagnostics.status_per_policy.get(policy)
                        if diagnostics.status_per_policy
                        else None
                    ),
                    "tail_index": (
                        diagnostics.tail_indices.get(policy)
                        if diagnostics.tail_indices
                        else None
                    ),
                }

            # Print warnings if issues found
            _print_weight_warnings(all_weight_diagnostics, sampler, estimator)

            return all_weight_diagnostics

    # Fallback: compute diagnostics manually
    base_rewards = [
        s.reward for s in calibrated_dataset.samples if s.reward is not None
    ]
    all_weight_diagnostics = {}

    # Base policy (uniform weights)
    base_diag = compute_weight_diagnostics(np.ones(len(base_rewards)), "base")
    all_weight_diagnostics["base"] = base_diag

    # Target policies
    for policy in sampler.target_policies:
        weights = estimator.get_weights(policy)
        if weights is not None:
            diag = compute_weight_diagnostics(weights, policy)
            all_weight_diagnostics[policy] = diag

    # Display table
    _display_weight_table(all_weight_diagnostics)

    # Print warnings
    _print_weight_warnings(all_weight_diagnostics, sampler, estimator)

    return all_weight_diagnostics


def display_dr_diagnostics(results: Any, args: Any) -> None:
    """Display DR diagnostics if available.

    Args:
        results: EstimationResult object
        args: Command-line arguments
    """
    # Check for DRDiagnostics object
    if hasattr(results, "diagnostics") and results.diagnostics is not None:
        from cje.diagnostics import DRDiagnostics

        if isinstance(results.diagnostics, DRDiagnostics):
            print(f"\n6. Doubly Robust diagnostics:")
            # Format the DR diagnostics
            summary = format_dr_diagnostic_summary(results.diagnostics)
            for line in summary.split("\n"):
                print(f"   {line}")

            # Check for issues
            if results.diagnostics.worst_if_tail_ratio > 100:
                print("\n   ⚠️  Warning: Heavy-tailed influence functions detected")
                print(
                    "      Consider using more fresh draws or checking policy overlap"
                )
            return

    # Fallback to legacy format
    if (
        args.estimator in ["dr-cpo", "mrdr", "tmle"]
        and "dr_diagnostics" in results.metadata
    ):
        print(f"\n6. Doubly Robust diagnostics:")
        # Use the dr_diagnostics directly from metadata
        dr_diagnostics = results.metadata["dr_diagnostics"]
        summary = format_dr_diagnostic_summary(dr_diagnostics)

        for line in summary.split("\n"):
            print(f"   {line}")

        # Check for issues
        if isinstance(dr_diagnostics, dict):
            worst_tail = (
                max(
                    d.get("if_tail_ratio_99_5", 0)
                    for d in dr_diagnostics.values()
                    if isinstance(d, dict)
                )
                if dr_diagnostics
                else 0
            )
            if worst_tail > 100:
                print("\n   ⚠️  Warning: Heavy-tailed influence functions detected")
                print(
                    "      Consider using more fresh draws or checking policy overlap"
                )

        if args.estimator == "tmle" and "tmle_max_score_z" in dr_diagnostics:
            if dr_diagnostics["tmle_max_score_z"] > 2:
                print("\n   ⚠️  Warning: TMLE orthogonality not achieved (|z| > 2)")
                print("      Targeting may not have fully converged")


def display_augmentation_diagnostics(
    estimator: Any, results: Any, oracle_coverage: float, args: Any
) -> None:
    """Display oracle augmentation diagnostics if available.

    Args:
        estimator: Fitted estimator (IPS or DR)
        results: EstimationResult object
        oracle_coverage: Fraction of oracle labels used
        args: Command-line arguments
    """
    # Check if augmentation diagnostics are available
    if not hasattr(estimator, "_aug_diagnostics"):
        return

    # Check if augmentation diagnostics are empty
    if not estimator._aug_diagnostics:
        # No augmentation data available
        if oracle_coverage < 1.0:
            print(f"\n7. Oracle Augmentation Impact:")
            print(f"   Oracle coverage used: {oracle_coverage:.1%}")
            print(f"   Note: Oracle augmentation not available for this configuration")
        return

    print(f"\n7. Oracle Augmentation Impact:")
    print(f"   Oracle coverage used: {oracle_coverage:.1%}")
    print(f"   " + "-" * 60)

    # Collect augmentation info for each policy
    for policy in estimator.sampler.target_policies:
        aug_diag = estimator._aug_diagnostics.get(policy, {})
        if not aug_diag:
            continue

        # Get the variance share
        slice_share = aug_diag.get("slice_variance_share", 0)

        # For DR estimators, calculate share of total variance
        if hasattr(estimator, "_influence_functions"):
            if_funcs = estimator._influence_functions.get(policy)
            if if_funcs is not None and len(if_funcs) > 1:
                total_var = np.var(if_funcs, ddof=1)
                aug_var = aug_diag.get("aug_var", 0)
                if total_var > 0:
                    aug_contribution = aug_var / total_var * 100
                    print(
                        f"   {policy}: {aug_contribution:.1f}% of uncertainty from calibration"
                    )
        else:
            # For IPS, use the stored slice_variance_share
            if slice_share > 0:
                print(
                    f"   {policy}: {slice_share:.1f}% of uncertainty from calibration"
                )

    print()
    print("   💡 Interpretation:")
    if oracle_coverage < 0.2:
        print("   - Low oracle coverage is inflating confidence intervals")
        print("   - Consider increasing oracle labels for tighter bounds")
    if oracle_coverage < 0.5:
        print("   - Augmentation accounts for calibration uncertainty")
        print("   - CIs are honest but could be tighter with more labels")


def analyze_extreme_weights_report(
    estimator: Any, sampler: Any, calibrated_dataset: Any, args: Any
) -> None:
    """Generate extreme weights analysis report.

    Args:
        estimator: Fitted estimator
        sampler: PrecomputedSampler
        calibrated_dataset: Dataset with calibrated rewards
        args: Command-line arguments
    """
    step_num = 7 if args.estimator in ["dr-cpo", "mrdr", "tmle"] else 6
    print(f"\n{step_num}. Analyzing extreme weights...")

    analysis_raw_weights = {}
    analysis_cal_weights = {}

    for policy in sampler.target_policies:
        raw_weights = estimator.get_raw_weights(policy)
        if raw_weights is not None:
            analysis_raw_weights[policy] = raw_weights

        cal_weights = estimator.get_weights(policy)
        if cal_weights is not None:
            analysis_cal_weights[policy] = cal_weights

    if analysis_raw_weights:
        try:
            report_dir = None
            if not args.no_plots:
                report_dir = (
                    Path(args.plot_dir)
                    if args.plot_dir
                    else Path(args.data).parent / "plots"
                )
                report_dir.mkdir(parents=True, exist_ok=True)

            json_report, text_report = analyze_extreme_weights(
                dataset=calibrated_dataset,
                sampler=sampler,
                raw_weights_dict=analysis_raw_weights,
                calibrated_weights_dict=analysis_cal_weights,
                n_extreme=5,
                output_dir=report_dir,
                near_zero_threshold=args.extreme_threshold_low,
            )

            # Print summary
            if "per_policy_analysis" in json_report:
                for policy, analysis in json_report["per_policy_analysis"].items():
                    if policy != "base":
                        stats = analysis.get("statistics", {})
                        n_high = stats.get("n_clipped_high", 0)
                        n_zero = stats.get("n_near_zero", 0)
                        print(f"   ✓ {policy}: {n_high} very high, {n_zero} near-zero")

            if report_dir:
                print(
                    f"   ✓ Saved detailed report to {report_dir}/extreme_weights_analysis.txt"
                )

        except Exception as e:
            print(f"   ⚠️  Could not generate extreme weights analysis: {e}")


def _display_weight_table(weight_diagnostics: Dict[str, Any]) -> None:
    """Display weight diagnostics in table format.

    Args:
        weight_diagnostics: Dictionary of diagnostics per policy
    """
    print("   " + "-" * 50)
    print("   Policy              ESS%    Max Weight    Status")
    print("   " + "-" * 50)

    for policy, diag in weight_diagnostics.items():
        ess_pct = diag.get("ess_fraction", 1.0) * 100
        max_weight = diag.get("max_weight", 1.0)
        status = diag.get("status", "OK")

        # Format status
        if isinstance(status, str):
            status_str = status
        elif hasattr(status, "name"):
            status_str = status.name
        else:
            status_str = str(status)

        print(f"   {policy:<18} {ess_pct:>5.1f}%  {max_weight:>10.2f}    {status_str}")

    print("   " + "-" * 50)


def _print_weight_warnings(
    weight_diagnostics: Dict[str, Any], sampler: Any, estimator: Any
) -> None:
    """Print warnings for weight diagnostic issues.

    Args:
        weight_diagnostics: Dictionary of diagnostics per policy
        sampler: PrecomputedSampler
        estimator: Fitted estimator
    """
    # Check for low ESS
    has_issues = any(
        d.get("ess_fraction", 1.0) < DIAGNOSTIC_THRESHOLDS["LOW_ESS"]
        for d in weight_diagnostics.values()
    )

    if has_issues:
        print("\n   ⚠️  Weight diagnostics warnings:")
        for policy, diag in weight_diagnostics.items():
            if diag.get("ess_fraction", 1.0) < DIAGNOSTIC_THRESHOLDS["LOW_ESS"]:
                print(f"   - {policy}: Low ESS ({diag['ess_fraction']:.1%})")

    # Check for extreme concentration
    for policy in sampler.target_policies:
        weights = estimator.get_weights(policy)
        if weights is not None and len(weights) > 0:
            near_zero = np.sum(
                weights < DIAGNOSTIC_THRESHOLDS["NEAR_ZERO_WEIGHT"]
            ) / len(weights)
            if near_zero > DIAGNOSTIC_THRESHOLDS["EXTREME_CONCENTRATION"]:
                print(f"\n   🔴 CRITICAL: {policy} has extreme weight concentration")
                print(f"      {near_zero:.1%} of samples have near-zero weight")
                print(
                    f"      Estimate based on only {len(weights) * (1-near_zero):.0f} effective samples"
                )
                print(
                    f"      Results may be unreliable - consider using DR with more fresh draws"
                )


=== ./cje/experiments/arena_10k_simplified/analysis/estimation.py ===

"""Estimator creation and configuration for CJE analysis.

This module handles creating the appropriate estimator (IPS, DR, TMLE, etc.)
based on configuration and adding fresh draws for DR-based estimators.

Following CLAUDE.md: Do one thing well - this module only handles estimation setup.
"""

from pathlib import Path
from typing import Any, Dict, Optional, Union
import sys

from cje.estimators import CalibratedIPS
from cje.estimators.dr_base import DRCPOEstimator
from cje.estimators.mrdr import MRDREstimator
from cje.estimators.tmle import TMLEEstimator
from cje.estimators.stacking import StackedDREstimator
from cje.data.precomputed_sampler import PrecomputedSampler
from cje.calibration.dataset import calibrate_dataset
from cje.data.fresh_draws import load_fresh_draws_auto

# Note: validation import removed - function doesn't exist


def create_estimator(
    args: Any,
    sampler: PrecomputedSampler,
    calibrated_dataset: Any,
    cal_result: Optional[Any] = None,
) -> Union[
    CalibratedIPS, DRCPOEstimator, MRDREstimator, TMLEEstimator, StackedDREstimator
]:
    """Create the appropriate estimator based on args.

    Args:
        args: Command-line arguments
        sampler: PrecomputedSampler with data
        calibrated_dataset: Dataset with rewards
        cal_result: Optional calibration result

    Returns:
        Configured estimator instance
    """
    estimator_config = args.estimator_config or {}

    if args.estimator == "calibrated-ips":
        refuse_unreliable = estimator_config.get("refuse_unreliable", False)
        return CalibratedIPS(sampler, refuse_unreliable=refuse_unreliable)

    elif args.estimator == "raw-ips":
        clip_weight = estimator_config.get("clip_weight", 1e10)
        # Use CalibratedIPS with calibrate=False for raw IPS
        return CalibratedIPS(sampler, calibrate=False, clip_weight=clip_weight)

    elif args.estimator == "dr-cpo":
        return _create_dr_cpo(args, sampler, cal_result, estimator_config)

    elif args.estimator == "mrdr":
        return _create_mrdr(
            args, sampler, calibrated_dataset, cal_result, estimator_config
        )

    elif args.estimator == "tmle":
        return _create_tmle(
            args, sampler, calibrated_dataset, cal_result, estimator_config
        )

    elif args.estimator == "stacked-dr":
        return _create_stacked_dr(
            args, sampler, calibrated_dataset, cal_result, estimator_config
        )

    else:
        raise ValueError(f"Unknown estimator: {args.estimator}")


def _create_dr_cpo(
    args: Any,
    sampler: PrecomputedSampler,
    cal_result: Optional[Any],
    estimator_config: Dict[str, Any],
) -> DRCPOEstimator:
    """Create DR-CPO estimator."""
    n_folds = estimator_config.get("n_folds", 5)

    if cal_result and cal_result.calibrator:
        dr_estimator = DRCPOEstimator(
            sampler,
            n_folds=n_folds,
            calibrator=cal_result.calibrator,
        )
        print("   Using CalibratorBackedOutcomeModel (reusing calibration models)")
    else:
        dr_estimator = DRCPOEstimator(sampler, n_folds=n_folds)
        print("   Using IsotonicOutcomeModel (refitting models)")

    # Load fresh draws
    print("   Loading fresh draws for DR estimation...")
    add_fresh_draws(dr_estimator, args, sampler, estimator_config)

    return dr_estimator


def _create_mrdr(
    args: Any,
    sampler: PrecomputedSampler,
    calibrated_dataset: Any,
    cal_result: Optional[Any],
    estimator_config: Dict[str, Any],
) -> MRDREstimator:
    """Create MRDR estimator."""
    n_folds = estimator_config.get("n_folds", 5)
    omega_mode = estimator_config.get("omega_mode", "snips")

    # MRDR works best with cross-fitted calibration
    if args.oracle_coverage < 1.0 and (not cal_result or not cal_result.calibrator):
        print("   ⚠️  MRDR works best with cross-fitted calibration. Re-calibrating...")
        # Note: validation check removed - function doesn't exist
        calibrated_dataset, cal_result = calibrate_dataset(
            calibrated_dataset,
            judge_field="judge_score",
            oracle_field="oracle_label",
            enable_cross_fit=True,
            n_folds=n_folds,
        )
        sampler = PrecomputedSampler(calibrated_dataset)

    mrdr_estimator = MRDREstimator(sampler, n_folds=n_folds, omega_mode=omega_mode)
    print(f"   Using MRDR with omega_mode='{omega_mode}'")

    # Load fresh draws
    print("   Loading fresh draws for MRDR estimation...")
    add_fresh_draws(mrdr_estimator, args, sampler, estimator_config)

    return mrdr_estimator


def _create_tmle(
    args: Any,
    sampler: PrecomputedSampler,
    calibrated_dataset: Any,
    cal_result: Optional[Any],
    estimator_config: Dict[str, Any],
) -> TMLEEstimator:
    """Create TMLE estimator."""
    n_folds = estimator_config.get("n_folds", 5)
    link = estimator_config.get("link", "logit")

    # TMLE works best with cross-fitted calibration
    if args.oracle_coverage < 1.0 and (not cal_result or not cal_result.calibrator):
        print("   ⚠️  TMLE works best with cross-fitted calibration. Re-calibrating...")
        # Note: validation check removed - function doesn't exist
        calibrated_dataset, cal_result = calibrate_dataset(
            calibrated_dataset,
            judge_field="judge_score",
            oracle_field="oracle_label",
            enable_cross_fit=True,
            n_folds=n_folds,
        )
        sampler = PrecomputedSampler(calibrated_dataset)

    tmle_estimator = TMLEEstimator(sampler, n_folds=n_folds, link=link)
    print(f"   Using TMLE with link='{link}'")

    # Load fresh draws
    print("   Loading fresh draws for TMLE estimation...")
    add_fresh_draws(tmle_estimator, args, sampler, estimator_config)

    return tmle_estimator


def add_fresh_draws(
    estimator: Any,
    args: Any,
    sampler: PrecomputedSampler,
    estimator_config: Dict[str, Any],
) -> None:
    """Add fresh draws to a DR estimator for all target policies.

    NOTE: As of the latest version, DR estimators auto-load fresh draws
    when estimate() is called. This function is kept for backward compatibility
    but is no longer necessary.

    Args:
        estimator: DR estimator instance
        args: Command-line arguments (contains data path)
        sampler: PrecomputedSampler with target policies
        estimator_config: Estimator configuration (unused but kept for compatibility)

    Raises:
        FileNotFoundError: If fresh draws are not available
    """
    # Auto-loading is now handled internally by DR estimators
    # This function is kept for backward compatibility but does nothing
    return


def _create_stacked_dr(
    args: Any,
    sampler: PrecomputedSampler,
    calibrated_dataset: Any,
    cal_result: Optional[Any],
    estimator_config: Dict[str, Any],
) -> StackedDREstimator:
    """Create stacked DR estimator combining DR-CPO, TMLE, and MRDR."""
    n_folds = estimator_config.get("n_folds", 5)
    estimators = estimator_config.get("estimators", ["dr-cpo", "tmle", "mrdr"])
    use_outer_split = estimator_config.get("use_outer_split", True)

    # Create stacked estimator
    if cal_result and cal_result.calibrator:
        stacked_estimator = StackedDREstimator(
            sampler,
            calibrator=cal_result.calibrator,
            estimators=estimators,
            n_folds=n_folds,
            use_outer_split=use_outer_split,
        )
        print(f"   Creating stacked estimator with: {', '.join(estimators)}")
        print("   Using CalibratorBackedOutcomeModel (reusing calibration models)")
    else:
        stacked_estimator = StackedDREstimator(
            sampler,
            estimators=estimators,
            n_folds=n_folds,
            use_outer_split=use_outer_split,
        )
        print(f"   Creating stacked estimator with: {', '.join(estimators)}")
        print("   Using IsotonicOutcomeModel (refitting models)")

    # Load fresh draws
    print("   Loading fresh draws for stacked DR estimation...")
    add_fresh_draws(stacked_estimator, args, sampler, estimator_config)

    return stacked_estimator


=== ./cje/experiments/arena_10k_simplified/analysis/export.py ===

"""Export functionality for CJE analysis results.

This module handles exporting analysis results to various formats (JSON, CSV).

Following CLAUDE.md: Do one thing well - this module only handles export.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional


def export_results(
    results: Any,
    dataset: Any,
    summary_data: Dict[str, Any],
    weight_diagnostics: Dict[str, Any],
    args: Any,
) -> None:
    """Export results to JSON or CSV format.

    Args:
        results: EstimationResult object
        dataset: Original dataset
        summary_data: Summary statistics from results display
        weight_diagnostics: Weight diagnostic information
        args: Command-line arguments (contains output path)
    """
    if not args.output:
        return

    # Prepare output data
    output_data = _prepare_output_data(
        results, dataset, summary_data, weight_diagnostics, args
    )

    # Determine format from extension
    output_path = Path(args.output)

    if output_path.suffix.lower() == ".csv":
        _export_to_csv(output_data, output_path)
    else:
        # Default to JSON
        _export_to_json(output_data, output_path)

    print(f"\n✓ Results written to: {args.output}")


def _prepare_output_data(
    results: Any,
    dataset: Any,
    summary_data: Dict[str, Any],
    weight_diagnostics: Dict[str, Any],
    args: Any,
) -> Dict[str, Any]:
    """Prepare data structure for export.

    Args:
        results: EstimationResult object
        dataset: Original dataset
        summary_data: Summary statistics
        weight_diagnostics: Weight diagnostics
        args: Command-line arguments

    Returns:
        Dictionary ready for export
    """
    best_policy = summary_data.get("best_policy")
    best_diag = weight_diagnostics.get(best_policy) if best_policy else None

    output_data = {
        "timestamp": datetime.now().isoformat(),
        "dataset": {
            "path": args.data,
            "n_samples": dataset.n_samples,
            "target_policies": dataset.target_policies,
        },
        "estimation": {
            "estimator": args.estimator,
            "estimator_config": args.estimator_config,
            "policies": {},
        },
        "best_policy": best_policy,
        "weight_diagnostics": {},
    }

    # Add weight diagnostics for best policy
    if best_diag:
        output_data["weight_diagnostics"]["best_policy"] = {
            "ess_fraction": float(best_diag.get("ess_fraction", 1.0)),
            "max_weight": float(best_diag.get("max_weight", 1.0)),
            "mean_weight": float(best_diag.get("mean_weight", 1.0)),
        }

    # Add base policy results
    output_data["estimation"]["policies"]["base"] = {
        "estimate": float(summary_data["base_mean"]),
        "standard_error": float(summary_data["base_se"]),
        "ci_lower": float(summary_data["base_ci_lower"]),
        "ci_upper": float(summary_data["base_ci_upper"]),
    }

    # Add target policy results
    ci_lower, ci_upper = results.confidence_interval(alpha=0.05)
    for policy, estimate, se, ci_l, ci_u in zip(
        summary_data["target_policies"],
        results.estimates,
        results.standard_errors,
        ci_lower,
        ci_upper,
    ):
        output_data["estimation"]["policies"][policy] = {
            "estimate": float(estimate),
            "standard_error": float(se),
            "ci_lower": float(ci_l),
            "ci_upper": float(ci_u),
        }

    # Add diagnostic information if available
    if hasattr(results, "diagnostics") and results.diagnostics:
        output_data["diagnostics"] = _extract_diagnostics(results.diagnostics)

    return output_data


def _export_to_json(output_data: Dict[str, Any], output_path: Path) -> None:
    """Export data to JSON format.

    Args:
        output_data: Data to export
        output_path: Path to output file
    """
    with open(output_path, "w") as f:
        json.dump(output_data, f, indent=2)


def _export_to_csv(output_data: Dict[str, Any], output_path: Path) -> None:
    """Export data to CSV format.

    Args:
        output_data: Data to export
        output_path: Path to output file
    """
    import csv

    # Flatten the nested structure for CSV
    rows = []

    # Header row
    header = [
        "policy",
        "estimate",
        "standard_error",
        "ci_lower",
        "ci_upper",
        "is_best",
    ]

    # Add data rows
    best_policy = output_data.get("best_policy")
    for policy, data in output_data["estimation"]["policies"].items():
        row = [
            policy,
            data["estimate"],
            data["standard_error"],
            data["ci_lower"],
            data["ci_upper"],
            "Yes" if policy == best_policy else "No",
        ]
        rows.append(row)

    # Write CSV
    with open(output_path, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(header)
        writer.writerows(rows)


def _extract_diagnostics(diagnostics: Any) -> Dict[str, Any]:
    """Extract key diagnostic information for export.

    Args:
        diagnostics: Diagnostics object (IPSDiagnostics or DRDiagnostics)

    Returns:
        Dictionary of diagnostic information
    """
    diag_dict = {}

    # Common fields
    if hasattr(diagnostics, "n_samples_total"):
        diag_dict["n_samples_total"] = diagnostics.n_samples_total
    if hasattr(diagnostics, "n_samples_valid"):
        diag_dict["n_samples_valid"] = diagnostics.n_samples_valid
    if hasattr(diagnostics, "weight_ess"):
        diag_dict["overall_ess"] = float(diagnostics.weight_ess)

    # IPS-specific fields
    if hasattr(diagnostics, "ess_per_policy"):
        diag_dict["ess_per_policy"] = {
            k: float(v) for k, v in diagnostics.ess_per_policy.items()
        }
    if hasattr(diagnostics, "max_weight_per_policy"):
        diag_dict["max_weight_per_policy"] = {
            k: float(v) for k, v in diagnostics.max_weight_per_policy.items()
        }

    # DR-specific fields
    if hasattr(diagnostics, "dr_cross_fitted"):
        diag_dict["dr_cross_fitted"] = diagnostics.dr_cross_fitted
    if hasattr(diagnostics, "dr_n_folds"):
        diag_dict["dr_n_folds"] = diagnostics.dr_n_folds
    if hasattr(diagnostics, "outcome_r2_range"):
        diag_dict["outcome_r2_range"] = list(diagnostics.outcome_r2_range)
    if hasattr(diagnostics, "worst_if_tail_ratio"):
        diag_dict["worst_if_tail_ratio"] = float(diagnostics.worst_if_tail_ratio)

    # Calibration fields
    if hasattr(diagnostics, "calibration_rmse"):
        diag_dict["calibration_rmse"] = (
            float(diagnostics.calibration_rmse)
            if diagnostics.calibration_rmse is not None
            else None
        )
    if hasattr(diagnostics, "calibration_r2"):
        diag_dict["calibration_r2"] = (
            float(diagnostics.calibration_r2)
            if diagnostics.calibration_r2 is not None
            else None
        )

    return diag_dict


=== ./cje/experiments/arena_10k_simplified/analysis/loading.py ===

"""Data loading and validation for CJE analysis.

This module handles loading datasets and basic validation.
Following CLAUDE.md: Do one thing well - this only loads data.
"""

from pathlib import Path
from typing import Any
from cje import load_dataset_from_jsonl


def load_data(data_path: str, verbose: bool = True) -> Any:
    """Load dataset from JSONL file.

    Args:
        data_path: Path to dataset file
        verbose: Whether to print loading status

    Returns:
        Loaded dataset

    Raises:
        FileNotFoundError: If data file doesn't exist
        ValueError: If dataset is invalid
    """
    data_file = Path(data_path)
    if not data_file.exists():
        raise FileNotFoundError(f"Dataset not found: {data_path}")

    if verbose:
        print("\n1. Loading dataset...")

    dataset = load_dataset_from_jsonl(data_path)

    if verbose:
        print(f"   ✓ Loaded {dataset.n_samples} samples")
        print(f"   ✓ Target policies: {dataset.target_policies}")

    # Basic validation
    if dataset.n_samples == 0:
        raise ValueError("Dataset is empty")

    if not dataset.target_policies:
        raise ValueError("No target policies found in dataset")

    return dataset


=== ./cje/experiments/arena_10k_simplified/analysis/multiple_passes/analyze_variance_decomposition.py ===

#!/usr/bin/env python3
"""
Analyze variance decomposition in multiple teacher forcing passes.

This canonical script demonstrates that 99.9% of log probability variance
occurs between prompts (not within), validating block bootstrap approaches.

Usage:
    python analyze_variance_decomposition.py

Outputs:
    ../../paper_plots/variance_decomposition.pdf - Figure for paper
    ../../paper_plots/variance_decomposition.png - Preview version
"""

import json
import numpy as np
from pathlib import Path
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# Set clean style
plt.style.use("seaborn-v0_8-whitegrid")
sns.set_palette("husl")


from typing import Dict, List, Any, DefaultDict


def load_all_passes(
    logprobs_dir: Path, policies: List[str], n_passes: int = 5
) -> Dict[str, Dict[str, List[float]]]:
    """Load all passes for all policies."""
    data: DefaultDict[str, DefaultDict[str, List[float]]] = defaultdict(
        lambda: defaultdict(list)
    )

    for policy in policies:
        for pass_num in range(1, n_passes + 1):
            if pass_num == 1:
                file_path = logprobs_dir / f"{policy}_logprobs.jsonl"
            else:
                file_path = logprobs_dir / f"{policy}_logprobs_pass{pass_num}.jsonl"

            if file_path.exists():
                with open(file_path, "r") as f:
                    for line in f:
                        entry = json.loads(line)
                        prompt_id = entry.get("prompt_id")
                        logprob = entry.get("logprob")
                        if prompt_id and logprob is not None and logprob <= 0:
                            data[policy][prompt_id].append(logprob)

    return dict(data)


def compute_variance_components(
    data: Dict[str, Dict[str, List[float]]],
) -> Dict[str, Dict[str, Any]]:
    """Compute within and between prompt variance for each policy."""
    results = {}

    for policy, prompts in data.items():
        # Get prompts with all 5 passes
        full_prompts = {pid: lps for pid, lps in prompts.items() if len(lps) == 5}

        if len(full_prompts) < 100:
            continue

        # Create matrix: rows = prompts, columns = passes
        matrix = np.array([lps for lps in full_prompts.values()])
        n_prompts, n_passes = matrix.shape

        # Compute variance components
        grand_mean = matrix.mean()
        prompt_means = matrix.mean(axis=1)

        # Between-prompt variance
        var_between = np.var(prompt_means)

        # Within-prompt variance (average across prompts)
        var_within = np.mean([np.var(row) for row in matrix])

        # Proportion of variance between prompts
        total_var = var_between + var_within
        prop_between = var_between / total_var if total_var > 0 else 0

        results[policy] = {
            "var_between": var_between,
            "var_within": var_within,
            "prop_between": prop_between,
            "n_prompts": n_prompts,
        }

    return results


def create_simple_figure(
    data: Dict[str, Dict[str, List[float]]], var_results: Dict[str, Dict[str, Any]]
) -> plt.Figure:
    """Create a simple, clear 2-panel figure."""

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # Define nice colors
    colors = {
        "base": "#2E7D32",
        "clone": "#1976D2",
        "parallel_universe_prompt": "#F57C00",
        "premium": "#C62828",
        "unhelpful": "#6A1B9A",
    }

    labels = {
        "base": "Base",
        "clone": "Clone",
        "parallel_universe_prompt": "Parallel",
        "premium": "Premium",
        "unhelpful": "Unhelpful",
    }

    # Panel A: Variance decomposition
    policies = []
    between_props = []
    within_props = []

    for policy in ["base", "clone", "parallel_universe_prompt", "premium", "unhelpful"]:
        if policy in var_results:
            policies.append(labels[policy])
            between_props.append(var_results[policy]["prop_between"] * 100)
            within_props.append((1 - var_results[policy]["prop_between"]) * 100)

    x = np.arange(len(policies))
    width = 0.6

    # Stack the bars
    p1 = ax1.bar(
        x, between_props, width, label="Between-prompt", color="#1976D2", alpha=0.8
    )
    p2 = ax1.bar(
        x,
        within_props,
        width,
        bottom=between_props,
        label="Within-prompt",
        color="#FF6B6B",
        alpha=0.8,
    )

    ax1.set_ylabel("Variance (%)", fontsize=14)
    ax1.set_title("A. Where is the variance?", fontsize=15, fontweight="bold")
    ax1.set_xticks(x)
    ax1.set_xticklabels(policies, fontsize=12)
    ax1.set_ylim([0, 105])
    ax1.legend(loc="upper right", fontsize=12)
    ax1.grid(True, alpha=0.3, axis="y")

    # Add percentage labels
    for i, (between, within) in enumerate(zip(between_props, within_props)):
        ax1.text(
            i,
            between / 2,
            f"{between:.1f}%",
            ha="center",
            va="center",
            fontsize=11,
            color="white",
            fontweight="bold",
        )
        if within > 1:  # Only show if visible
            ax1.text(
                i,
                between + within / 2,
                f"{within:.1f}%",
                ha="center",
                va="center",
                fontsize=10,
                color="white",
            )

    # Panel B: Example showing 5 passes for selected prompts
    # Show how passes cluster tightly within prompts but vary across prompts

    # Select a few representative prompts
    policy = "parallel_universe_prompt"  # Most variable policy
    prompt_data = data[policy]

    # Get prompts with all 5 passes and select a diverse sample
    full_prompts = [(pid, lps) for pid, lps in prompt_data.items() if len(lps) == 5]

    if len(full_prompts) < 5:
        # Fallback if not enough data
        print(f"Warning: Only {len(full_prompts)} prompts with 5 passes for {policy}")
        return fig

    full_prompts.sort(key=lambda x: np.mean(x[1]))  # Sort by mean logprob

    # Select prompts at different percentiles (but ensure indices are valid)
    n_total = len(full_prompts)
    percentiles = [0.1, 0.3, 0.5, 0.7, 0.9]
    indices = []
    for p in percentiles:
        idx = min(int(n_total * p), n_total - 1)
        indices.append(idx)
    selected = [full_prompts[i] for i in indices]

    # Plot
    x_pos = 0
    x_positions = []
    x_labels = []

    for i, (prompt_id, logprobs) in enumerate(selected):
        # Plot the 5 passes as points
        passes_x = [x_pos] * 5
        ax2.scatter(
            passes_x,
            logprobs,
            alpha=0.6,
            s=50,
            color=colors["parallel_universe_prompt"],
        )

        # Add a line showing the range
        ax2.plot(
            [x_pos - 0.1, x_pos + 0.1],
            [np.mean(logprobs)] * 2,
            "k-",
            linewidth=2,
            alpha=0.8,
        )

        # Store position
        x_positions.append(x_pos)
        x_labels.append(f"Prompt {i+1}")
        x_pos += 1

    # Add connecting lines to show between-prompt variance
    all_means = [np.mean(lps) for _, lps in selected]
    ax2.plot(x_positions, all_means, "k--", alpha=0.3, linewidth=1)

    ax2.set_xlabel("Different Prompts", fontsize=14)
    ax2.set_ylabel("Log Probability", fontsize=14)
    ax2.set_title(
        "B. Five passes per prompt (Parallel policy)", fontsize=15, fontweight="bold"
    )
    ax2.set_xticks(x_positions)
    ax2.set_xticklabels(x_labels, fontsize=12)
    ax2.grid(True, alpha=0.3)

    # Add legend
    ax2.scatter(
        [],
        [],
        alpha=0.6,
        s=50,
        color=colors["parallel_universe_prompt"],
        label="Individual passes",
    )
    ax2.plot([], [], "k-", linewidth=2, alpha=0.8, label="Prompt mean")
    ax2.legend(loc="upper right", fontsize=11)

    # Add annotation
    ax2.text(
        0.02,
        0.98,
        "Within-prompt variance: ~0.3%\nBetween-prompt variance: ~99.7%",
        transform=ax2.transAxes,
        fontsize=11,
        verticalalignment="top",
        bbox=dict(boxstyle="round", facecolor="white", alpha=0.9),
    )

    plt.suptitle(
        "Multiple Teacher Forcing Passes: Variance Decomposition",
        fontsize=16,
        fontweight="bold",
        y=1.02,
    )
    plt.tight_layout()

    return fig


def main() -> None:
    """Create simple visualization and save."""

    print("Loading data...")
    logprobs_dir = Path("../../data/logprobs")
    policies = ["base", "clone", "parallel_universe_prompt", "premium", "unhelpful"]

    # Load data
    data = load_all_passes(logprobs_dir, policies)

    # Compute variance components
    print("Computing variance components...")
    var_results = compute_variance_components(data)

    # Print key statistics
    print("\nKEY FINDING: Variance Decomposition")
    print("=" * 50)
    for policy in policies:
        if policy in var_results:
            r = var_results[policy]
            print(
                f"{policy:30s}: {r['prop_between']*100:5.1f}% between prompts ({r['n_prompts']} prompts analyzed)"
            )

    # Create figure
    print("\nCreating figure...")
    fig = create_simple_figure(data, var_results)

    # Save to paper_plots directory
    output_dir = Path("../../paper_plots")
    output_dir.mkdir(exist_ok=True)

    fig.savefig(output_dir / "variance_decomposition.pdf", dpi=300, bbox_inches="tight")
    fig.savefig(output_dir / "variance_decomposition.png", dpi=150, bbox_inches="tight")
    print(f"Saved: {output_dir}/variance_decomposition.pdf and .png")


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/analysis/results.py ===

"""Results formatting and display for CJE analysis.

This module handles formatting estimation results, computing statistics,
and displaying them in a clear, readable format.

Following CLAUDE.md: Do one thing well - this module only handles results display.
"""

import numpy as np
from typing import Any, Dict, List, Tuple
from pathlib import Path
import sys

# Note: oracle_comparison module doesn't exist - functions commented out
# from ..oracle_comparison import (
#     load_oracle_ground_truth as local_load_oracle_ground_truth,
#     compare_estimates_to_oracle,
#     format_oracle_comparison_table,
# )


def display_results(
    results: Any,
    calibrated_dataset: Any,
    sampler: Any,
    estimator: Any,
    args: Any,
    dataset: Any,
) -> Dict[str, Any]:
    """Display analysis results and return summary data.

    Args:
        results: EstimationResult object
        calibrated_dataset: Dataset with calibrated rewards
        sampler: PrecomputedSampler
        estimator: Fitted estimator
        args: Command-line arguments
        dataset: Original dataset (for oracle comparison)

    Returns:
        Dictionary with summary statistics and best policy
    """
    target_policies = list(sampler.target_policies)
    base_mean, base_se, base_ci_lower, base_ci_upper = compute_base_statistics(
        calibrated_dataset
    )

    print("\n4. Results:")
    print("   " + "-" * 40)

    # Base policy
    print(f"   base (observed):")
    print(f"     Estimate: {base_mean:.3f}")
    print(f"     Std Error: {base_se:.3f}")
    print(f"     95% CI: [{base_ci_lower:.3f}, {base_ci_upper:.3f}]")

    # Target policies
    ci_lower, ci_upper = results.confidence_interval(alpha=0.05)
    for policy, estimate, se, ci_l, ci_u in zip(
        target_policies, results.estimates, results.standard_errors, ci_lower, ci_upper
    ):
        print(f"   {policy}:")
        print(f"     Estimate: {estimate:.3f}")
        print(f"     Std Error: {se:.3f}")
        print(f"     95% CI: [{ci_l:.3f}, {ci_u:.3f}]")

    # Best policy (handle NaN values properly)
    all_estimates = [base_mean] + list(results.estimates)
    all_policies = ["base"] + target_policies

    # Filter out NaN values for best policy selection
    valid_estimates = [
        (est, pol) for est, pol in zip(all_estimates, all_policies) if not np.isnan(est)
    ]

    if valid_estimates:
        best_estimate, best_policy = max(valid_estimates, key=lambda x: x[0])
        print(f"\n   🏆 Best policy: {best_policy}")
    else:
        print(f"\n   ⚠️ No valid estimates available for best policy selection")
        best_policy = None

    # Add sanity check for extreme estimates
    _check_extreme_estimates(all_policies, all_estimates, base_mean, estimator)

    # Oracle comparison if available
    if args.oracle_field in dataset.samples[0].metadata:
        _display_oracle_comparison(args, dataset, target_policies, base_mean, results)

    return {
        "best_policy": best_policy,
        "base_mean": base_mean,
        "base_se": base_se,
        "base_ci_lower": base_ci_lower,
        "base_ci_upper": base_ci_upper,
        "target_policies": target_policies,
    }


def compute_base_statistics(
    calibrated_dataset: Any,
) -> Tuple[float, float, float, float]:
    """Compute base policy statistics.

    Args:
        calibrated_dataset: Dataset with rewards

    Returns:
        Tuple of (mean, standard_error, ci_lower, ci_upper)
    """
    base_rewards = [
        s.reward for s in calibrated_dataset.samples if s.reward is not None
    ]
    base_mean = sum(base_rewards) / len(base_rewards) if base_rewards else 0.0
    base_se = (
        np.std(base_rewards, ddof=1) / np.sqrt(len(base_rewards))
        if len(base_rewards) > 1
        else 0.0
    )
    base_ci_lower = base_mean - 1.96 * base_se
    base_ci_upper = base_mean + 1.96 * base_se
    return base_mean, base_se, base_ci_lower, base_ci_upper


def _check_extreme_estimates(
    all_policies: List[str],
    all_estimates: List[float],
    base_mean: float,
    estimator: Any,
) -> None:
    """Check for extreme estimates that may indicate problems.

    Args:
        all_policies: List of all policy names
        all_estimates: List of all estimates
        base_mean: Base policy mean
        estimator: Fitted estimator (for getting weights)
    """
    EXTREME_DIFF_THRESHOLD = 0.3  # >30% difference is suspicious
    NEAR_ZERO_WEIGHT_THRESHOLD = 1e-10
    EXTREME_CONCENTRATION_THRESHOLD = 0.9

    for policy, estimate in zip(all_policies, all_estimates):
        if (
            not np.isnan(estimate)
            and abs(estimate - base_mean) > EXTREME_DIFF_THRESHOLD
        ):
            print(
                f"\n   ⚠️ WARNING: {policy} estimate ({estimate:.3f}) differs greatly from base ({base_mean:.3f})"
            )
            print(
                f"      This may indicate estimation failure or extreme distribution shift"
            )
            if policy != "base":
                # Check weight concentration for this policy
                weights = estimator.get_weights(policy)
                if weights is not None:
                    near_zero = np.sum(weights < NEAR_ZERO_WEIGHT_THRESHOLD) / len(
                        weights
                    )
                    if near_zero > EXTREME_CONCENTRATION_THRESHOLD:
                        print(
                            f"      Likely cause: {near_zero:.1%} of samples have near-zero weight"
                        )


def _display_oracle_comparison(
    args: Any,
    dataset: Any,
    target_policies: List[str],
    base_mean: float,
    results: Any,
) -> None:
    """Display comparison with oracle ground truth if available.

    Args:
        args: Command-line arguments
        dataset: Original dataset with oracle labels
        target_policies: List of target policies
        base_mean: Base policy mean
        results: EstimationResult object
    """
    print(f"\n   📊 Oracle Ground Truth Comparison:")
    oracle_means = load_oracle_ground_truth(args, dataset, target_policies)

    if oracle_means:
        # Build estimates dictionary including base
        all_estimates_dict = {"base": base_mean}
        for i, policy in enumerate(target_policies):
            all_estimates_dict[policy] = results.estimates[i]

        # Compare estimates to oracle
        comparison = compare_estimates_to_oracle(
            all_estimates_dict, oracle_means, results
        )
        formatted_table = format_oracle_comparison_table(comparison, precision=3)
        for line in formatted_table.split("\n"):
            print(f"   {line}")


def load_oracle_ground_truth(
    args: Any, dataset: Any, target_policies: List[str]
) -> Dict[str, float]:
    """Load oracle ground truth values for comparison.

    Args:
        args: Command-line arguments (contains data path and oracle field)
        dataset: Dataset object
        target_policies: List of target policies

    Returns:
        Dictionary mapping policy names to oracle mean values
    """
    from pathlib import Path
    import json

    oracle_means = {}
    data_dir = Path(args.data).parent
    responses_dir = data_dir / "responses"

    # Load base policy oracle labels from dataset
    base_oracle_values = []
    for sample in dataset.samples:
        if hasattr(sample, "metadata") and sample.metadata:
            oracle_val = sample.metadata.get(args.oracle_field)
            if oracle_val is not None:
                base_oracle_values.append(oracle_val)

    if base_oracle_values:
        oracle_means["base"] = float(np.mean(base_oracle_values))

    # Load oracle labels for each target policy from response files
    for policy in target_policies:
        response_file = responses_dir / f"{policy}_responses.jsonl"
        if response_file.exists():
            oracle_values = []
            with open(response_file, "r") as f:
                for line in f:
                    try:
                        data = json.loads(line)
                        if "metadata" in data and args.oracle_field in data["metadata"]:
                            oracle_val = data["metadata"][args.oracle_field]
                            if oracle_val is not None:
                                oracle_values.append(oracle_val)
                    except json.JSONDecodeError:
                        continue

            if oracle_values:
                oracle_means[policy] = float(np.mean(oracle_values))

    return oracle_means


def compare_estimates_to_oracle(
    estimates: Dict[str, float], oracle_means: Dict[str, float], results: Any
) -> Dict[str, Dict[str, Any]]:
    """Compare CJE estimates to oracle ground truth.

    Args:
        estimates: Dictionary of policy -> estimate
        oracle_means: Dictionary of policy -> oracle mean
        results: EstimationResult object (for standard errors)

    Returns:
        Dictionary of policy -> comparison stats
    """
    comparison = {}

    # Get standard errors if available
    se_dict = {}
    if hasattr(results, "standard_errors") and results.standard_errors is not None:
        for i, policy in enumerate(estimates.keys()):
            if policy != "base":
                idx = list(estimates.keys()).index(policy) - 1  # Adjust for base
                if idx < len(results.standard_errors):
                    se_dict[policy] = results.standard_errors[idx]

    for policy in estimates:
        if policy in oracle_means:
            estimate = estimates[policy]
            oracle = oracle_means[policy]
            diff = estimate - oracle

            comparison[policy] = {
                "estimate": estimate,
                "oracle": oracle,
                "difference": diff,
                "abs_difference": abs(diff),
                "relative_error": (
                    abs(diff) / abs(oracle) if oracle != 0 else float("inf")
                ),
            }

            # Add coverage check if we have standard errors
            if policy in se_dict:
                se = se_dict[policy]
                comparison[policy]["se"] = se
                # Check if oracle is within 95% CI
                ci_lower = estimate - 1.96 * se
                ci_upper = estimate + 1.96 * se
                comparison[policy]["oracle_in_ci"] = ci_lower <= oracle <= ci_upper
                comparison[policy]["ci_lower"] = float(ci_lower)
                comparison[policy]["ci_upper"] = float(ci_upper)

    return comparison


def format_oracle_comparison_table(
    comparison: Dict[str, Dict[str, Any]], precision: int = 3
) -> str:
    """Format oracle comparison as a readable table.

    Args:
        comparison: Dictionary from compare_estimates_to_oracle
        precision: Number of decimal places

    Returns:
        Formatted table string
    """
    lines = []

    # Header
    lines.append("-" * 80)
    lines.append(
        f"{'Policy':<25} {'Estimate':<12} {'Oracle':<12} {'Diff':<10} {'In CI?':<8}"
    )
    lines.append("-" * 80)

    # Sort policies (base first if present)
    policies = sorted(comparison.keys())
    if "base" in policies:
        policies.remove("base")
        policies = ["base"] + policies

    # Data rows
    for policy in policies:
        stats = comparison[policy]
        estimate = f"{stats['estimate']:.{precision}f}"
        oracle = f"{stats['oracle']:.{precision}f}"
        diff = f"{stats['difference']:+.{precision}f}"

        # Check if oracle is in CI
        in_ci = ""
        if "oracle_in_ci" in stats:
            in_ci = "✓" if stats["oracle_in_ci"] else "✗"

        lines.append(f"{policy:<25} {estimate:<12} {oracle:<12} {diff:<10} {in_ci:<8}")

    lines.append("-" * 80)

    # Summary statistics
    all_diffs = [abs(stats["difference"]) for stats in comparison.values()]
    mean_abs_error = np.mean(all_diffs)
    max_abs_error = np.max(all_diffs)

    lines.append(f"Mean Absolute Error: {mean_abs_error:.{precision}f}")
    lines.append(f"Max Absolute Error: {max_abs_error:.{precision}f}")

    # Coverage if available
    coverage_stats = [stats.get("oracle_in_ci", None) for stats in comparison.values()]
    coverage_stats = [x for x in coverage_stats if x is not None]
    if coverage_stats:
        coverage = sum(coverage_stats) / len(coverage_stats) * 100
        lines.append(
            f"95% CI Coverage: {coverage:.1f}% ({sum(coverage_stats)}/{len(coverage_stats)})"
        )

    return "\n".join(lines)


=== ./cje/experiments/arena_10k_simplified/analysis/visualization.py ===

"""Visualization generation for CJE analysis.

This module handles generating all visualization plots and dashboards.

Following CLAUDE.md: Do one thing well - this module only handles visualization.
"""

import numpy as np
from pathlib import Path
from typing import Any, Dict, Optional
import warnings

# Check if visualization is available
try:
    import matplotlib.pyplot as plt
    from cje.visualization import (
        plot_weight_dashboard_summary,
        plot_weight_dashboard_detailed,
        plot_calibration_comparison,
        plot_policy_estimates,
        plot_dr_dashboard,
    )

    VIZ_AVAILABLE = True
except ImportError:
    VIZ_AVAILABLE = False
    warnings.warn(
        "Visualization dependencies not available. Install with: pip install cje[viz]"
    )

from .results import load_oracle_ground_truth


def generate_visualizations(
    results: Any,
    dataset: Any,
    calibrated_dataset: Any,
    estimator: Any,
    sampler: Any,
    args: Any,
    summary_data: Dict[str, Any],
    cal_result: Optional[Any] = None,
) -> None:
    """Generate all visualization plots.

    Args:
        results: EstimationResult object
        dataset: Original dataset (for oracle comparison)
        calibrated_dataset: Dataset with calibrated rewards
        estimator: Fitted estimator
        sampler: PrecomputedSampler
        args: Command-line arguments
        summary_data: Summary statistics from results
        cal_result: Optional calibration result
    """
    if not VIZ_AVAILABLE or args.no_plots:
        return

    plot_dir = (
        Path(args.plot_dir) if args.plot_dir else Path(args.data).parent / "plots"
    )
    step_num = 8 if args.estimator in ["dr-cpo", "mrdr", "tmle"] else 7
    print(f"\n{step_num}. Generating visualizations in {plot_dir}/...")
    plot_dir.mkdir(parents=True, exist_ok=True)

    try:
        # Generate weight dashboards
        _generate_weight_dashboards(estimator, sampler, results, plot_dir, cal_result)

        # Generate calibration comparison
        _generate_calibration_comparison(
            dataset, calibrated_dataset, args, cal_result, plot_dir
        )

        # Generate policy estimates plot
        _generate_policy_estimates(dataset, results, args, summary_data, plot_dir)

        # Generate DR dashboard if applicable
        _generate_dr_dashboard(results, args, plot_dir)

        # Close all figures to free memory
        plt.close("all")

    except Exception as e:
        print(f"   ⚠️  Failed to generate some plots: {e}")
        if args.debug:
            import traceback

            traceback.print_exc()


def _generate_weight_dashboards(
    estimator: Any,
    sampler: Any,
    results: Any,
    plot_dir: Path,
    cal_result: Optional[Any],
) -> None:
    """Generate weight dashboard visualizations.

    Args:
        estimator: Fitted estimator
        sampler: PrecomputedSampler
        results: EstimationResult object
        plot_dir: Directory to save plots
        cal_result: Optional calibration result
    """
    raw_weights_dict = {}
    calibrated_weights_dict = {}

    for policy in sampler.target_policies:
        weights = estimator.get_weights(policy)
        if weights is not None:
            calibrated_weights_dict[policy] = weights

        raw_weights = estimator.get_raw_weights(policy)
        if raw_weights is not None:
            raw_weights_dict[policy] = raw_weights
        elif weights is not None:
            raw_weights_dict[policy] = weights

    if raw_weights_dict and calibrated_weights_dict:
        # Generate combined overview dashboard (6-panel summary)
        fig, _ = plot_weight_dashboard_summary(
            raw_weights_dict,
            calibrated_weights_dict,
            n_samples=sampler.n_valid_samples,
            save_path=plot_dir / "weight_dashboard",
            diagnostics=results.diagnostics,
        )
        print(f"   ✓ Weight dashboard → {plot_dir}/weight_dashboard.png")
        plt.close(fig)

        # Generate per-policy detailed dashboard
        fig, _ = plot_weight_dashboard_detailed(
            raw_weights_dict,
            calibrated_weights_dict,
            n_samples=sampler.n_valid_samples,
            save_path=plot_dir / "weight_dashboard_per_policy",
            diagnostics=results.diagnostics,
            sampler=sampler,
            calibrator=cal_result.calibrator if cal_result else None,
        )
        print(f"   ✓ Per-policy dashboard → {plot_dir}/weight_dashboard_per_policy.png")
        plt.close(fig)


def _generate_calibration_comparison(
    dataset: Any,
    calibrated_dataset: Any,
    args: Any,
    cal_result: Optional[Any],
    plot_dir: Path,
) -> None:
    """Generate calibration comparison plot.

    Args:
        dataset: Original dataset
        calibrated_dataset: Dataset with calibrated rewards
        args: Command-line arguments
        cal_result: Optional calibration result
        plot_dir: Directory to save plots
    """
    # Use the ORIGINAL dataset (before masking) to get all judge/oracle pairs
    # All samples in the logged dataset are from the base policy by definition
    # (fresh draws would be in separate FreshDrawDataset objects)
    judge_scores = []
    oracle_labels = []
    for s in dataset.samples:
        # All logged samples are base policy samples
        if args.judge_field in s.metadata and args.oracle_field in s.metadata:
            j_score = s.metadata.get(args.judge_field)
            o_label = s.metadata.get(args.oracle_field)
            if j_score is not None and o_label is not None:
                judge_scores.append(j_score)
                oracle_labels.append(o_label)

    if judge_scores and oracle_labels:
        # Get calibrated rewards if calibration was performed
        calibrated_preds = None
        if cal_result is not None:  # Calibration was performed
            # Get rewards for ALL samples (calibration was done on partial oracle labels)
            calibrated_preds_list = []
            for s_orig, s_cal in zip(dataset.samples, calibrated_dataset.samples):
                # Match samples from original dataset (with all oracle labels)
                # to calibrated dataset (with rewards)
                if (
                    args.judge_field in s_orig.metadata
                    and args.oracle_field in s_orig.metadata
                    and s_cal.reward is not None
                ):
                    calibrated_preds_list.append(s_cal.reward)

            # Use calibrated scores if we have them for all samples
            if len(calibrated_preds_list) == len(judge_scores):
                calibrated_preds = calibrated_preds_list

        fig = plot_calibration_comparison(
            judge_scores=np.array(judge_scores),
            oracle_labels=np.array(oracle_labels),
            calibrated_scores=(
                np.array(calibrated_preds) if calibrated_preds else None
            ),
            save_path=plot_dir / "calibration_comparison",
        )
        print(f"   ✓ Calibration comparison → {plot_dir}/calibration_comparison.png")
        plt.close(fig)


def _generate_policy_estimates(
    dataset: Any,
    results: Any,
    args: Any,
    summary_data: Dict[str, Any],
    plot_dir: Path,
) -> None:
    """Generate policy estimates forest plot.

    Args:
        dataset: Original dataset
        results: EstimationResult object
        args: Command-line arguments
        summary_data: Summary statistics from results
        plot_dir: Directory to save plots
    """
    policy_estimates = {"base": summary_data["base_mean"]}
    policy_ses = {"base": summary_data["base_se"]}

    for policy, estimate, se in zip(
        summary_data["target_policies"], results.estimates, results.standard_errors
    ):
        if not np.isnan(estimate):
            policy_estimates[policy] = estimate
            policy_ses[policy] = se

    # Try to get oracle values
    oracle_values = None
    if args.oracle_field in dataset.samples[0].metadata:
        oracle_values = load_oracle_ground_truth(
            args, dataset, summary_data["target_policies"]
        )

    fig = plot_policy_estimates(
        estimates=policy_estimates,
        standard_errors=policy_ses,
        oracle_values=oracle_values,
        base_policy="base",
        save_path=plot_dir / "policy_estimates",
    )
    print(f"   ✓ Policy estimates → {plot_dir}/policy_estimates.png")
    plt.close(fig)


def _generate_dr_dashboard(
    results: Any,
    args: Any,
    plot_dir: Path,
) -> None:
    """Generate DR dashboard if applicable.

    Args:
        results: EstimationResult object
        args: Command-line arguments
        plot_dir: Directory to save plots
    """
    if (
        args.estimator in ["dr-cpo", "mrdr", "tmle"]
        and "dr_diagnostics" in results.metadata
    ):
        try:
            fig, _ = plot_dr_dashboard(results)
            fig.savefig(plot_dir / "dr_dashboard.png", dpi=150, bbox_inches="tight")
            print(f"   ✓ DR dashboard → {plot_dir}/dr_dashboard.png")
            plt.close(fig)

        except Exception as e:
            print(f"   ⚠️  Could not generate DR dashboard: {e}")


=== ./cje/experiments/arena_10k_simplified/analyze_dataset.py ===

#!/usr/bin/env python3
"""CJE Analysis Pipeline - Clean Orchestrator.

This is a thin orchestrator that coordinates the analysis pipeline modules.
Each module does one thing well, following the Unix philosophy.

Usage:
    python analyze_refactored.py --data path/to/dataset.jsonl --estimator calibrated-ips
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

# CJE imports
from cje.data.precomputed_sampler import PrecomputedSampler

# Analysis pipeline modules
from analysis import (
    load_data,
    handle_rewards,
    restore_oracle_labels,
    create_estimator,
    display_results,
    display_weight_diagnostics,
    display_dr_diagnostics,
    display_augmentation_diagnostics,
    analyze_extreme_weights_report,
    generate_visualizations,
    export_results,
)

# CF-bits imports
from cje.cfbits.playbooks import cfbits_report_fresh_draws, cfbits_report_logging_only


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Run CJE analysis on Arena data",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Data arguments
    parser.add_argument(
        "--data",
        default="data/cje_dataset.jsonl",
        help="Path to CJE dataset",
    )

    # Estimator arguments
    parser.add_argument(
        "--estimator",
        default="calibrated-ips",
        choices=[
            "calibrated-ips",
            "raw-ips",
            "dr-cpo",
            "mrdr",
            "tmle",
            "stacked-dr",
        ],
        help="Estimator to use",
    )
    parser.add_argument(
        "--estimator-config",
        type=json.loads,
        help="JSON config for estimator",
    )

    # Oracle/calibration arguments
    parser.add_argument(
        "--use-oracle",
        action="store_true",
        help="Use oracle labels directly as rewards",
    )
    parser.add_argument(
        "--oracle-coverage",
        type=float,
        default=1.0,
        help="Fraction of oracle labels to use for calibration",
    )
    parser.add_argument(
        "--judge-field",
        default="judge_score",
        help="Field name for judge scores",
    )
    parser.add_argument(
        "--oracle-field",
        default="oracle_label",
        help="Field name for oracle labels",
    )
    parser.add_argument(
        "--n-folds",
        type=int,
        default=5,
        help="Number of cross-fitting folds",
    )

    # Output arguments
    parser.add_argument(
        "--output",
        type=str,
        help="Output file path for results (JSON or CSV)",
    )
    parser.add_argument(
        "--plot-dir",
        type=str,
        help="Directory for saving plots",
    )
    parser.add_argument(
        "--no-plots",
        action="store_true",
        help="Disable plot generation",
    )

    # Analysis arguments
    parser.add_argument(
        "--extreme-threshold-low",
        type=float,
        default=1e-10,
        help="Threshold for near-zero weights",
    )
    parser.add_argument(
        "--extreme-threshold-high",
        type=float,
        default=10.0,
        help="Threshold for extreme high weights",
    )
    parser.add_argument(
        "--no-cfbits",
        action="store_true",
        help="Disable CF-bits uncertainty decomposition analysis",
    )
    parser.add_argument(
        "--cfbits-bootstrap",
        type=int,
        default=500,
        help="Number of bootstrap samples for CF-bits confidence intervals (default: 500)",
    )

    # Debug arguments
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress progress messages",
    )

    return parser.parse_args()


def setup_logging(args: argparse.Namespace) -> None:
    """Configure logging based on arguments."""
    if args.debug:
        level = logging.DEBUG
    elif args.quiet:
        level = logging.WARNING
    else:
        level = logging.INFO

    logging.basicConfig(
        level=level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )


def main() -> int:
    """Run the CJE analysis pipeline.

    This orchestrator coordinates the pipeline modules:
    1. Load data
    2. Handle rewards/calibration
    3. Create and fit estimator
    4. Run estimation
    5. Display results
    6. Display diagnostics
    7. Generate visualizations
    8. Export results

    Returns:
        0 on success, 1 on failure
    """
    # Parse arguments
    args = parse_arguments()
    setup_logging(args)

    # Print header
    if not args.quiet:
        print("\n" + "=" * 50)
        print(f"CJE Analysis Pipeline")
        print(f"Estimator: {args.estimator}")
        print(f"Dataset: {args.data}")
        print("=" * 50)

    try:
        # 1. Load data
        dataset = load_data(args.data, verbose=not args.quiet)

        # 2. Handle rewards and calibration
        analysis_config = {
            "n_folds": args.n_folds,
            "oracle_coverage": args.oracle_coverage,
        }
        calibrated_dataset, cal_result = handle_rewards(
            dataset, args, analysis_config, verbose=not args.quiet
        )

        # 3. Create sampler and estimator
        if not args.quiet:
            print("\n3. Setting up estimator...")
        sampler = PrecomputedSampler(calibrated_dataset)
        estimator = create_estimator(args, sampler, calibrated_dataset, cal_result)

        # 4. Fit and run estimation
        if not args.quiet:
            print(f"   ✓ Created {args.estimator} estimator")
            print(f"   Fitting estimator...")
        estimator.fit()

        if not args.quiet:
            print(f"   Running estimation...")
        results = estimator.estimate()

        if not args.quiet:
            print(f"   ✓ Estimation complete")

        # 5. Restore oracle labels for visualization
        # (They were masked during calibration for partial coverage)
        restore_oracle_labels(calibrated_dataset, args)
        # Also restore on original dataset for oracle comparison
        restore_oracle_labels(dataset, args)

        # 6. Display results
        summary_data = display_results(
            results,
            calibrated_dataset,
            sampler,
            estimator,
            args,
            dataset,
        )

        # 7. Display diagnostics
        weight_diagnostics = display_weight_diagnostics(
            estimator, sampler, calibrated_dataset, args
        )

        # Display DR diagnostics if applicable
        if args.estimator in ["dr-cpo", "mrdr", "tmle"]:
            display_dr_diagnostics(results, args)

        # Display augmentation diagnostics (for all estimators)
        display_augmentation_diagnostics(estimator, results, args.oracle_coverage, args)

        # Analyze extreme weights if requested
        if hasattr(estimator, "get_raw_weights"):
            analyze_extreme_weights_report(estimator, sampler, calibrated_dataset, args)

        # Run CF-bits analysis by default (unless disabled)
        cfbits_data = {}
        if not args.no_cfbits:
            if not args.quiet:
                step_num = 9 if args.estimator in ["dr-cpo", "mrdr", "tmle"] else 8
                print(f"\n{step_num}. CF-bits Uncertainty Decomposition:")
                print("   " + "-" * 60)
            
            # Determine which playbook to use
            is_dr = args.estimator in ["dr-cpo", "mrdr", "tmle", "stacked-dr"]
            
            # Run CF-bits for each policy
            for policy in sampler.target_policies:
                if not args.quiet:
                    print(f"\n   {policy}:")
                
                try:
                    if is_dr:
                        # Use fresh draws playbook for DR estimators
                        report = cfbits_report_fresh_draws(
                            estimator=estimator,
                            policy=policy,
                            n_boot=args.cfbits_bootstrap,
                            alpha=0.05
                        )
                    else:
                        # Use logging-only playbook for IPS estimators
                        report = cfbits_report_logging_only(
                            estimator=estimator,
                            policy=policy,
                            n_boot=args.cfbits_bootstrap,
                            alpha=0.05
                        )
                    
                    if report:
                        cfbits_data[policy] = report
                        
                        # Display key metrics
                        cfbits = report.get("cfbits", {})
                        if cfbits:
                            bits_tot = cfbits.get("bits_tot", "N/A")
                            w_tot = cfbits.get("w_tot", "N/A") 
                            w_id = cfbits.get("w_id", "N/A")
                            w_var = cfbits.get("w_var", "N/A")
                            dominant = cfbits.get("dominant", "unknown")
                            if not args.quiet:
                                if isinstance(bits_tot, (int, float)) and isinstance(w_tot, (int, float)):
                                    print(f"     Total bits: {bits_tot:.2f} (width: {w_tot:.3f}, dominant: {dominant})")
                                    if args.debug and isinstance(w_id, (int, float)) and isinstance(w_var, (int, float)):
                                        print(f"       - Wid={w_id:.3f}, Wvar={w_var:.3f}")
                                else:
                                    print(f"     Total bits: {bits_tot} (width: {w_tot}, dominant: {dominant})")
                        
                        # Display overlap with confidence interval
                        overlap = report.get("overlap", {})
                        if overlap and not args.quiet:
                            aessf = overlap.get("aessf")
                            aessf_lcb = overlap.get("aessf_lcb")
                            aessf_ucb = overlap.get("aessf_ucb")
                            if aessf:
                                if args.debug and aessf_lcb and aessf_ucb:
                                    print(f"     A-ESSF: {aessf:.1%} [{aessf_lcb:.1%}, {aessf_ucb:.1%}]")
                                else:
                                    print(f"     A-ESSF: {aessf:.1%} (structural overlap)")
                        
                        # Display efficiency metrics for all estimators
                        efficiency = report.get("efficiency", {})
                        sampling = report.get("sampling_width", {})
                        
                        if efficiency:
                            ifr_main = efficiency.get("ifr_main")
                            ifr_oua = efficiency.get("ifr_oua")
                            if ifr_main is not None and not args.quiet:
                                if ifr_oua is not None and ifr_oua != ifr_main:
                                    print(f"     IFR: {ifr_main:.1%} (main) / {ifr_oua:.1%} (with OUA)")
                                else:
                                    print(f"     IFR: {ifr_main:.1%} (efficiency vs EIF)")
                        elif is_dr and sampling:
                            # Fallback for DR when efficiency not in expected place
                            ifr = sampling.get("IFR_main")
                            if ifr is not None and not args.quiet:
                                print(f"     IFR: {ifr:.1%} (efficiency vs EIF)")
                        
                        # Display gates with detailed reasons
                        gates = report.get("gates", {})
                        if gates and not args.quiet:
                            state = gates.get("state", "UNKNOWN")
                            emoji = {"GOOD": "✅", "WARNING": "⚠️", "CRITICAL": "❌", "REFUSE": "🚫"}.get(state, "?")
                            print(f"     Gates: {emoji} {state}")
                            if state != "GOOD":
                                reasons = gates.get("reasons", [])
                                if reasons:
                                    # Show all reasons in debug mode, first reason otherwise
                                    if args.debug and len(reasons) > 1:
                                        for reason in reasons:
                                            print(f"       - {reason}")
                                    else:
                                        print(f"       Issues: {reasons[0]}")
                                
                                # Show suggestions if available
                                suggestions = gates.get("suggestions", {})
                                if suggestions and args.debug:
                                    first_suggestion = list(suggestions.values())[0] if suggestions else None
                                    if first_suggestion:
                                        print(f"       → {first_suggestion}")
                
                except Exception as e:
                    if args.debug:
                        print(f"     ⚠️ CF-bits failed: {e}")
                    cfbits_data[policy] = {"error": str(e)}
            
            if not args.quiet and cfbits_data:
                # Summary table if we have multiple policies
                if len(cfbits_data) > 1:
                    print("\n   Summary Table:")
                    print("   " + "-" * 75)
                    print(f"   {'Policy':<30} {'A-ESSF':>8} {'IFR':>8} {'Bits':>7} {'Gates':>10}")
                    print("   " + "-" * 75)
                    
                    for pol, rep in cfbits_data.items():
                        if isinstance(rep, dict) and "error" not in rep:
                            overlap = rep.get("overlap", {})
                            aessf = overlap.get("aessf", 0) if overlap else 0
                            
                            efficiency = rep.get("efficiency", {})
                            sampling = rep.get("sampling_width", {})
                            ifr = None
                            if efficiency:
                                ifr = efficiency.get("ifr_main") or efficiency.get("ifr_oua")
                            elif sampling:
                                ifr = sampling.get("IFR_main")
                            
                            cfbits = rep.get("cfbits", {})
                            bits = cfbits.get("bits_tot", 0) if cfbits else 0
                            
                            gates = rep.get("gates", {})
                            state = gates.get("state", "?") if gates else "?"
                            
                            # Format display
                            aessf_str = f"{aessf:.1%}" if aessf else "N/A"
                            ifr_str = f"{ifr:.1%}" if ifr is not None else "N/A"
                            bits_str = f"{bits:.2f}" if bits else "N/A"
                            
                            print(f"   {pol:<30} {aessf_str:>8} {ifr_str:>8} {bits_str:>7} {state:>10}")
                    
                    print("   " + "-" * 75)
                
                print("\n   💡 CF-bits Interpretation:")
                print("   - Bits: Information gain (each bit = halving of width)")
                print("   - A-ESSF: Structural overlap quality (higher is better)")
                print("   - IFR: Efficiency vs theoretical best (higher is better)")
                print("   - Gates: Reliability assessment (GOOD > WARNING > CRITICAL > REFUSE)")

        # 8. Generate visualizations
        generate_visualizations(
            results,
            dataset,
            calibrated_dataset,
            estimator,
            sampler,
            args,
            summary_data,
            cal_result,
        )

        # 9. Export results
        export_results(
            results,
            dataset,
            summary_data,
            weight_diagnostics,
            args,
        )

        # Success message
        if not args.quiet:
            steps_completed = 7  # Base steps
            if args.estimator in ["dr-cpo", "mrdr", "tmle"]:
                steps_completed += 1  # DR diagnostics
            if not args.no_cfbits:
                steps_completed += 1  # CF-bits analysis
            if not args.no_plots:
                steps_completed += 1  # Visualizations

            print(f"\n✓ Analysis complete! ({steps_completed} steps)")

        return 0

    except Exception as e:
        print(f"\n❌ Analysis failed: {e}")
        if args.debug:
            import traceback

            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())


=== ./cje/experiments/arena_10k_simplified/data_generation/__init__.py ===

"""Data generation pipeline for Arena experiments.

This package contains the data generation pipeline that creates CJE datasets
from ChatBot Arena data. Each module handles a specific step in the pipeline.
"""


=== ./cje/experiments/arena_10k_simplified/data_generation/add_scores_with_resume.py ===

#!/usr/bin/env python3
"""
Enhanced scoring script with resume capability and progress tracking.

This module provides functions to add judge scores and oracle labels
with proper resume logic for interrupted runs.
"""

import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import sys
from tqdm import tqdm

# Add parent directory to path for imports
parent_dir = Path(__file__).parent.parent
sys.path.insert(0, str(parent_dir))

from evaluation_utils import (
    FireworksEvaluator,
    DEFAULT_JUDGE_MODEL,
    DEFAULT_ORACLE_MODEL,
)
from experiment_config import BATCH_SIZES


def load_existing_scores(
    file_path: str, score_field: str = "judge_score"
) -> Tuple[List[Dict], List[int], int]:
    """Load existing records and identify which ones need scoring.

    Args:
        file_path: Path to JSONL file
        score_field: Field name to check in metadata

    Returns:
        Tuple of (records, indices_to_score, already_scored_count)
    """
    records = []
    indices_to_score = []
    already_scored = 0

    with open(file_path, "r") as f:
        for i, line in enumerate(f):
            record = json.loads(line)
            records.append(record)

            # Ensure metadata exists
            if "metadata" not in record:
                record["metadata"] = {}

            # Check if already scored
            if (
                score_field in record.get("metadata", {})
                and record["metadata"][score_field] is not None
            ):
                already_scored += 1
            else:
                # Check if we have prompt and response
                if record.get("prompt") and record.get("response"):
                    indices_to_score.append(i)

    return records, indices_to_score, already_scored


def save_progress(
    records: List[Dict], output_file: str, temp_suffix: str = ".tmp"
) -> None:
    """Save progress atomically using temp file + rename.

    Args:
        records: Records to save
        output_file: Output file path
        temp_suffix: Suffix for temp file
    """
    import os

    # Include PID to avoid collision with parallel runs
    temp_file = f"{output_file}{temp_suffix}.{os.getpid()}"
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # Write to temp file
    with open(temp_file, "w") as f:
        for record in records:
            f.write(json.dumps(record) + "\n")

    # Atomic rename
    Path(temp_file).replace(output_path)


def add_scores_with_resume(
    input_file: str,
    output_file: str,
    evaluator: Any,
    score_field: str = "judge_score",
    desc: str = "Scoring",
    batch_size: int = BATCH_SIZES["judge_scoring"],
    save_every: int = BATCH_SIZES[
        "judge_scoring"
    ],  # Default to batch_size for consistent saves
    force_rescore: bool = False,
) -> Dict[str, Any]:
    """Add scores with resume capability and progress tracking.

    Args:
        input_file: Input JSONL file
        output_file: Output JSONL file (can be same as input for in-place update)
        evaluator: Evaluator instance (FireworksEvaluator)
        score_field: Field name in metadata for scores
        desc: Description for progress bar
        batch_size: Number of items to score in one API call
        save_every: Save progress every N scores
        force_rescore: If True, rescore even if scores exist

    Returns:
        Dictionary with statistics
    """
    print(f"\n{'='*60}")
    print(f"Adding {score_field} to dataset")
    print(f"{'='*60}")
    print(f"Input:  {input_file}")
    print(f"Output: {output_file}")
    print(f"Model:  {evaluator.model}")

    # Load existing data and check what needs scoring
    if force_rescore:
        print("Force rescore enabled - will overwrite existing scores")
        records = []
        with open(input_file, "r") as f:
            for line in f:
                record = json.loads(line)
                if "metadata" not in record:
                    record["metadata"] = {}
                # Clear existing score
                if score_field in record.get("metadata", {}):
                    del record["metadata"][score_field]
                records.append(record)

        indices_to_score = [
            i for i, r in enumerate(records) if r.get("prompt") and r.get("response")
        ]
        already_scored = 0
    else:
        # Use output file if it exists, otherwise input file
        check_file = output_file if Path(output_file).exists() else input_file
        records, indices_to_score, already_scored = load_existing_scores(
            check_file, score_field
        )

    total_records = len(records)
    valid_records = sum(1 for r in records if r.get("prompt") and r.get("response"))

    print(f"\n📊 Status:")
    print(f"  Total records:     {total_records}")
    print(f"  Valid records:     {valid_records}")
    print(f"  Already scored:    {already_scored}")
    print(f"  Need scoring:      {len(indices_to_score)}")

    if not indices_to_score:
        print("\n✅ All records already scored!")
        if output_file != input_file and not Path(output_file).exists():
            # Still need to save output file
            save_progress(records, output_file)
            print(f"✓ Saved to {output_file}")
        return {
            "total": total_records,
            "scored": already_scored,
            "skipped": 0,
            "failed": 0,
        }

    # Prepare for scoring
    print(f"\n🚀 Starting scoring of {len(indices_to_score)} records...")
    print(f"  Batch size: {batch_size}")
    print(f"  Save every: {save_every} scores")

    failed_count = 0
    scores_added = 0

    # Process in batches with progress bar
    with tqdm(total=len(indices_to_score), desc=desc, unit="score") as pbar:
        for batch_start in range(0, len(indices_to_score), batch_size):
            batch_end = min(batch_start + batch_size, len(indices_to_score))
            batch_indices = indices_to_score[batch_start:batch_end]

            # Collect prompts and responses for this batch
            prompts = []
            responses = []
            for idx in batch_indices:
                prompts.append(records[idx]["prompt"])
                responses.append(records[idx]["response"])

            # Score the batch
            try:
                result = evaluator.score_batch(
                    prompts,
                    responses,
                    show_progress=False,  # We're using our own progress bar
                    skip_failures=True,
                )

                # Update records with scores
                for i, record_idx in enumerate(batch_indices):
                    score = result.scores[i] if result.scores[i] is not None else None
                    records[record_idx]["metadata"][score_field] = score

                    # Also store metadata about the scoring (model name, etc)
                    if result.metadata and i < len(result.metadata):
                        score_metadata = result.metadata[i]
                        if score_metadata and isinstance(score_metadata, dict):
                            # Store judge_model if present
                            if "judge_model" in score_metadata:
                                records[record_idx]["metadata"]["judge_model"] = (
                                    score_metadata["judge_model"]
                                )

                    if score is not None:
                        scores_added += 1
                    else:
                        failed_count += 1

                    pbar.update(1)

            except Exception as e:
                print(f"\n❌ Batch failed: {e}")
                failed_count += len(batch_indices)
                pbar.update(len(batch_indices))

            # Save progress periodically
            # Save progress periodically based on save_every parameter
            if scores_added > 0 and scores_added % save_every == 0:
                save_progress(records, output_file)
                pbar.set_postfix({"saved": scores_added, "failed": failed_count})

    # Final save
    save_progress(records, output_file)

    # Print summary
    print(f"\n{'='*60}")
    print(f"✅ Scoring Complete!")
    print(f"{'='*60}")
    print(f"  New scores added:  {scores_added}")
    print(f"  Failed to score:   {failed_count}")
    print(f"  Total scored:      {already_scored + scores_added}")
    print(f"  Output saved to:   {output_file}")

    # Print score statistics
    all_scores = [
        r["metadata"].get(score_field)
        for r in records
        if r.get("metadata", {}).get(score_field) is not None
    ]

    if all_scores:
        import numpy as np

        print(f"\n📈 Score Statistics:")
        print(f"  Mean:   {np.mean(all_scores):.3f}")
        print(f"  Median: {np.median(all_scores):.3f}")
        print(f"  Std:    {np.std(all_scores):.3f}")
        print(f"  Range:  [{np.min(all_scores):.3f}, {np.max(all_scores):.3f}]")

    return {
        "total": total_records,
        "scored": already_scored + scores_added,
        "skipped": total_records - valid_records,
        "failed": failed_count,
    }


def add_judge_scores_with_resume(
    input_file: str, output_file: str, model: str = DEFAULT_JUDGE_MODEL, **kwargs: Any
) -> Dict[str, Any]:
    """Add judge scores with resume capability.

    Args:
        input_file: Input JSONL file
        output_file: Output JSONL file
        model: Judge model to use
        **kwargs: Additional arguments for add_scores_with_resume

    Returns:
        Statistics dictionary
    """
    evaluator = FireworksEvaluator(model=model)
    return add_scores_with_resume(
        input_file,
        output_file,
        evaluator,
        score_field="judge_score",
        desc="Judge scoring",
        **kwargs,
    )


def add_oracle_labels_with_resume(
    input_file: str, output_file: str, model: str = DEFAULT_ORACLE_MODEL, **kwargs: Any
) -> Dict[str, Any]:
    """Add oracle labels with resume capability.

    Args:
        input_file: Input JSONL file
        output_file: Output JSONL file
        model: Oracle model to use
        **kwargs: Additional arguments for add_scores_with_resume

    Returns:
        Statistics dictionary
    """
    evaluator = FireworksEvaluator(model=model)
    return add_scores_with_resume(
        input_file,
        output_file,
        evaluator,
        score_field="oracle_label",
        desc="Oracle evaluation",
        **kwargs,
    )


def main() -> int:
    """CLI for adding scores with resume capability."""
    import argparse

    parser = argparse.ArgumentParser(description="Add scores with resume capability")
    parser.add_argument("input", help="Input JSONL file")
    parser.add_argument("-o", "--output", help="Output file (default: overwrite input)")
    parser.add_argument(
        "--type", choices=["judge", "oracle"], default="judge", help="Type of scoring"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=BATCH_SIZES["judge_scoring"],
        help=f"Batch size for API calls (default: {BATCH_SIZES['judge_scoring']})",
    )
    parser.add_argument(
        "--save-every",
        type=int,
        default=BATCH_SIZES["judge_scoring"],
        help=f"Save progress every N scores (default: {BATCH_SIZES['judge_scoring']}, matches batch size)",
    )
    parser.add_argument(
        "--force", action="store_true", help="Force rescore even if scores exist"
    )

    args = parser.parse_args()

    output_file = args.output or args.input

    if args.type == "judge":
        stats = add_judge_scores_with_resume(
            args.input,
            output_file,
            batch_size=args.batch_size,
            save_every=args.save_every,
            force_rescore=args.force,
        )
    else:
        stats = add_oracle_labels_with_resume(
            args.input,
            output_file,
            batch_size=args.batch_size,
            save_every=args.save_every,
            force_rescore=args.force,
        )

    return 0 if stats["failed"] == 0 else 1


if __name__ == "__main__":
    sys.exit(main())


=== ./cje/experiments/arena_10k_simplified/data_generation/compute_logprobs.py ===

#!/usr/bin/env python3
"""
Compute log probabilities for responses using teacher forcing.

Simplified version with single API call per sample.
"""

import json
import os
import shutil
import time
from pathlib import Path
from typing import Dict, List, Optional, Any

import sys

sys.path.append(str(Path(__file__).parent.parent.parent.parent))
sys.path.append(str(Path(__file__).parent.parent))  # Add arena_10k_simplified to path

from cje.teacher_forcing import compute_chat_logprob
from cje.data.models import LogProbResult, LogProbStatus
from experiment_config import POLICIES, get_policy_config, POLICY_NAMES, BATCH_SIZES


def load_existing_logprobs(output_file: str) -> Dict[str, Dict]:
    """Load existing log probabilities from file.

    Returns:
        Dictionary mapping prompt_id to logprob data
    """
    existing = {}
    corrupted_lines = 0
    if Path(output_file).exists():
        with open(output_file, "r") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:  # Skip empty lines
                    continue
                try:
                    data = json.loads(line)
                    if "prompt_id" in data:
                        existing[data["prompt_id"]] = data
                except json.JSONDecodeError as e:
                    corrupted_lines += 1
                    print(f"  ⚠️  Skipping corrupted line {line_num}: {e}")
                    continue  # Skip corrupted lines

    if corrupted_lines > 0:
        print(f"  ⚠️  Found {corrupted_lines} corrupted lines during resume")

    return existing


def compute_logprobs_for_responses(
    base_responses_file: str,
    output_file: str,
    max_samples: Optional[int] = None,
    policy_name: str = "base",
    batch_size: Optional[int] = None,
) -> List[Dict]:
    """Compute log probabilities for BASE policy responses under a given policy's model.

    Simplified to use single API call per sample.

    Args:
        base_responses_file: Path to BASE policy responses JSONL file
        output_file: Where to save log probabilities
        max_samples: Limit number of samples to process
        policy_name: Name of the policy whose model to use for computing log probs
        batch_size: Save progress every N samples (for resume capability)

    Returns:
        List of dictionaries with log probability results
    """
    if not os.getenv("FIREWORKS_API_KEY"):
        raise ValueError("FIREWORKS_API_KEY environment variable required")

    # Get policy configuration
    policy_config = get_policy_config(policy_name)
    model = policy_config["model"]
    temperature = policy_config["temperature"]
    system_prompt = policy_config["system_prompt"]
    template_config = policy_config.get(
        "template_config"
    )  # Get template config from policy

    # Load existing logprobs if resuming
    existing_logprobs = load_existing_logprobs(output_file) if batch_size else {}

    # Load BASE policy responses
    responses = []
    with open(base_responses_file, "r") as f:
        for line in f:
            data = json.loads(line)
            if data.get("response"):  # Skip failed responses
                # Skip if already computed and using batching
                if batch_size and data.get("prompt_id") in existing_logprobs:
                    continue
                responses.append(data)

    if max_samples:
        # Adjust for existing logprobs
        total_needed = max_samples - len(existing_logprobs)
        responses = responses[:total_needed]

    if not responses:
        print(
            f"✓ All {len(existing_logprobs)} log probs already computed for {policy_name}"
        )
        return list(existing_logprobs.values())

    print(f"Computing log probs for {len(responses)} BASE responses...")
    if existing_logprobs:
        print(
            f"  📂 Resuming from previous run: {len(existing_logprobs)} already completed"
        )
        print(f"  🔄 Continuing with {len(responses)} remaining computations")
    print(f"Using {policy_name} policy model: {model}")
    print(f"Temperature: {temperature}, System prompt: {system_prompt[:50]}...")
    if batch_size:
        print(f"Batch size: {batch_size} (saving progress incrementally)")

    # Log template configuration
    if template_config:
        print(f"Using explicit template config: {template_config.__class__.__name__}")
    else:
        print("Using auto-detected template for Fireworks model")

    # Setup output file for appending if using batching
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # For batching, we'll write to temp file then rename atomically
    temp_file = None
    output_f = None
    if batch_size:
        # Copy existing file to temp if it exists
        temp_file = f"{output_file}.tmp"
        if output_path.exists():
            shutil.copy2(output_file, temp_file)
        output_f = open(temp_file, "a")

    # Compute log probabilities
    results: List[Dict[str, Any]] = (
        list(existing_logprobs.values()) if batch_size else []
    )
    failed_count = 0
    successful_count = 0

    try:
        for i, data in enumerate(responses):
            prompt = data["prompt"]
            response = data["response"]

            # Create chat format with system prompt
            chat = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response},
            ]

            # Smart retry with validation
            start_time = time.time()
            max_retries = 3
            retry_delay = 1.0  # Initial delay in seconds

            for attempt in range(max_retries):
                result = compute_chat_logprob(
                    chat=chat,
                    model=model,
                    temperature=temperature,
                    template_config=template_config,
                )

                # Check if we got a valid result
                if result.is_valid and result.value is not None:
                    # Additional validation for positive log prob
                    if result.value > 0:
                        # This shouldn't happen - chat.py already checks this
                        # But double-check for safety before saving
                        error_msg = f"Positive log probability: {result.value:.3f}"
                        print(f"    ERROR: {error_msg}")
                        result = LogProbResult(
                            value=None,
                            status=LogProbStatus.API_ERROR,
                            error=error_msg,
                            metadata={"invalid_logprob": result.value},
                        )
                    else:
                        # Success! Valid negative log probability
                        break

                # Check if error is retryable
                if result.error:
                    # Token boundary errors often succeed with retry (different tokenization)
                    # API errors might be transient
                    retryable = any(
                        phrase in result.error.lower()
                        for phrase in [
                            "boundary",
                            "token",
                            "timeout",
                            "rate",
                            "api",
                            "connection",
                        ]
                    )

                    if not retryable:
                        print(f"    Non-retryable error: {result.error}")
                        break  # Don't retry for permanent errors

                # Retry with backoff if not last attempt
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2**attempt)  # Exponential backoff
                    print(
                        f"    Retry {attempt + 1}/{max_retries - 1} after {wait_time:.1f}s: {result.error}"
                    )
                    time.sleep(wait_time)

            duration = time.time() - start_time

            # Record result
            if result.is_valid and result.value is not None:
                logprob = result.value
                error = None
                successful_count += 1
                print(
                    f"  [{i + 1}/{len(responses)}] {data['prompt_id']}: {logprob:.3f} ({duration:.1f}s)"
                )
            else:
                logprob = None
                error = result.error or "Unknown error"
                failed_count += 1
                print(
                    f"  [{i + 1}/{len(responses)}] {data['prompt_id']}: FAILED - {error}"
                )

            output_record = {
                "prompt_id": data["prompt_id"],
                "prompt": prompt,
                "response": response,
                "source_policy": "base",  # Always computing base responses
                "eval_model": model,
                "logprob": logprob,
                "error": error,
            }

            # Save immediately if using batching
            if batch_size and output_f:
                output_f.write(json.dumps(output_record) + "\n")
                output_f.flush()  # Ensure written to disk
                # Save progress message every batch_size computations
                if (i + 1) % batch_size == 0:
                    total_so_far = len(existing_logprobs) + i + 1
                    print(
                        f"  💾 Progress saved: {total_so_far} total log probs ({i + 1} new this run)"
                    )
            else:
                results.append(output_record)

    finally:
        # Always close file if using batching
        if batch_size and output_f:
            output_f.close()
            # Atomic rename from temp to final
            if temp_file and Path(temp_file).exists():
                os.replace(temp_file, output_file)
                total_results = len(existing_logprobs) + len(responses)
                print(f"\n✓ Saved {total_results} total log probs to {output_path}")

    # Handle return for batch mode
    if batch_size:
        # Return all results including existing
        return list(load_existing_logprobs(output_file).values())
    else:
        # Save all results at once (original behavior)
        print(f"\nSaving results to {output_file}")
        with open(output_file, "w") as f:
            for record in results:
                f.write(json.dumps(record) + "\n")

    # Print summary
    print(f"\nSummary:")
    print(f"  Successful: {successful_count}/{len(responses)}")
    print(f"  Failed: {failed_count}/{len(responses)}")

    if successful_count > 0:
        logprobs = [r["logprob"] for r in results if r["logprob"] is not None]
        mean_logprob = sum(logprobs) / len(logprobs)
        min_logprob = min(logprobs)
        max_logprob = max(logprobs)
        print(f"  Mean logprob: {mean_logprob:.3f}")
        print(f"  Range: [{min_logprob:.3f}, {max_logprob:.3f}]")

    return results


def main() -> None:
    """Main entry point for the script."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Compute log probabilities for responses (simplified single-sample version)"
    )
    parser.add_argument(
        "--responses-dir",
        required=True,
        help="Directory containing response JSONL files",
    )
    parser.add_argument(
        "--output-dir", required=True, help="Directory to save log probability files"
    )
    parser.add_argument(
        "--max-samples", type=int, help="Maximum number of samples to process"
    )
    parser.add_argument(
        "--policies",
        nargs="+",
        default=POLICY_NAMES,
        help="List of policies to compute log probs for (default: all)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=BATCH_SIZES["logprob_computation"],
        help=f"Save progress every N log probs (0 to disable, default: {BATCH_SIZES['logprob_computation']})",
    )
    parser.add_argument(
        "--pass-number",
        type=int,
        default=1,
        help="Pass number for multi-pass generation (1=original, 2-N=additional passes)",
    )

    args = parser.parse_args()

    # Create output directory
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Process each policy
    base_responses_file = Path(args.responses_dir) / "base_responses.jsonl"
    if not base_responses_file.exists():
        raise FileNotFoundError(f"Base responses file not found: {base_responses_file}")

    for policy in args.policies:
        print(f"\n{'=' * 60}")
        print(f"Computing log probs for {policy} policy (pass {args.pass_number})")
        print(f"{'=' * 60}")

        # Determine output filename based on pass number
        if args.pass_number == 1:
            output_file = output_dir / f"{policy}_logprobs.jsonl"
        else:
            output_file = output_dir / f"{policy}_logprobs_pass{args.pass_number}.jsonl"

        compute_logprobs_for_responses(
            base_responses_file=str(base_responses_file),
            output_file=str(output_file),
            max_samples=args.max_samples,
            policy_name=policy,
            batch_size=args.batch_size if args.batch_size > 0 else None,
        )


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/data_generation/generate_additional_passes.py ===

#!/usr/bin/env python3
"""
Generate additional passes for log probability computation to study API non-determinism.

This script orchestrates multiple runs of compute_logprobs.py with different pass numbers,
allowing us to:
1. Document variance in API responses
2. Identify deterministic vs non-deterministic failures
3. Improve data quality through aggregation (in analysis phase)

Usage:
    # Generate passes 2-5 for all policies
    python generate_additional_passes.py --n-passes 5

    # Generate specific passes
    python generate_additional_passes.py --start-pass 3 --end-pass 5

    # Run in parallel (faster but more API load)
    python generate_additional_passes.py --n-passes 5 --parallel
"""

import argparse
import subprocess
import sys
from pathlib import Path
from typing import List, Tuple, Dict
import concurrent.futures
from collections import defaultdict

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent))
from experiment_config import POLICY_NAMES, BATCH_SIZES


def check_existing_passes(logprobs_dir: Path) -> Dict[str, List[int]]:
    """Check which passes already exist for each policy.

    Returns:
        Dictionary mapping policy name to list of existing pass numbers
    """
    existing: Dict[str, List[int]] = defaultdict(list)

    if not logprobs_dir.exists():
        return existing

    # Check for original files (pass 1)
    for policy in POLICY_NAMES:
        original = logprobs_dir / f"{policy}_logprobs.jsonl"
        if original.exists():
            existing[policy].append(1)

    # Check for additional passes
    for logprob_file in logprobs_dir.glob("*_logprobs_pass*.jsonl"):
        # Extract policy and pass number from filename
        parts = logprob_file.stem.split("_logprobs_pass")
        if len(parts) == 2:
            policy = parts[0]
            try:
                pass_num = int(parts[1])
                existing[policy].append(pass_num)
            except ValueError:
                print(f"Warning: Could not parse pass number from {logprob_file.name}")

    return existing


def run_single_pass(
    policy: str,
    pass_number: int,
    responses_dir: Path,
    output_dir: Path,
    batch_size: int,
) -> subprocess.CompletedProcess:
    """Run compute_logprobs.py for a single policy and pass."""

    cmd = [
        "python",
        "data_generation/compute_logprobs.py",
        "--responses-dir",
        str(responses_dir),
        "--output-dir",
        str(output_dir),
        "--policies",
        policy,
        "--pass-number",
        str(pass_number),
        "--batch-size",
        str(batch_size),
    ]

    print(f"Running: {' '.join(cmd)}")
    return subprocess.run(cmd, capture_output=True, text=True)


def run_sequential_passes(
    passes_to_run: List[Tuple[str, int]],
    responses_dir: Path,
    output_dir: Path,
    batch_size: int,
) -> None:
    """Run passes sequentially."""

    total = len(passes_to_run)
    completed = 0
    failed = []

    for policy, pass_num in passes_to_run:
        print(f"\n[{completed + 1}/{total}] Processing {policy} pass {pass_num}...")

        result = run_single_pass(
            policy, pass_num, responses_dir, output_dir, batch_size
        )

        if result.returncode == 0:
            completed += 1
            print(f"✓ Completed {policy} pass {pass_num}")
        else:
            failed.append((policy, pass_num))
            print(f"✗ Failed {policy} pass {pass_num}")
            print(f"  Error: {result.stderr[:500]}")

    # Summary
    print(f"\n{'=' * 60}")
    print(f"Completed: {completed}/{total}")
    if failed:
        print(f"Failed: {len(failed)}")
        for policy, pass_num in failed:
            print(f"  - {policy} pass {pass_num}")


def run_parallel_passes(
    passes_to_run: List[Tuple[str, int]],
    responses_dir: Path,
    output_dir: Path,
    batch_size: int,
    max_workers: int = 4,
) -> None:
    """Run passes in parallel using process pool."""

    total = len(passes_to_run)
    completed = 0
    failed = []

    print(f"Running {total} passes in parallel (max {max_workers} workers)...")

    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks
        future_to_pass = {}
        for policy, pass_num in passes_to_run:
            future = executor.submit(
                run_single_pass, policy, pass_num, responses_dir, output_dir, batch_size
            )
            future_to_pass[future] = (policy, pass_num)

        # Process results as they complete
        for future in concurrent.futures.as_completed(future_to_pass):
            policy, pass_num = future_to_pass[future]
            try:
                result = future.result()
                if result.returncode == 0:
                    completed += 1
                    print(f"✓ [{completed}/{total}] Completed {policy} pass {pass_num}")
                else:
                    failed.append((policy, pass_num))
                    print(f"✗ Failed {policy} pass {pass_num}")
            except Exception as e:
                failed.append((policy, pass_num))
                print(f"✗ Exception for {policy} pass {pass_num}: {e}")

    # Summary
    print(f"\n{'=' * 60}")
    print(f"Completed: {completed}/{total}")
    if failed:
        print(f"Failed: {len(failed)}")
        for policy, pass_num in failed:
            print(f"  - {policy} pass {pass_num}")


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Generate additional passes for log probability computation"
    )
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=Path("data"),
        help="Data directory containing responses/ and logprobs/ subdirectories",
    )
    parser.add_argument(
        "--n-passes",
        type=int,
        default=5,
        help="Total number of passes to have (including original)",
    )
    parser.add_argument(
        "--start-pass",
        type=int,
        help="Starting pass number (default: 2 if original exists, else 1)",
    )
    parser.add_argument(
        "--end-pass", type=int, help="Ending pass number (default: n-passes)"
    )
    parser.add_argument(
        "--policies",
        nargs="+",
        default=POLICY_NAMES,
        help="Specific policies to process (default: all)",
    )
    parser.add_argument(
        "--parallel", action="store_true", help="Run passes in parallel"
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        help="Maximum parallel workers (default: 4)",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=BATCH_SIZES["logprob_computation"],
        help=f"Batch size for saving progress (default: {BATCH_SIZES['logprob_computation']})",
    )
    parser.add_argument(
        "--force", action="store_true", help="Regenerate even if pass already exists"
    )

    args = parser.parse_args()

    # Setup directories
    responses_dir = args.data_dir / "responses"
    logprobs_dir = args.data_dir / "logprobs"

    if not responses_dir.exists():
        print(f"Error: Responses directory not found: {responses_dir}")
        sys.exit(1)

    logprobs_dir.mkdir(parents=True, exist_ok=True)

    # Check what already exists
    existing = check_existing_passes(logprobs_dir)

    print("Existing passes:")
    for policy in args.policies:
        if policy in existing:
            print(f"  {policy}: passes {sorted(existing[policy])}")
        else:
            print(f"  {policy}: none")

    # Determine which passes to run
    passes_to_run = []

    # Determine pass range
    start_pass = (
        args.start_pass if args.start_pass else 2
    )  # Default to 2 (skip original)
    end_pass = args.end_pass if args.end_pass else args.n_passes

    for policy in args.policies:
        for pass_num in range(start_pass, end_pass + 1):
            # Skip if already exists (unless force)
            if not args.force and pass_num in existing.get(policy, []):
                print(f"Skipping {policy} pass {pass_num} (already exists)")
                continue

            passes_to_run.append((policy, pass_num))

    if not passes_to_run:
        print("\nNo passes to generate. All requested passes already exist.")
        print("Use --force to regenerate existing passes.")
        return

    print(f"\nWill generate {len(passes_to_run)} passes:")
    for policy, pass_num in passes_to_run[:5]:
        print(f"  - {policy} pass {pass_num}")
    if len(passes_to_run) > 5:
        print(f"  ... and {len(passes_to_run) - 5} more")

    # Run the passes
    if args.parallel:
        run_parallel_passes(
            passes_to_run,
            responses_dir,
            logprobs_dir,
            args.batch_size,
            args.max_workers,
        )
    else:
        run_sequential_passes(
            passes_to_run, responses_dir, logprobs_dir, args.batch_size
        )

    print("\n✅ Additional pass generation complete!")
    print("Next step: Run analysis to study API non-determinism")
    print("  python data_generation/analyze_nondeterminism.py --data-dir data")


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/data_generation/generate_responses.py ===

#!/usr/bin/env python3
"""
Generate responses for Arena prompts using different policies.

Uses Fireworks API with different system prompts to simulate different policies:
- base/clone: Helpful assistant
- unhelpful: Deliberately confusing assistant

Includes retry logic with exponential backoff for handling API failures.
"""

import json
import os
import shutil
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
import requests  # type: ignore

import sys

sys.path.append(str(Path(__file__).parent.parent.parent.parent))
sys.path.append(str(Path(__file__).parent.parent))  # Add arena_10k_simplified to path

from experiment_config import get_all_policies, BATCH_SIZES


class ErrorType(Enum):
    """Categorize errors for retry logic."""

    RETRYABLE = "retryable"  # Network errors, rate limits, server errors
    NON_RETRYABLE = "non_retryable"  # Bad request, auth errors
    UNKNOWN = "unknown"


def classify_error(error: Exception) -> Tuple[ErrorType, str]:
    """Classify an error to determine if it's retryable.

    Returns:
        Tuple of (ErrorType, error_message)
    """
    error_str = str(error)

    # Check for HTTP status codes in requests.exceptions.HTTPError
    if isinstance(error, requests.exceptions.HTTPError):
        if hasattr(error.response, "status_code"):
            status = error.response.status_code
            # Retryable errors
            if status in [429, 500, 502, 503, 504, 530]:
                return ErrorType.RETRYABLE, f"HTTP {status}: {error_str}"
            # Non-retryable errors
            elif status in [400, 401, 403, 404]:
                return ErrorType.NON_RETRYABLE, f"HTTP {status}: {error_str}"

    # Connection errors are retryable
    if isinstance(
        error,
        (
            requests.exceptions.ConnectionError,
            requests.exceptions.Timeout,
            requests.exceptions.ReadTimeout,
        ),
    ):
        return ErrorType.RETRYABLE, f"Connection error: {error_str}"

    # Rate limiting messages
    if any(
        msg in error_str.lower() for msg in ["rate limit", "too many requests", "quota"]
    ):
        return ErrorType.RETRYABLE, f"Rate limit: {error_str}"

    # Default to unknown (which we'll retry a few times)
    return ErrorType.UNKNOWN, error_str


def exponential_backoff_with_jitter(
    attempt: int, base_delay: float = 1.0, max_delay: float = 60.0
) -> float:
    """Calculate exponential backoff delay with jitter.

    Args:
        attempt: Current attempt number (0-indexed)
        base_delay: Base delay in seconds
        max_delay: Maximum delay in seconds

    Returns:
        Delay in seconds
    """
    import random

    delay = min(base_delay * (2**attempt), max_delay)
    # Add jitter: ±25% of the delay
    jitter = delay * 0.25 * (2 * random.random() - 1)
    return float(max(0.1, delay + jitter))


def call_fireworks_with_retry(
    url: str,
    headers: Dict[str, str],
    payload: Dict[str, Any],
    max_retries: int = 5,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    verbose: bool = True,
) -> Dict[str, Any]:
    """Call Fireworks API with exponential backoff retry logic.

    Args:
        url: API endpoint
        headers: Request headers
        payload: Request payload
        max_retries: Maximum number of retry attempts
        base_delay: Base delay for exponential backoff
        max_delay: Maximum delay between retries
        verbose: Whether to print retry information

    Returns:
        Response data from successful API call

    Raises:
        Exception: If all retries are exhausted
    """
    last_error = None

    for attempt in range(max_retries + 1):
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            result: Dict[str, Any] = response.json()
            return result

        except Exception as e:
            last_error = e
            error_type, error_msg = classify_error(e)

            if attempt == max_retries:
                # No more retries
                if verbose:
                    print(
                        f"    ❌ Failed after {max_retries + 1} attempts: {error_msg}"
                    )
                raise

            if error_type == ErrorType.NON_RETRYABLE:
                # Don't retry non-retryable errors
                if verbose:
                    print(f"    ❌ Non-retryable error: {error_msg}")
                raise

            # Calculate delay for retryable and unknown errors
            delay = exponential_backoff_with_jitter(attempt, base_delay, max_delay)

            if verbose:
                print(
                    f"    ⚠️  Attempt {attempt + 1}/{max_retries + 1} failed: {error_msg}"
                )
                print(f"    ⏱️  Retrying in {delay:.1f} seconds...")

            time.sleep(delay)

    # This should never be reached, but just in case
    if last_error:
        raise last_error
    raise RuntimeError("Unexpected error in retry logic")


def load_existing_responses(output_file: str) -> Dict[str, Dict]:
    """Load existing responses from file.

    Returns:
        Dictionary mapping prompt_id to response data
    """
    existing = {}
    corrupted_lines = 0
    if Path(output_file).exists():
        with open(output_file, "r") as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:  # Skip empty lines
                    continue
                try:
                    data = json.loads(line)
                    if "prompt_id" in data:
                        existing[data["prompt_id"]] = data
                except json.JSONDecodeError as e:
                    corrupted_lines += 1
                    print(f"  ⚠️  Skipping corrupted line {line_num}: {e}")
                    continue  # Skip corrupted lines

    if corrupted_lines > 0:
        print(f"  ⚠️  Found {corrupted_lines} corrupted lines during resume")

    return existing


def generate_responses(
    prompts_file: str,
    output_file: str,
    model: str,
    temperature: float = 0.7,
    max_responses: Optional[int] = None,
    policy_name: str = "base",
    system_prompt: str = "You are a helpful assistant.",
    max_tokens: int = 1000,
    batch_size: Optional[int] = None,
    max_retries: int = 5,
    retry_delay: float = 1.0,
    max_retry_delay: float = 60.0,
    skip_failed: bool = False,
) -> List[Dict]:
    """Generate responses for prompts using Fireworks API with retry logic.

    Args:
        prompts_file: Path to JSONL file with prompts
        output_file: Where to save responses
        model: Fireworks model identifier
        temperature: Sampling temperature
        max_responses: Limit number of responses to generate
        policy_name: Name of the policy (for tracking)
        system_prompt: System prompt to use
        max_tokens: Maximum number of tokens to generate
        batch_size: Save progress every N responses (None to disable)
        max_retries: Maximum number of retry attempts for API calls
        retry_delay: Base delay for exponential backoff
        max_retry_delay: Maximum delay between retries
        skip_failed: If True, skip prompts that previously failed (don't retry them)

    Returns:
        List of response dictionaries
    """
    # Check for API key
    api_key = os.getenv("FIREWORKS_API_KEY")
    if not api_key:
        raise ValueError("FIREWORKS_API_KEY environment variable required")

    # Load existing responses if resuming (always check for existing work)
    existing_responses = load_existing_responses(output_file)

    # Track statistics
    stats = {
        "successful": 0,
        "failed": 0,
        "skipped": 0,
        "retried": 0,
    }

    # Load prompts
    prompts = []
    with open(prompts_file, "r") as f:
        for line in f:
            prompt_data = json.loads(line)
            prompt_id = prompt_data.get("prompt_id")

            # Skip if already exists
            if prompt_id in existing_responses:
                existing_resp = existing_responses[prompt_id]
                # Check if it was a failed response
                if existing_resp.get("response") is None and not skip_failed:
                    # Include failed responses for retry
                    prompts.append(prompt_data)
                else:
                    stats["skipped"] += 1
                continue
            prompts.append(prompt_data)

    if max_responses:
        # Adjust for existing successful responses
        successful_existing = sum(
            1 for r in existing_responses.values() if r.get("response") is not None
        )
        total_needed = max_responses - successful_existing
        prompts = prompts[:total_needed]

    if not prompts:
        print(
            f"✓ All {len(existing_responses)} responses already exist for {policy_name}"
        )
        return list(existing_responses.values())

    print(f"Generating {len(prompts)} responses with {policy_name} policy...")
    if existing_responses:
        failed_count = sum(
            1 for r in existing_responses.values() if r.get("response") is None
        )
        print(f"  📂 Existing: {len(existing_responses)} total ({failed_count} failed)")
        if not skip_failed and failed_count > 0:
            print(f"  🔄 Will retry {failed_count} failed responses")
    print(f"Model: {model}, Temperature: {temperature}, Max tokens: {max_tokens}")
    print(f"System prompt: {system_prompt[:50]}...")
    print(f"Retry config: max_retries={max_retries}, base_delay={retry_delay}s")
    if batch_size:
        print(f"Batch size: {batch_size} (saving progress incrementally)")

    # Fireworks API endpoint
    url = "https://api.fireworks.ai/inference/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    # Setup output file for appending if using batching
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # For batching, we'll write to temp file then rename atomically
    temp_file = None
    output_f = None
    if batch_size:
        # We'll rewrite the entire file with updated responses
        # Include PID to avoid collision with parallel runs
        temp_file = f"{output_file}.tmp.{os.getpid()}"
        # Don't copy existing file - we'll write all responses fresh
        output_f = open(temp_file, "w")

        # Write existing successful responses first
        for resp in existing_responses.values():
            if resp.get("response") is not None or skip_failed:
                output_f.write(json.dumps(resp) + "\n")

    # Generate responses
    results = []

    try:
        for i, prompt_data in enumerate(prompts):
            prompt_id = prompt_data["prompt_id"]
            prompt = prompt_data["prompt"]

            # Check if this is a retry
            is_retry = (
                prompt_id in existing_responses
                and existing_responses[prompt_id].get("response") is None
            )
            if is_retry:
                stats["retried"] += 1

            try:
                # Prepare messages with system prompt
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt},
                ]

                # Call Fireworks API with retry logic
                payload = {
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                }

                # Show which prompt we're processing
                if is_retry:
                    print(
                        f"  🔄 Retrying {prompt_id} (attempt {i + 1}/{len(prompts)})..."
                    )
                elif (i + 1) % 10 == 0:
                    print(f"  📝 Processing {i + 1}/{len(prompts)} ({prompt_id})...")

                response_data = call_fireworks_with_retry(
                    url,
                    headers,
                    payload,
                    max_retries=max_retries,
                    base_delay=retry_delay,
                    max_delay=max_retry_delay,
                    verbose=is_retry,  # Show retry details for retries
                )

                result = {
                    "prompt_id": prompt_id,
                    "prompt": prompt,
                    "response": response_data["choices"][0]["message"]["content"],
                    "policy": policy_name,
                    "model": model,
                    "temperature": temperature,
                }

                stats["successful"] += 1

                if is_retry:
                    print(f"    ✅ Successfully regenerated {prompt_id}")

                # Save immediately if using batching
                if batch_size and output_f is not None:
                    output_f.write(json.dumps(result) + "\n")
                    output_f.flush()  # Ensure written to disk
                    # Show progress every batch_size responses
                    if stats["successful"] % batch_size == 0:
                        print(
                            f"  💾 Progress: {stats['successful']} successful, "
                            f"{stats['failed']} failed, {stats['retried']} retries"
                        )
                else:
                    results.append(result)

            except Exception as e:
                error_type, error_msg = classify_error(e)
                print(f"  ❌ Failed on {prompt_id}: {error_msg}")
                stats["failed"] += 1

                # Add failed result
                result = {
                    "prompt_id": prompt_id,
                    "prompt": prompt,
                    "response": None,
                    "policy": policy_name,
                    "error": error_msg,
                    "error_type": error_type.value,
                }

                if batch_size and output_f is not None:
                    output_f.write(json.dumps(result) + "\n")
                    output_f.flush()
                else:
                    results.append(result)

    finally:
        # Always close file if using batching
        if batch_size and output_f:
            output_f.close()
            # Atomic rename from temp to final
            if temp_file and Path(temp_file).exists():
                os.replace(temp_file, output_file)
                print(f"\n✓ Results saved to {output_path}")
                print(
                    f"  📊 Final stats: {stats['successful']} successful, "
                    f"{stats['failed']} failed, {stats['retried']} retries attempted"
                )

    # Handle return for batch mode
    if batch_size:
        # Return all results including existing
        return list(load_existing_responses(output_file).values())
    else:
        # Save all results at once (original behavior)
        with open(output_path, "w") as f:
            # Include existing responses if any
            for resp in existing_responses.values():
                f.write(json.dumps(resp) + "\n")
            for result in results:
                f.write(json.dumps(result) + "\n")

        total_responses = len(existing_responses) + len(results)
        print(f"\n✓ Saved {total_responses} responses to {output_path}")
        print(f"  📊 Stats: {stats['successful']} successful, {stats['failed']} failed")
        return list(existing_responses.values()) + results


def main() -> None:
    """Generate responses for different policies."""
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--prompts", default="data/arena_prompts.jsonl", help="Input prompts file"
    )
    parser.add_argument(
        "--output-dir", default="data/responses", help="Output directory"
    )
    parser.add_argument("--max-responses", type=int, help="Limit number of responses")
    parser.add_argument(
        "--max-tokens", type=int, default=1000, help="Maximum tokens per response"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=BATCH_SIZES["response_generation"],
        help=f"Save progress every N responses (0 to disable, default: {BATCH_SIZES['response_generation']})",
    )
    parser.add_argument(
        "--max-retries",
        type=int,
        default=5,
        help="Maximum retry attempts for API calls",
    )
    parser.add_argument(
        "--retry-delay",
        type=float,
        default=1.0,
        help="Base delay for exponential backoff (seconds)",
    )
    parser.add_argument(
        "--max-retry-delay",
        type=float,
        default=60.0,
        help="Maximum delay between retries (seconds)",
    )
    parser.add_argument(
        "--skip-failed",
        action="store_true",
        help="Skip previously failed responses instead of retrying them",
    )
    parser.add_argument(
        "--policies", nargs="+", help="Specific policies to generate (default: all)"
    )

    args = parser.parse_args()

    # Get policies from centralized configuration
    all_policies = get_all_policies()

    # Filter policies if specified
    if args.policies:
        policies = [p for p in all_policies if p["name"] in args.policies]
        if not policies:
            print(f"Error: No matching policies found for {args.policies}")
            return
    else:
        policies = all_policies

    for policy in policies:
        output_file = f"{args.output_dir}/{policy['name']}_responses.jsonl"
        generate_responses(
            prompts_file=args.prompts,
            output_file=output_file,
            model=policy["model"],
            temperature=policy["temperature"],
            policy_name=policy["name"],
            system_prompt=policy["system_prompt"],
            max_responses=args.max_responses,
            max_tokens=args.max_tokens,
            batch_size=args.batch_size if args.batch_size > 0 else None,
            max_retries=args.max_retries,
            retry_delay=args.retry_delay,
            max_retry_delay=args.max_retry_delay,
            skip_failed=args.skip_failed,
        )


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/data_generation/prepare_arena_data.py ===

#!/usr/bin/env python3
"""
Prepare ChatBot Arena data for CJE experiments.

Downloads ChatBot Arena conversations and extracts unique prompts.
Key approach: Deduplicate contexts to ensure we're comparing fresh
responses from our policies rather than reusing existing Arena responses.
"""

import json
import random
import re
from pathlib import Path
from typing import List, Dict, Any

import sys

sys.path.append(str(Path(__file__).parent.parent.parent.parent))
sys.path.append(str(Path(__file__).parent.parent))  # Add arena_10k_simplified to path

from experiment_config import DATA_QUALITY


def is_moderation_flagged(moderation_data: Dict[str, Any]) -> bool:
    """Check if OpenAI moderation scores indicate problematic content.

    Note: The 'flagged' field is unreliable (always False in the dataset),
    so we check the actual scores instead.

    Args:
        moderation_data: OpenAI moderation API response

    Returns:
        True if any moderation score exceeds 0.3 (30% confidence)
    """
    if not moderation_data:
        # If no moderation data, don't filter
        return False

    # Check scores directly since 'flagged' is unreliable
    # This threshold filters ~0.28% of prompts (the most problematic ones)
    scores = moderation_data.get("category_scores", {})
    threshold = DATA_QUALITY["content_moderation_threshold"]

    for score in scores.values():
        if score > threshold:
            return True

    return False


def is_problematic_prompt(prompt: str) -> bool:
    """Check if a prompt is likely to cause oracle scoring failures.

    This filters out prompts that:
    1. Ask to spell offensive words letter by letter
    2. Contains patterns that lead to problematic content generation

    Args:
        prompt: The prompt to check

    Returns:
        True if the prompt should be filtered out
    """
    prompt_lower = prompt.lower()

    # Pattern 1: Letter-by-letter spelling that could form offensive words
    letter_patterns = [
        # N-I-G-G-E-R patterns
        r"write.*letters.*n.*i.*g.*g.*e.*r",
        r"spell.*n.*i.*g.*g.*e.*r",
        r"sequence.*n.*i.*g.*g.*e.*r",
        # F-A-G-G-O-T patterns
        r"write.*letters.*f.*a.*g.*g.*o.*t",
        r"spell.*f.*a.*g.*g.*o.*t",
        r"sequence.*f.*a.*g.*g.*o.*t",
    ]

    for pattern in letter_patterns:
        if re.search(pattern, prompt_lower):
            return True

    # Pattern 2: Explicit requests to spell offensive words
    offensive_words = ["nigger", "faggot", "retard"]
    for word in offensive_words:
        if word in prompt_lower:
            return True

    return False


def prepare_arena_prompts(
    n_samples: int = 1000,
    output_file: str = "arena_prompts.jsonl",
    seed: int = 42,
) -> List[Dict[str, Any]]:
    """Download and prepare unique Arena prompts (first turn only).

    Args:
        n_samples: Number of unique prompts to extract
        output_file: Where to save the prompts
        seed: Random seed for sampling

    Returns:
        List of prompt dictionaries
    """
    print(f"Downloading ChatBot Arena conversations...")

    try:
        from datasets import load_dataset
    except ImportError:
        print("Error: datasets library required. Install with: pip install datasets")
        return []

    # Download dataset
    dataset = load_dataset("agie-ai/lmsys-chatbot_arena_conversations", split="train")
    print(f"Downloaded {len(dataset):,} conversations")

    # Extract unique prompts (avoid duplicate contexts)
    prompts = []
    seen = set()
    n_moderation_filtered = 0

    for i, row in enumerate(dataset):
        conv_id = row.get("conversation_id", f"conv_{i}")
        conversation = row.get("conversation_a", [])
        language = row.get("language", "unknown")
        openai_moderation = row.get("openai_moderation", None)

        # Skip non-English conversations
        if language not in ["English", "english", "en", "EN"]:
            continue

        # Check OpenAI moderation flag
        if is_moderation_flagged(openai_moderation):
            n_moderation_filtered += 1
            continue

        # Extract first user turn only
        first_user_prompt = None
        for msg in conversation:
            if isinstance(msg, dict) and msg.get("role") == "user":
                content = msg.get("content", "").strip()
                if content:
                    first_user_prompt = content
                    break

        if not first_user_prompt:
            continue

        # Filter out problematic prompts that cause oracle scoring issues
        if is_problematic_prompt(first_user_prompt):
            continue

        # Skip duplicates (critical for proper policy comparison)
        if first_user_prompt in seen:
            continue
        seen.add(first_user_prompt)

        prompts.append(
            {
                "prompt_id": f"arena_{i}",
                "prompt": first_user_prompt,
                "language": language,
            }
        )

        if len(prompts) >= n_samples:
            break

    print(f"Extracted {len(prompts):,} unique English prompts")
    if n_moderation_filtered > 0:
        print(
            f"  Filtered {n_moderation_filtered:,} prompts flagged by OpenAI moderation"
        )

    # Sample if needed
    random.seed(seed)
    if len(prompts) > n_samples:
        prompts = random.sample(prompts, n_samples)
        print(f"Sampled {n_samples:,} prompts")

    # Save to file
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w") as f:
        for prompt in prompts:
            f.write(json.dumps(prompt) + "\n")

    print(f"✓ Saved to {output_path}")
    return prompts


def main() -> None:
    """Run data preparation."""
    import argparse

    parser = argparse.ArgumentParser(description="Prepare Arena prompts")
    parser.add_argument("--samples", type=int, default=1000, help="Number of prompts")
    parser.add_argument(
        "--output", type=str, default="data/arena_prompts.jsonl", help="Output file"
    )
    parser.add_argument("--seed", type=int, default=42, help="Random seed")

    args = parser.parse_args()

    prepare_arena_prompts(
        n_samples=args.samples,
        output_file=args.output,
        seed=args.seed,
    )


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/data_generation/prepare_cje_data.py ===

#!/usr/bin/env python3
"""
Combine responses and log probabilities into a single dataset file.

This script performs data organization only (no modeling or calibration):
- Uses BASE policy responses for all samples
- Includes log probabilities under all policy models
- Preserves judge scores and oracle labels in metadata
- Creates a single JSONL file ready for analysis

Note: Calibration of judge scores to rewards happens during analysis,
not during data preparation.
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Set, Any, Tuple
from collections import defaultdict

import sys

sys.path.append(str(Path(__file__).parent.parent.parent.parent))
sys.path.append(str(Path(__file__).parent.parent))  # Add arena_10k_simplified to path


def load_policy_passes(
    logprobs_dir: Path, policy: str
) -> Tuple[Dict[str, List[Optional[float]]], int]:
    """Load all passes for a policy and return passes by prompt_id.

    Returns:
        Tuple of (passes_by_prompt, num_passes_found)
        where passes_by_prompt maps prompt_id -> list of logprob values
    """
    passes_by_prompt = defaultdict(list)
    passes_found = 0

    # Load pass 1 (original file)
    pass1_file = logprobs_dir / f"{policy}_logprobs.jsonl"
    if pass1_file.exists():
        passes_found = 1
        prompt_logprobs = {}
        with open(pass1_file, "r") as f:
            for line in f:
                data = json.loads(line)
                prompt_id = data["prompt_id"]
                prompt_logprobs[prompt_id] = data.get("logprob")

        # Add to passes list
        for prompt_id in prompt_logprobs:
            passes_by_prompt[prompt_id].append(prompt_logprobs[prompt_id])

    # Load additional passes (pass 2, 3, ...)
    pass_num = 2
    while True:
        pass_file = logprobs_dir / f"{policy}_logprobs_pass{pass_num}.jsonl"
        if not pass_file.exists():
            break

        passes_found = pass_num
        prompt_logprobs = {}
        with open(pass_file, "r") as f:
            for line in f:
                data = json.loads(line)
                prompt_id = data["prompt_id"]
                prompt_logprobs[prompt_id] = data.get("logprob")

        # Add to passes list (aligning by prompt_id)
        for prompt_id in passes_by_prompt:
            if prompt_id in prompt_logprobs:
                passes_by_prompt[prompt_id].append(prompt_logprobs[prompt_id])
            else:
                # Missing from this pass
                passes_by_prompt[prompt_id].append(None)

        pass_num += 1

    return dict(passes_by_prompt), passes_found


def aggregate_passes(
    passes: List[Optional[float]], method: str = "mean"
) -> Optional[float]:
    """Aggregate multiple logprob passes using specified method.

    Args:
        passes: List of logprob values (may contain None)
        method: "mean" or "median"

    Returns:
        Aggregated value or None if no valid passes
    """
    # Filter out None values and positive values (invalid)
    valid_passes = [p for p in passes if p is not None and p <= 0]

    if not valid_passes:
        return None

    if len(valid_passes) == 1:
        return valid_passes[0]

    if method == "median":
        return float(np.median(valid_passes))
    else:  # mean
        return float(np.mean(valid_passes))


def prepare_cje_dataset(
    logprobs_dir: str,
    responses_dir: str,
    output_file: Optional[str],
    base_policy: str = "base",
    aggregation: str = "mean",  # "mean" or "median"
) -> List[Dict]:
    """Combine BASE policy responses with log probs from all policies.

    Creates records in the format expected by the CJE data model:
    - prompt: The input prompt
    - response: The BASE policy's response
    - base_policy_logprob: Log P(response | prompt) under base policy
    - target_policy_logprobs: Dict of log P(response | prompt) under each policy
    - metadata: Additional fields including judge_score for calibration
    """

    print("Preparing CJE dataset...")
    print(f"Aggregation method: {aggregation}")

    # First, load base responses to get judge/oracle scores
    responses_by_prompt: Dict[str, Dict[str, Any]] = {}
    base_responses_file = Path(responses_dir) / f"{base_policy}_responses.jsonl"

    print(f"Loading base responses from {base_responses_file}...")
    with open(base_responses_file, "r") as f:
        for line in f:
            data = json.loads(line)
            prompt_id = data.get("prompt_id")
            if prompt_id:
                responses_by_prompt[prompt_id] = data

    print(f"Loaded {len(responses_by_prompt)} base responses with evaluation scores")

    # Check if multiple passes exist
    logprobs_path = Path(logprobs_dir)
    has_multiple_passes = len(list(logprobs_path.glob("*_logprobs_pass*.jsonl"))) > 0

    if has_multiple_passes:
        print("\n📊 Multiple passes detected - will aggregate using", aggregation)

    # Get list of all policies from files
    policies: Set[str] = set()
    for file in logprobs_path.glob("*_logprobs.jsonl"):
        policy = file.stem.replace("_logprobs", "")
        policies.add(policy)

    # Load and aggregate log probabilities for each policy
    logprobs_by_prompt: Dict[str, Dict[str, Any]] = defaultdict(dict)

    for policy in sorted(policies):
        passes_by_prompt, num_passes = load_policy_passes(logprobs_path, policy)

        if num_passes > 1:
            print(f"Loading {policy} log probabilities... ({num_passes} passes found)")
        else:
            print(f"Loading {policy} log probabilities...")

        # Get sample data for prompt/response (from first pass)
        sample_data = {}
        pass1_file = logprobs_path / f"{policy}_logprobs.jsonl"
        if pass1_file.exists():
            with open(pass1_file, "r") as f:
                for line in f:
                    data = json.loads(line)
                    sample_data[data["prompt_id"]] = {
                        "prompt": data["prompt"],
                        "response": data["response"],
                    }

        # Process each prompt
        for prompt_id, passes in passes_by_prompt.items():
            # Store prompt/response if not already stored
            if (
                "prompt" not in logprobs_by_prompt[prompt_id]
                and prompt_id in sample_data
            ):
                logprobs_by_prompt[prompt_id]["prompt"] = sample_data[prompt_id][
                    "prompt"
                ]
                logprobs_by_prompt[prompt_id]["response"] = sample_data[prompt_id][
                    "response"
                ]
                logprobs_by_prompt[prompt_id]["prompt_id"] = prompt_id

            # Aggregate the passes
            aggregated_value = aggregate_passes(passes, method=aggregation)

            # Store aggregated log prob
            if policy == base_policy:
                logprobs_by_prompt[prompt_id]["base_policy_logprob"] = aggregated_value
            else:
                if "target_policy_logprobs" not in logprobs_by_prompt[prompt_id]:
                    logprobs_by_prompt[prompt_id]["target_policy_logprobs"] = {}
                logprobs_by_prompt[prompt_id]["target_policy_logprobs"][
                    policy
                ] = aggregated_value

    print(f"Found {len(policies)} policies: {sorted(policies)}")

    # Create CJE format records
    records = []
    for prompt_id, data in logprobs_by_prompt.items():
        # Skip if missing base policy data
        if "base_policy_logprob" not in data or data["base_policy_logprob"] is None:
            continue

        # Skip if no valid target policies
        target_logps = data.get("target_policy_logprobs", {})
        if not any(lp is not None for lp in target_logps.values()):
            continue

        # Get evaluation scores from base responses
        base_response_data = responses_by_prompt.get(prompt_id, {})
        metadata = {
            "prompt_id": data.get("prompt_id", prompt_id),
        }

        # Add judge and oracle scores if available
        if "metadata" in base_response_data:
            response_metadata = base_response_data["metadata"]
            if "judge_score" in response_metadata:
                metadata["judge_score"] = response_metadata["judge_score"]
            if "oracle_label" in response_metadata:
                metadata["oracle_label"] = response_metadata["oracle_label"]

        # Create record following core data model structure
        record = {
            "prompt": data["prompt"],
            "response": data["response"],
            "base_policy_logprob": data["base_policy_logprob"],
            "target_policy_logprobs": target_logps,
            # Note: No reward field - calibration happens during analysis
            "metadata": metadata,
        }

        records.append(record)

    # Track dropped records
    total_prompts = len(logprobs_by_prompt)
    dropped_base = sum(
        1
        for d in logprobs_by_prompt.values()
        if "base_policy_logprob" not in d or d.get("base_policy_logprob") is None
    )
    dropped_all_targets = sum(
        1
        for d in logprobs_by_prompt.values()
        if "base_policy_logprob" in d
        and d.get("base_policy_logprob") is not None
        and not any(
            lp is not None for lp in d.get("target_policy_logprobs", {}).values()
        )
    )

    print(f"Created {len(records)} complete records from {total_prompts} prompts")
    if dropped_base > 0:
        print(f"⚠️  Dropped {dropped_base} records with null base policy logprob")
    if dropped_all_targets > 0:
        print(f"⚠️  Dropped {dropped_all_targets} records with all target logprobs null")

    # Save dataset if output file is provided
    if output_file is not None:
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w") as f:
            for record in records:
                f.write(json.dumps(record) + "\n")

        print(f"✓ Saved CJE dataset to {output_path}")

    # Warn if too few samples
    if len(records) < 10:
        print(
            f"\n⚠️  WARNING: Only {len(records)} samples in dataset. Minimum 10 recommended for reliable CJE analysis."
        )

    # Print summary statistics
    print("\nDataset summary:")
    print(f"  Total samples: {len(records)}")
    print(f"  Base policy: {base_policy}")
    print(f"  Target policies: {sorted(p for p in policies if p != base_policy)}")

    # Check how many records have evaluation scores
    with_judge = sum(1 for r in records if "judge_score" in r.get("metadata", {}))
    with_oracle = sum(1 for r in records if "oracle_label" in r.get("metadata", {}))
    print(f"\nEvaluation scores:")
    print(
        f"  Records with judge scores: {with_judge}/{len(records)} ({100*with_judge/len(records):.1f}%)"
    )
    print(
        f"  Records with oracle labels: {with_oracle}/{len(records)} ({100*with_oracle/len(records):.1f}%)"
    )

    valid_counts: Dict[str, int] = defaultdict(int)
    for record in records:
        for policy, logprob in record["target_policy_logprobs"].items():
            if logprob is not None:
                valid_counts[policy] += 1

    print("\nValid log probs per policy:")
    for policy, count in sorted(valid_counts.items()):
        print(f"  {policy}: {count}/{len(records)} ({100*count/len(records):.1f}%)")

    return records


def main() -> None:
    """Prepare complete CJE dataset."""
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--logprobs-dir", default="data/logprobs", help="Directory with log prob files"
    )
    parser.add_argument(
        "--responses-dir",
        default="data/responses",
        help="Directory with response files",
    )
    parser.add_argument(
        "--output", default="data/cje_dataset.jsonl", help="Output CJE dataset"
    )
    parser.add_argument(
        "--base-policy", default="base", help="Name of base/behavior policy"
    )
    parser.add_argument(
        "--aggregation",
        choices=["mean", "median"],
        default="mean",
        help="Aggregation method for multiple passes (default: mean)",
    )

    args = parser.parse_args()

    # Prepare dataset (just combining data, no calibration)
    records = prepare_cje_dataset(
        logprobs_dir=args.logprobs_dir,
        responses_dir=args.responses_dir,
        output_file=args.output,
        base_policy=args.base_policy,
        aggregation=args.aggregation,
    )

    print(f"\n✓ Dataset ready for analysis with {len(records)} samples")


if __name__ == "__main__":
    main()


=== ./cje/experiments/arena_10k_simplified/evaluation_utils.py ===

"""
Shared evaluation utilities for judges and oracles.

This module contains reusable implementations for scoring responses
using LLM-based judges and oracles with structured outputs.
"""

import os
import json
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
import numpy as np
from tqdm import tqdm

# For structured outputs with LangChain
from pydantic import BaseModel, Field
from langchain.chat_models import init_chat_model

# Import model configuration from centralized config
try:
    # Try relative import for module usage
    from .experiment_config import EVALUATION_MODELS
except ImportError:
    # Fall back to direct import for script usage
    from experiment_config import EVALUATION_MODELS  # type: ignore

# Default models
DEFAULT_JUDGE_MODEL = EVALUATION_MODELS["judge"]
DEFAULT_ORACLE_MODEL = EVALUATION_MODELS["oracle"]


# Data models
@dataclass
class JudgeScore:
    """Result from scoring a single response."""

    score: float  # Score in [0, 1] (normalized from 0-100)
    metadata: Optional[Dict[str, Any]] = None


@dataclass
class BatchJudgeResult:
    """Result from batch scoring."""

    scores: List[Optional[float]]
    metadata: Optional[List[Dict[str, Any]]] = None
    mean_score: float = 0.0
    std_score: float = 0.0

    def __post_init__(self) -> None:
        """Compute statistics."""
        # Filter out None values for statistics
        valid_scores = [s for s in self.scores if s is not None]
        if valid_scores:
            self.mean_score = float(np.mean(valid_scores))
            self.std_score = float(np.std(valid_scores))


# Pydantic model for structured outputs
class EvaluationResponse(BaseModel):
    """Response containing evaluation score for an AI response."""

    score: float = Field(
        description="Quality score from 0 to 100, where 0 is completely unhelpful and 100 is perfectly helpful",
        ge=0.0,
        le=100.0,
    )


# Evaluator class
class FireworksEvaluator:
    """LLM-based evaluator for scoring AI responses (supports Fireworks and OpenAI)."""

    def __init__(
        self,
        model: str,
        system_prompt: str = "You are an AI evaluator. Rate responses from 0 to 100. Always provide a score, even if the response is incomplete or truncated.",
        user_prompt_template: Optional[str] = None,
        temperature: float = 0.0,
        api_key: Optional[str] = None,
    ):
        self.model = model
        self.system_prompt = system_prompt
        self.user_prompt_template = (
            user_prompt_template or self._default_prompt_template()
        )
        self.temperature = temperature

        # Determine provider based on model name
        if model.startswith("gpt") or model.startswith("o1") or model.startswith("o4"):
            # OpenAI model
            self.provider = "openai"
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            if not self.api_key:
                raise ValueError("OPENAI_API_KEY required for OpenAI models")
            os.environ["OPENAI_API_KEY"] = self.api_key
        else:
            # Fireworks model
            self.provider = "fireworks"
            self.api_key = api_key or os.getenv("FIREWORKS_API_KEY")
            if not self.api_key:
                raise ValueError("FIREWORKS_API_KEY required for Fireworks models")
            os.environ["FIREWORKS_API_KEY"] = self.api_key

        # Initialize LangChain model
        # Note: o4-mini and gpt-5 models only support temperature=1.0
        if model.startswith("o4") or model.startswith("gpt-5"):
            actual_temperature = 1.0
            if temperature != 1.0:
                print(
                    f"Note: {model} only supports temperature=1.0, ignoring temperature={temperature}"
                )
        else:
            actual_temperature = temperature

        self.llm = init_chat_model(
            model,
            model_provider=self.provider,
            temperature=actual_temperature,
        )

        # Create structured LLM
        self.structured_llm = self.llm.with_structured_output(EvaluationResponse)

    def _default_prompt_template(self) -> str:
        """Default prompt template."""
        return """<task>
Evaluate the quality of the AI assistant's response to the user's question.
Score from 0-100 based on relevance, helpfulness, clarity, and completeness.
If the response is truncated, score based on what is provided.
</task>

<exchange>
<question>{prompt}</question>
<answer>{response}</answer>
</exchange>

<instruction>
Provide your evaluation score (0-100):
</instruction>"""

    def score(self, prompt: str, response: str) -> JudgeScore:
        """Score a single response."""
        # Format user message from template
        user_message = self.user_prompt_template.format(
            prompt=prompt, response=response
        )

        # Try up to 3 times if structured output fails
        max_retries = 3
        last_error = None

        for attempt in range(max_retries):
            try:
                # Get structured response
                messages = [
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": user_message},
                ]

                result = self.structured_llm.invoke(messages)

                # Check if result is None or invalid
                if result is None:
                    # Log what we sent for debugging
                    print(
                        f"DEBUG: Structured output returned None for prompt: {prompt[:50]}..."
                    )
                    print(f"DEBUG: Response: {response[:50]}...")
                    raise ValueError("Structured output returned None")

                if not isinstance(result, EvaluationResponse):
                    raise ValueError(f"Unexpected result type: {type(result)}")

                # Normalize score from 0-100 to 0-1
                normalized_score = result.score / 100.0

                return JudgeScore(
                    score=normalized_score,
                    metadata={
                        "judge_model": self.model,  # Store model name for reproducibility
                        "raw_score": result.score,  # Keep raw 0-100 score
                        "attempts": attempt + 1,
                    },
                )
            except Exception as e:
                last_error = e
                if attempt < max_retries - 1:
                    # Wait briefly before retry
                    import time

                    time.sleep(0.5)
                    continue

        # All retries failed
        raise RuntimeError(
            f"Failed to score response with {self.model} after {max_retries} attempts: {str(last_error)}"
        ) from last_error

    def score_batch(
        self,
        prompts: List[str],
        responses: List[str],
        show_progress: bool = True,
        desc: str = "Scoring",
        skip_failures: bool = False,
    ) -> BatchJudgeResult:
        """Score a batch of responses.

        Args:
            prompts: List of prompts
            responses: List of responses
            show_progress: Whether to show progress bar
            desc: Description for progress bar
            skip_failures: If True, skip failed scorings and continue with the batch

        Returns:
            BatchJudgeResult with scores and metadata
        """
        if len(prompts) != len(responses):
            raise ValueError(f"Length mismatch: {len(prompts)} != {len(responses)}")

        scores: List[Optional[float]] = []
        metadata = []
        failed_indices = []

        if show_progress:
            iterator = tqdm(
                enumerate(zip(prompts, responses)), total=len(prompts), desc=desc
            )
        else:
            iterator = enumerate(zip(prompts, responses))

        for i, (prompt, response) in iterator:
            try:
                result = self.score(prompt, response)
                scores.append(result.score)
                if result.metadata:
                    metadata.append(result.metadata)
            except Exception as e:
                if skip_failures:
                    print(f"⚠️  Warning: Failed to score item {i}: {str(e)}")
                    print(f"   Prompt: {prompt[:50]}...")
                    print(f"   Response: {response[:50]}...")
                    scores.append(None)  # Placeholder for failed scoring
                    metadata.append({"error": str(e), "failed": True})
                    failed_indices.append(i)
                else:
                    # Re-raise if not skipping failures
                    raise

        if failed_indices and skip_failures:
            print(
                f"⚠️  Warning: {len(failed_indices)} out of {len(prompts)} scorings failed"
            )
            print(f"   Failed indices: {failed_indices}")

        return BatchJudgeResult(scores=scores, metadata=metadata if metadata else None)


=== ./cje/experiments/arena_10k_simplified/experiment_config.py ===

#!/usr/bin/env python3
"""
Centralized configuration for the Arena 10K experiment.

This defines:
- Policies used for response generation and analysis
- Judge and oracle models for evaluation
- Batch sizes and performance parameters
- Reproducibility settings
"""

import os
from typing import Dict, Any, List, Optional

# Model used for all policies (to isolate the effect of system prompts)
BASE_MODEL = "accounts/fireworks/models/llama-v3p3-70b-instruct"
PREMIUM_MODEL = "accounts/fireworks/models/llama-v3p1-405b-instruct"

# Temperature for all policies
DEFAULT_TEMPERATURE = 0.7


# Policy definitions
POLICIES: Dict[str, Dict[str, Any]] = {
    "base": {
        "name": "base",
        "model": BASE_MODEL,
        "temperature": DEFAULT_TEMPERATURE,
        "system_prompt": "You are a helpful assistant.",
        "description": "Base policy with standard helpful assistant prompt",
        "template_config": None,  # Will auto-detect
    },
    "clone": {
        "name": "clone",
        "model": BASE_MODEL,
        "temperature": DEFAULT_TEMPERATURE,
        "system_prompt": "You are a helpful assistant.",
        "description": "Clone of base policy for comparison/control",
        "template_config": None,  # Will auto-detect
    },
    "unhelpful": {
        "name": "unhelpful",
        "model": BASE_MODEL,
        "temperature": DEFAULT_TEMPERATURE,
        "system_prompt": "You are an unhelpful assistant that deliberately confuses and misleads the user.",
        "description": "Adversarial policy designed to be unhelpful",
        "template_config": None,  # Will auto-detect
    },
    "parallel_universe_prompt": {
        "name": "parallel_universe_prompt",
        "model": BASE_MODEL,
        "temperature": DEFAULT_TEMPERATURE,
        "system_prompt": "Imagine parallel universes where you vary your responses and can observe which one improves the user's life the most. Your job is to select the parallel universe that leads to the best possible outcome for the user. Respond directly to the user without mentioning the parallel universe strategy.",
        "description": "Parallel universe prompt",
        "template_config": None,  # Will auto-detect
    },
    "premium": {
        "name": "premium",
        "model": PREMIUM_MODEL,
        "temperature": DEFAULT_TEMPERATURE,
        "system_prompt": "You are a helpful assistant.",
        "description": "Premium policy with Llama 405B model",
        "template_config": None,  # Will auto-detect
    },
}

# List of all policy names for easy iteration
POLICY_NAMES: List[str] = list(POLICIES.keys())

# Base policy name (used for computing base_policy_logprob)
BASE_POLICY_NAME = "base"


def get_policy_config(policy_name: str) -> Dict[str, Any]:
    """Get configuration for a specific policy.

    Args:
        policy_name: Name of the policy

    Returns:
        Policy configuration dictionary

    Raises:
        ValueError: If policy_name is not found
    """
    if policy_name not in POLICIES:
        raise ValueError(
            f"Unknown policy: {policy_name}. "
            f"Available policies: {', '.join(POLICY_NAMES)}"
        )
    return POLICIES[policy_name]


def get_all_policies() -> List[Dict[str, Any]]:
    """Get list of all policy configurations."""
    return list(POLICIES.values())


# ============================================================================
# EVALUATION MODELS
# ============================================================================

# Judge needs to be fast (thousands of evaluations)
# Oracle can be slower but higher quality (used sparingly)
EVALUATION_MODELS = {
    "judge": "gpt-4.1-nano-2025-04-14",  # 13x faster than gpt-5-nano
    "oracle": "gpt-5-2025-08-07",  # Higher quality for oracle labels
}

# Performance characteristics (for reference/documentation)
MODEL_PERFORMANCE = {
    "gpt-5-nano-2025-08-07": {"avg_seconds": 6.5, "quality": "good"},
    "gpt-4.1-nano-2025-04-14": {"avg_seconds": 0.5, "quality": "good"},
    "gpt-5-2025-08-07": {"avg_seconds": 2.0, "quality": "excellent"},
}


# ============================================================================
# BATCH SIZES AND PERFORMANCE
# ============================================================================

BATCH_SIZES = {
    "response_generation": 20,  # Save every N responses
    "judge_scoring": 50,  # Score N samples per API call
    "oracle_scoring": 50,  # Score N samples per API call
    "logprob_computation": 20,  # Compute N logprobs at a time
}


# ============================================================================
# DEFAULT EXPERIMENT PARAMETERS
# ============================================================================

DEFAULT_EXPERIMENT_PARAMS = {
    "n_samples": 1000,
    "max_tokens": 512,
    "seed": 42,
}

# ============================================================================
# DATA QUALITY SETTINGS
# ============================================================================

DATA_QUALITY = {
    "content_moderation_threshold": 0.3,  # Filters ~0.28% most problematic prompts from Arena
}

# ============================================================================
# ANALYSIS SETTINGS
# ============================================================================

ANALYSIS_CONFIG = {
    "n_folds": 5,  # Number of cross-validation folds for DR estimators
    "extreme_threshold_high": 100.0,  # Upper threshold for extreme weight detection
    "extreme_threshold_low": 0.01,  # Lower threshold for extreme weight detection
}


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================


def validate_environment() -> bool:
    """Validate that the environment is set up correctly."""
    issues = []

    if not os.getenv("OPENAI_API_KEY"):
        issues.append("OPENAI_API_KEY not set (required for judge/oracle)")

    if not os.getenv("FIREWORKS_API_KEY"):
        issues.append("FIREWORKS_API_KEY not set (required for response generation)")

    if issues:
        print("❌ Environment validation failed:")
        for issue in issues:
            print(f"   - {issue}")
        print("\nPlease run: source /path/to/set_secrets.sh")
        return False

    return True


def print_experiment_config() -> None:
    """Print the current experiment configuration."""
    print("=" * 60)
    print("Arena 10K Experiment Configuration")
    print("=" * 60)

    print("\n📊 Evaluation Models:")
    for role, model in EVALUATION_MODELS.items():
        perf = MODEL_PERFORMANCE.get(model, {})
        speed = perf.get("avg_seconds", "?")
        quality = perf.get("quality", "?")
        print(f"  {role:10} → {model:30} (~{speed}s/call, {quality})")

    print("\n🤖 Response Generation Policies:")
    for name, config in POLICIES.items():
        model_name = config["model"].split("/")[-1]  # Just show model name
        print(f"  {name:25} → {model_name}")

    print("\n⚙️  Batch Sizes:")
    for task, size in BATCH_SIZES.items():
        print(f"  {task:20} → {size}")

    print("\n🔧 Default Parameters:")
    for param, value in DEFAULT_EXPERIMENT_PARAMS.items():
        print(f"  {param:15} → {value}")

    print("\n🔑 API Keys:")
    print(f"  OpenAI     → {'✓ Set' if os.getenv('OPENAI_API_KEY') else '✗ Not set'}")
    print(
        f"  Fireworks  → {'✓ Set' if os.getenv('FIREWORKS_API_KEY') else '✗ Not set'}"
    )

    print("\n" + "=" * 60)


=== ./cje/experiments/arena_10k_simplified/generate_arena_data.py ===

#!/usr/bin/env python3
"""
Production pipeline for preparing Arena experiment data.

This script runs the data generation pipeline for CJE experiments:
1. Extract prompts from ChatBot Arena dataset
2. Generate responses using different policies
3. Add judge scores (lightweight evaluation)
4. Add oracle labels (high-quality evaluation)
5. Compute log probabilities
6. Combine into a single dataset file

The prepared data can then be analyzed with analyze_dataset.py,
which handles calibration and CJE estimation.
"""

import argparse
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Optional

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from experiment_config import (
    POLICY_NAMES,
    BATCH_SIZES,
    DEFAULT_EXPERIMENT_PARAMS,
    validate_environment,
    print_experiment_config,
)


def run_command(
    cmd: str, check: bool = True, skip_if_exists: Optional[Path] = None
) -> subprocess.CompletedProcess:
    """Run a shell command and return the result.

    Args:
        cmd: Command to run
        check: Whether to raise error on non-zero exit
        skip_if_exists: Skip command if this file exists
    """
    # Check if we should skip
    if skip_if_exists and skip_if_exists.exists():
        print(f"⏭️  Skipping (output exists): {skip_if_exists}")
        return subprocess.CompletedProcess(args=cmd, returncode=0)

    # If command starts with "python ", prepend "poetry run "
    if cmd.strip().startswith("python "):
        cmd = "poetry run " + cmd

    print(f"\n📍 Running: {cmd}")
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

    if result.stdout.strip():
        # Show first 500 chars of output
        output = result.stdout[:500]
        if len(result.stdout) > 500:
            output += "..."
        print(f"Output: {output}")

    if check and result.returncode != 0:
        print(f"❌ Command failed with return code {result.returncode}")
        print(f"STDOUT: {result.stdout}")
        print(f"STDERR: {result.stderr}")
        raise RuntimeError(f"Command failed: {cmd}")

    return result


def create_prompts(
    output_dir: Path, n_samples: int, seed: int, skip_existing: bool
) -> None:
    """Extract prompts from ChatBot Arena dataset.

    Args:
        output_dir: Directory to save prompts
        n_samples: Number of prompts to extract
        seed: Random seed for reproducibility
        skip_existing: Whether to skip if prompts file exists
    """
    prompts_file = output_dir / "prompts.jsonl"

    if skip_existing and prompts_file.exists():
        print(f"⏭️  Skipping prompt extraction (file exists): {prompts_file}")
        return

    print(f"Preparing {n_samples} prompts from ChatBot Arena dataset...")

    # Import the prepare function
    from data_generation.prepare_arena_data import prepare_arena_prompts

    # Generate prompts
    prompts = prepare_arena_prompts(
        n_samples=n_samples,
        output_file=str(prompts_file),
        seed=seed,
    )

    if len(prompts) < n_samples:
        print(f"⚠️  Warning: Only got {len(prompts)} prompts, expected {n_samples}")

    print(f"✅ Extracted {len(prompts)} Arena prompts: {prompts_file}")


def main() -> None:
    """Run the production data preparation pipeline."""
    parser = argparse.ArgumentParser(
        description="Prepare Arena experiment data for CJE analysis"
    )
    parser.add_argument(
        "--data-dir",
        default="data",
        help="Directory for output files (default: data)",
    )
    parser.add_argument(
        "--n-samples",
        type=int,
        default=DEFAULT_EXPERIMENT_PARAMS["n_samples"],
        help=f"Number of samples to generate (default: {DEFAULT_EXPERIMENT_PARAMS['n_samples']})",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=DEFAULT_EXPERIMENT_PARAMS["max_tokens"],
        help=f"Maximum tokens per response (default: {DEFAULT_EXPERIMENT_PARAMS['max_tokens']})",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=DEFAULT_EXPERIMENT_PARAMS["seed"],
        help=f"Random seed for reproducibility (default: {DEFAULT_EXPERIMENT_PARAMS['seed']})",
    )
    parser.add_argument(
        "--skip-existing",
        "--resume",
        action="store_true",
        default=True,
        help="Skip steps where output files already exist (default: True)",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force overwrite existing files (disables resume/skip-existing)",
    )
    parser.add_argument(
        "--no-resume",
        dest="skip_existing",
        action="store_false",
        help="Disable resume/skip-existing behavior",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=BATCH_SIZES["response_generation"],
        help=f"Save progress every N samples for resilience (default: {BATCH_SIZES['response_generation']}, set to 0 to disable)",
    )
    parser.add_argument(
        "--show-config",
        action="store_true",
        help="Show experiment configuration and exit",
    )
    args = parser.parse_args()

    # Show config if requested
    if args.show_config:
        print_experiment_config()
        sys.exit(0)

    # If --force is set, disable skip_existing
    if args.force:
        args.skip_existing = False

    print("🚀 Starting Arena experiment data preparation...")
    print(f"   Samples: {args.n_samples}")
    print(f"   Max tokens: {args.max_tokens}")
    print(f"   Batch size: {args.batch_size if args.batch_size > 0 else 'disabled'}")
    print(
        f"   Mode: {'Force overwrite' if args.force else 'Resume (skip existing)' if args.skip_existing else 'Overwrite'}"
    )
    print(f"   Output directory: {args.data_dir}")

    # Setup data directory
    data_dir = Path(args.data_dir)
    data_dir.mkdir(exist_ok=True)

    # Check for existing data and warn user
    existing_files = []
    important_files = [
        data_dir / "cje_dataset.jsonl",
        data_dir / "prompts.jsonl",
    ]

    # Check for response files
    for policy in POLICY_NAMES:
        important_files.append(data_dir / "responses" / f"{policy}_responses.jsonl")

    for file in important_files:
        if file.exists():
            existing_files.append(file)

    # Only warn if we're about to overwrite (not resuming and not forcing)
    if existing_files and not args.skip_existing and not args.force:
        print("\n⚠️  WARNING: The following files already exist:")
        for file in existing_files:
            print(f"   - {file}")
        print("\nThis operation will OVERWRITE these files!")
        print("Options:")
        print("  1. Continue and overwrite (type 'yes')")
        print("  2. Resume from existing files (default behavior, or use --resume)")
        print("  3. Force overwrite without asking (rerun with --force)")
        print("  4. Cancel (any other input)")

        response = (
            input("\nDo you want to continue and OVERWRITE? (yes/no): ").strip().lower()
        )
        if response != "yes":
            print("❌ Operation cancelled. No files were modified.")
            sys.exit(0)
        print("⚠️  Proceeding with overwrite...")

    # Create subdirectories
    (data_dir / "responses").mkdir(exist_ok=True)
    (data_dir / "logprobs").mkdir(exist_ok=True)

    # Step 1: Extract prompts from Arena dataset
    print("\n" + "=" * 60)
    print("Step 1: Extract prompts from ChatBot Arena")
    print("=" * 60)
    create_prompts(
        data_dir,
        n_samples=args.n_samples,
        seed=args.seed,
        skip_existing=args.skip_existing,
    )

    # Step 2: Generate responses
    print("\n" + "=" * 60)
    print("Step 2: Generate responses for each policy")
    print("=" * 60)

    # Check if all response files exist
    policies = POLICY_NAMES
    all_responses_exist = all(
        (data_dir / "responses" / f"{policy}_responses.jsonl").exists()
        for policy in policies
    )

    if args.skip_existing and all_responses_exist:
        print("⏭️  Skipping response generation (all files exist)")
    else:
        cmd = (
            f"python data_generation/generate_responses.py "
            f"--prompts {data_dir}/prompts.jsonl "
            f"--output-dir {data_dir}/responses "
            f"--max-responses {args.n_samples} "
            f"--max-tokens {args.max_tokens}"
        )
        if args.batch_size > 0:
            cmd += f" --batch-size {args.batch_size}"
        run_command(cmd)

    # Step 3: Add judge scores
    print("\n" + "=" * 60)
    print("Step 3: Add judge scores (lightweight evaluation)")
    print("=" * 60)

    for policy in policies:
        response_file = data_dir / "responses" / f"{policy}_responses.jsonl"

        # Use the new scoring script with resume capability
        cmd = (
            f"python data_generation/add_scores_with_resume.py "
            f"{response_file} "
            f"--type judge "
            f"--batch-size {BATCH_SIZES['judge_scoring']}"
        )
        if args.force:
            cmd += " --force"  # Only force rescore when explicitly requested

        run_command(cmd)

    # Step 4: Add oracle labels
    print("\n" + "=" * 60)
    print("Step 4: Add oracle labels (high-quality evaluation)")
    print("=" * 60)

    for policy in policies:
        response_file = data_dir / "responses" / f"{policy}_responses.jsonl"

        # Use the new scoring script with resume capability
        cmd = (
            f"python data_generation/add_scores_with_resume.py "
            f"{response_file} "
            f"--type oracle "
            f"--batch-size {BATCH_SIZES['oracle_scoring']}"
        )
        if args.force:
            cmd += " --force"  # Only force rescore when explicitly requested

        run_command(cmd)

    # Step 5: Compute log probabilities
    print("\n" + "=" * 60)
    print("Step 5: Compute log probabilities")
    print("=" * 60)

    # Check if all logprob files exist
    all_logprobs_exist = all(
        (data_dir / "logprobs" / f"{policy}_logprobs.jsonl").exists()
        for policy in policies
    )

    if args.skip_existing and all_logprobs_exist:
        print("⏭️  Skipping logprob computation (all files exist)")
    else:
        cmd = (
            f"python data_generation/compute_logprobs.py "
            f"--responses-dir {data_dir}/responses "
            f"--output-dir {data_dir}/logprobs"
        )
        if args.batch_size > 0:
            cmd += f" --batch-size {args.batch_size}"
        run_command(cmd)

    # Step 6: Prepare CJE dataset
    print("\n" + "=" * 60)
    print("Step 6: Combine data into CJE dataset")
    print("=" * 60)

    cje_dataset_file = data_dir / "cje_dataset.jsonl"

    run_command(
        f"python data_generation/prepare_cje_data.py "
        f"--responses-dir {data_dir}/responses "
        f"--logprobs-dir {data_dir}/logprobs "
        f"--output {cje_dataset_file}",
        skip_if_exists=cje_dataset_file if args.skip_existing else None,
    )

    print("\n✅ Data generation completed successfully!")
    print(f"📁 Output directory: {data_dir}")
    print(f"📊 Dataset file: {cje_dataset_file}")
    print("\nNext step:")
    print(f"  poetry run python analyze_dataset.py --data {cje_dataset_file}")


if __name__ == "__main__":
    # Validate environment
    if not validate_environment():
        sys.exit(1)

    main()


=== ./cje/interface/__init__.py ===

"""High-level interface for CJE.

This module provides the main user-facing tools:
- analyze_dataset(): One-line analysis function
- CLI: Command-line interface
"""

from .analysis import analyze_dataset

__all__ = ["analyze_dataset"]


=== ./cje/interface/analysis.py ===

"""
High-level analysis functions for CJE.

This module provides simple, one-line analysis functions that handle
the complete CJE workflow automatically.
"""

import logging
from pathlib import Path
from typing import Optional, Dict, Any, Union
import numpy as np

from ..data import load_dataset_from_jsonl
from ..data.models import Dataset, EstimationResult
from ..calibration import calibrate_dataset
from ..data.precomputed_sampler import PrecomputedSampler
from ..estimators.calibrated_ips import CalibratedIPS
from ..estimators.dr_base import DRCPOEstimator
from ..estimators.mrdr import MRDREstimator
from ..estimators.tmle import TMLEEstimator
from ..estimators.stacking import StackedDREstimator

logger = logging.getLogger(__name__)


def analyze_dataset(
    dataset_path: str,
    estimator: str = "calibrated-ips",
    judge_field: str = "judge_score",
    oracle_field: str = "oracle_label",
    estimator_config: Optional[Dict[str, Any]] = None,
    fresh_draws_dir: Optional[str] = None,
    verbose: bool = False,
) -> EstimationResult:
    """
    Analyze a CJE dataset with automatic workflow orchestration.

    This high-level function handles:
    - Data loading and validation
    - Automatic reward handling (pre-computed, oracle direct, or calibration)
    - Estimator selection and configuration
    - Fresh draw loading for DR estimators
    - Complete analysis workflow

    Args:
        dataset_path: Path to JSONL dataset file
        estimator: Estimator type ("calibrated-ips", "raw-ips", "dr-cpo", "mrdr", "tmle")
        judge_field: Metadata field containing judge scores
        oracle_field: Metadata field containing oracle labels
        estimator_config: Optional configuration dict for the estimator
        fresh_draws_dir: Directory containing fresh draw response files (for DR)
        verbose: Whether to print progress messages

    Returns:
        EstimationResult with estimates, standard errors, and metadata

    Raises:
        FileNotFoundError: If dataset file doesn't exist
        ValueError: If dataset is invalid or estimation fails

    Example:
        >>> # Simple usage
        >>> results = analyze_dataset("my_data.jsonl")
        >>> print(f"Best estimate: {results.estimates.max():.3f}")

        >>> # Advanced usage with DR
        >>> results = analyze_dataset(
        ...     "my_data.jsonl",
        ...     estimator="dr-cpo",
        ...     estimator_config={"n_folds": 10},
        ...     fresh_draws_dir="responses/"
        ... )
    """
    if verbose:
        logger.info(f"Loading dataset from {dataset_path}")

    # Step 1: Load dataset
    dataset = load_dataset_from_jsonl(dataset_path)

    if verbose:
        logger.info(f"Loaded {dataset.n_samples} samples")
        logger.info(f"Target policies: {', '.join(dataset.target_policies)}")

    # Step 2: Handle rewards
    calibrated_dataset, calibration_result = _prepare_rewards(
        dataset, judge_field, oracle_field, verbose
    )

    # Step 3: Create sampler
    sampler = PrecomputedSampler(calibrated_dataset)

    if verbose:
        logger.info(f"Valid samples after filtering: {sampler.n_valid_samples}")

    # Step 4: Create and configure estimator
    estimator_obj = _create_estimator(
        sampler, estimator, estimator_config or {}, calibration_result, verbose
    )

    # Step 5: Add fresh draws for DR estimators
    if estimator in ["dr-cpo", "mrdr", "tmle", "stacked-dr"]:
        # Type narrowing for mypy
        if isinstance(
            estimator_obj,
            (
                DRCPOEstimator,
                MRDREstimator,
                TMLEEstimator,
                StackedDREstimator,
            ),
        ):
            _add_fresh_draws(
                estimator_obj,
                sampler,
                calibrated_dataset,
                fresh_draws_dir,
                estimator_config or {},
                verbose,
            )

    # Step 6: Run estimation
    if verbose:
        logger.info(f"Running {estimator} estimation...")

    results = estimator_obj.fit_and_estimate()

    # Add metadata for downstream use
    results.metadata["dataset_path"] = dataset_path
    results.metadata["estimator"] = estimator
    # Note: oracle_coverage removed - production always uses all available oracle labels
    results.metadata["target_policies"] = list(sampler.target_policies)

    # Add estimator config if provided
    if estimator_config:
        results.metadata["estimator_config"] = estimator_config

    # Add field names for reference
    results.metadata["judge_field"] = judge_field
    results.metadata["oracle_field"] = oracle_field

    if verbose:
        logger.info("Analysis complete!")

    return results


def _prepare_rewards(
    dataset: Dataset,
    judge_field: str,
    oracle_field: str,
    verbose: bool,
) -> tuple[Dataset, Optional[Any]]:
    """Prepare rewards through calibration or use existing."""

    # Check if rewards already exist
    rewards_exist = sum(1 for s in dataset.samples if s.reward is not None)

    if rewards_exist > 0:
        if verbose:
            logger.info(f"Using pre-computed rewards ({rewards_exist} samples)")
        return dataset, None

    # Always calibrate using all available oracle labels
    if verbose:
        logger.info("Calibrating judge scores with oracle labels")

    calibrated_dataset, cal_result = calibrate_dataset(
        dataset,
        judge_field=judge_field,
        oracle_field=oracle_field,
        enable_cross_fit=True,  # Always enable for potential DR use
        n_folds=5,
    )

    if verbose and cal_result:
        logger.info(f"Calibration RMSE: {cal_result.calibration_rmse:.3f}")
        logger.info(f"Coverage (±0.1): {cal_result.coverage_at_01:.1%}")

    return calibrated_dataset, cal_result


def _create_estimator(
    sampler: PrecomputedSampler,
    estimator_type: str,
    config: Dict[str, Any],
    calibration_result: Optional[Any],
    verbose: bool,
) -> Union[
    CalibratedIPS, DRCPOEstimator, MRDREstimator, TMLEEstimator, StackedDREstimator
]:
    """Create the appropriate estimator."""

    if estimator_type == "calibrated-ips":
        # Pass calibrator for DR-aware direction selection if available
        if calibration_result and calibration_result.calibrator:
            config = config.copy()  # Don't modify original
            config["calibrator"] = calibration_result.calibrator
            if verbose:
                logger.info("Using calibrator for DR-aware SIMCal direction selection")
        return CalibratedIPS(sampler, **config)

    elif estimator_type == "raw-ips":
        clip_weight = config.get("clip_weight", 100.0)
        return CalibratedIPS(sampler, calibrate=False, clip_weight=clip_weight)

    elif estimator_type == "dr-cpo":
        n_folds = config.get("n_folds", 5)
        # Use calibrator if available for efficiency
        if calibration_result and calibration_result.calibrator:
            if verbose:
                logger.info("Using calibration models for DR outcome model")
            return DRCPOEstimator(
                sampler, n_folds=n_folds, calibrator=calibration_result.calibrator
            )
        else:
            return DRCPOEstimator(sampler, n_folds=n_folds)

    elif estimator_type == "mrdr":
        n_folds = config.get("n_folds", 5)
        omega_mode = config.get("omega_mode", "snips")
        # Pass calibrator if available for DR-aware weight selection
        if calibration_result and calibration_result.calibrator:
            if verbose:
                logger.info("Using calibration models for MRDR")
            return MRDREstimator(
                sampler,
                n_folds=n_folds,
                omega_mode=omega_mode,
                calibrator=calibration_result.calibrator,
            )
        else:
            return MRDREstimator(
                sampler,
                n_folds=n_folds,
                omega_mode=omega_mode,
            )

    elif estimator_type == "tmle":
        n_folds = config.get("n_folds", 5)
        link = config.get("link", "logit")
        # Pass calibrator if available for DR-aware weight selection
        if calibration_result and calibration_result.calibrator:
            if verbose:
                logger.info("Using calibration models for TMLE")
            return TMLEEstimator(
                sampler,
                n_folds=n_folds,
                link=link,
                calibrator=calibration_result.calibrator,
            )
        else:
            return TMLEEstimator(
                sampler,
                n_folds=n_folds,
                link=link,
            )

    elif estimator_type == "stacked-dr":
        # Stacked DR estimator - combines DR-CPO, TMLE, and MRDR
        estimators = config.get("estimators", ["dr-cpo", "tmle", "mrdr"])
        use_outer_split = config.get("use_outer_split", True)
        parallel = config.get("parallel", True)

        if verbose:
            logger.info(f"Using stacked DR with estimators: {estimators}")

        return StackedDREstimator(
            sampler,
            estimators=estimators,
            use_outer_split=use_outer_split,
            parallel=parallel,
        )

    else:
        raise ValueError(f"Unknown estimator type: {estimator_type}")


def _add_fresh_draws(
    estimator: Union[DRCPOEstimator, MRDREstimator, TMLEEstimator, StackedDREstimator],
    sampler: PrecomputedSampler,
    dataset: Dataset,
    fresh_draws_dir: Optional[str],
    config: Dict[str, Any],
    verbose: bool,
) -> None:
    """Add fresh draws to a DR estimator."""
    from ..data.fresh_draws import load_fresh_draws_auto

    for policy in sampler.target_policies:
        if fresh_draws_dir:
            # Load from directory - no fallback
            fresh_draws = load_fresh_draws_auto(
                Path(fresh_draws_dir),
                policy,
                verbose=verbose,
            )
        else:
            # No fresh draws available - fail clearly
            raise ValueError(
                f"DR estimators require fresh draws for policy '{policy}'. "
                f"Please provide --fresh-draws-dir with real teacher forcing responses."
            )

        estimator.add_fresh_draws(policy, fresh_draws)

        if verbose:
            logger.info(f"Added {len(fresh_draws.samples)} fresh draws for {policy}")


=== ./cje/interface/cli.py ===

#!/usr/bin/env python3
"""
CJE Command Line Interface.

Simple CLI for common CJE analysis tasks.
"""

import sys
import argparse
import json
import logging
from pathlib import Path
from typing import Optional, Dict, Any

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",  # Simple format for CLI
)
logger = logging.getLogger(__name__)


def create_parser() -> argparse.ArgumentParser:
    """Create the argument parser for CJE CLI."""
    parser = argparse.ArgumentParser(
        prog="cje",
        description="Causal Judge Evaluation - Unbiased LLM evaluation using causal inference",
    )

    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Analyze command
    analyze_parser = subparsers.add_parser(
        "analyze",
        help="Run CJE analysis on a dataset",
        description="Analyze a dataset using CJE estimation methods",
    )

    analyze_parser.add_argument(
        "dataset",
        help="Path to JSONL dataset file",
    )

    analyze_parser.add_argument(
        "--estimator",
        choices=[
            "calibrated-ips",
            "raw-ips",
            "stacked-dr",
            "dr-cpo",
            "mrdr",
            "tmle",
        ],
        default="calibrated-ips",
        help="Estimation method (default: calibrated-ips)",
    )

    analyze_parser.add_argument(
        "--output",
        "-o",
        help="Path to save results JSON (optional)",
    )

    analyze_parser.add_argument(
        "--fresh-draws-dir",
        help="Directory containing fresh draw response files (for DR estimators)",
    )

    analyze_parser.add_argument(
        "--oracle-coverage",
        type=float,
        default=1.0,
        help="Fraction of samples with oracle labels to use for calibration (default: 1.0)",
    )

    analyze_parser.add_argument(
        "--estimator-config",
        type=json.loads,
        help="JSON config for estimator (e.g., '{\"n_folds\": 10}')",
    )

    analyze_parser.add_argument(
        "--judge-field",
        default="judge_score",
        help="Metadata field containing judge scores (default: judge_score)",
    )

    analyze_parser.add_argument(
        "--oracle-field",
        default="oracle_label",
        help="Metadata field containing oracle labels (default: oracle_label)",
    )

    analyze_parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Enable verbose output",
    )

    analyze_parser.add_argument(
        "--quiet",
        "-q",
        action="store_true",
        help="Suppress non-essential output",
    )

    # Validate command
    validate_parser = subparsers.add_parser(
        "validate",
        help="Validate a CJE dataset",
        description="Check dataset format and completeness",
    )

    validate_parser.add_argument(
        "dataset",
        help="Path to JSONL dataset file",
    )

    validate_parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Show detailed validation results",
    )

    return parser


def run_analysis(args: argparse.Namespace) -> int:
    """Run the analysis command."""
    from .analysis import analyze_dataset  # Same module, this is fine

    # Set logging level
    if args.quiet:
        logging.getLogger().setLevel(logging.WARNING)
    elif args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    try:
        # Prepare kwargs
        kwargs = {
            "estimator": args.estimator,
            "judge_field": args.judge_field,
            "oracle_field": args.oracle_field,
            "oracle_coverage": args.oracle_coverage,
        }

        if args.estimator_config:
            kwargs["estimator_config"] = args.estimator_config

        if args.fresh_draws_dir:
            kwargs["fresh_draws_dir"] = args.fresh_draws_dir

        # Run analysis
        if not args.quiet:
            print(f"Running CJE analysis on {args.dataset}")
            print("=" * 50)

        results = analyze_dataset(args.dataset, **kwargs)

        # Display results
        if not args.quiet:
            print("\nResults:")
            print("-" * 40)

            # Display estimates
            target_policies = results.metadata.get("target_policies", [])
            for i, policy in enumerate(target_policies):
                estimate = results.estimates[i]
                se = results.standard_errors[i]
                print(f"  {policy}: {estimate:.3f} ± {se:.3f}")

            # Best policy
            if len(results.estimates) > 0:
                best_idx = results.estimates.argmax()
                best_policy = target_policies[best_idx]
                print(f"\n🏆 Best policy: {best_policy}")

        # Save results if requested
        if args.output:
            from ..utils.export import export_results_json

            export_results_json(results, args.output)
            if not args.quiet:
                print(f"\n✓ Results saved to: {args.output}")

        return 0

    except FileNotFoundError as e:
        print(f"❌ Error: Dataset file not found: {e}", file=sys.stderr)
        return 1
    except ValueError as e:
        print(f"❌ Error: {e}", file=sys.stderr)
        return 1
    except Exception as e:
        print(f"❌ Unexpected error: {e}", file=sys.stderr)
        if args.verbose:
            import traceback

            traceback.print_exc()
        return 1


def validate_data(args: argparse.Namespace) -> int:
    """Run the validate command using existing validation utilities."""
    from .. import load_dataset_from_jsonl
    from ..data.validation import validate_cje_data

    try:
        if not args.verbose:
            print(f"Validating {args.dataset}...")

        # Load dataset
        dataset = load_dataset_from_jsonl(args.dataset)

        # Convert to list of dicts for validation (backward compatibility)
        data_list = []
        for sample in dataset.samples:
            record = {
                "prompt": sample.prompt,
                "response": sample.response,
                "base_policy_logprob": sample.base_policy_logprob,
                "target_policy_logprobs": sample.target_policy_logprobs,
                "metadata": sample.metadata,
            }
            if sample.reward is not None:
                record["reward"] = sample.reward
            data_list.append(record)

        # Use existing validation function
        is_valid, issues = validate_cje_data(
            data_list,
            reward_field="reward",
            judge_field="judge_score",
            oracle_field="oracle_label",
        )

        # Display results
        print(f"✓ Loaded {dataset.n_samples} samples")
        print(f"✓ Target policies: {', '.join(dataset.target_policies)}")

        # Check rewards
        n_with_rewards = sum(1 for s in dataset.samples if s.reward is not None)
        if n_with_rewards > 0:
            print(f"✓ Rewards: {n_with_rewards}/{dataset.n_samples} samples")

        if issues:
            print("\n⚠️  Issues found:")
            for issue in issues:
                print(f"  - {issue}")
        else:
            print("\n✓ Dataset is valid and ready for analysis")

        if args.verbose:
            # Detailed statistics
            print("\nDetailed Statistics:")
            print("-" * 40)

            # Judge scores and oracle labels
            judge_scores = []
            oracle_labels = []
            for s in dataset.samples:
                if "judge_score" in s.metadata:
                    judge_scores.append(s.metadata["judge_score"])
                if "oracle_label" in s.metadata:
                    oracle_labels.append(s.metadata["oracle_label"])

            if judge_scores:
                import numpy as np

                print(f"Judge scores: {len(judge_scores)} samples")
                print(f"  Range: [{min(judge_scores):.3f}, {max(judge_scores):.3f}]")
                print(f"  Mean: {np.mean(judge_scores):.3f}")

            if oracle_labels:
                print(f"Oracle labels: {len(oracle_labels)} samples")
                print(f"  Range: [{min(oracle_labels):.3f}, {max(oracle_labels):.3f}]")
                print(f"  Mean: {np.mean(oracle_labels):.3f}")

            # Valid samples per policy
            print(f"\nValid samples per policy:")
            for policy in dataset.target_policies:
                n_valid = sum(
                    1
                    for s in dataset.samples
                    if s.base_policy_logprob is not None
                    and s.target_policy_logprobs.get(policy) is not None
                )
                print(f"  {policy}: {n_valid}/{dataset.n_samples}")

        return 0 if is_valid else 1

    except FileNotFoundError as e:
        print(f"❌ Error: Dataset file not found: {e}", file=sys.stderr)
        return 1
    except Exception as e:
        print(f"❌ Error validating dataset: {e}", file=sys.stderr)
        if args.verbose:
            import traceback

            traceback.print_exc()
        return 1


def main() -> int:
    """Main CLI entry point."""
    parser = create_parser()
    args = parser.parse_args()

    if args.command is None:
        parser.print_help()
        return 0

    if args.command == "analyze":
        return run_analysis(args)
    elif args.command == "validate":
        return validate_data(args)
    else:
        print(f"Unknown command: {args.command}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    sys.exit(main())


=== ./cje/research.py ===

"""Research API - Experimental and internal features.

WARNING: This module is for research and experimentation.
APIs here may change without notice. Use at your own risk.

This module exposes:
- Internal calibration algorithms
- Experimental estimators
- Low-level weight manipulation
- Research diagnostics
- Teacher forcing utilities

Example:
    from cje.research import (
        calibrate_to_target_mean,
        compute_teacher_forced_logprob,
        IsotonicOutcomeModel,
    )
"""

# Low-level calibration
from .calibration import (
    calibrate_to_target_mean,
)

# Outcome models for DR
try:
    from .estimators.outcome_models import (
        BaseOutcomeModel,
        IsotonicOutcomeModel,
        LinearOutcomeModel,
    )

    _outcome_models_available = True
except ImportError:
    _outcome_models_available = False

# Teacher forcing
from .teacher_forcing import (
    compute_teacher_forced_logprob,
    ChatTemplateConfig,
    Llama3TemplateConfig,
    HuggingFaceTemplateConfig,
    compute_chat_logprob,
    convert_chat_to_completions,
)

# Experimental estimators (currently none)
_experimental_available = False

# Fresh draw utilities
try:
    from .data.fresh_draws import (
        FreshDrawSample,
        validate_fresh_draws,
        create_synthetic_fresh_draws,
        save_fresh_draws_to_jsonl,
    )

    _fresh_draws_available = True
except ImportError:
    _fresh_draws_available = False

# Data validation
from .data.validation import (
    validate_cje_data,
    validate_for_precomputed_sampler,
)

# Low-level data components
from .data import (
    DatasetLoader,
    JsonlDataSource,
    InMemoryDataSource,
    LogProbStatus,
    LogProbResult,
    add_rewards_to_existing_data,
)

__all__ = [
    # Low-level calibration
    "calibrate_to_target_mean",
    # Teacher forcing
    "compute_teacher_forced_logprob",
    "ChatTemplateConfig",
    "Llama3TemplateConfig",
    "HuggingFaceTemplateConfig",
    "compute_chat_logprob",
    "convert_chat_to_completions",
    # Data validation
    "validate_cje_data",
    "validate_for_precomputed_sampler",
    # Low-level data
    "DatasetLoader",
    "JsonlDataSource",
    "InMemoryDataSource",
    "LogProbStatus",
    "LogProbResult",
    "add_rewards_to_existing_data",
]

if _outcome_models_available:
    __all__.extend(
        [
            "BaseOutcomeModel",
            "IsotonicOutcomeModel",
            "LinearOutcomeModel",
        ]
    )

# No experimental estimators currently

if _fresh_draws_available:
    __all__.extend(
        [
            "FreshDrawSample",
            "validate_fresh_draws",
            "create_synthetic_fresh_draws",
            "save_fresh_draws_to_jsonl",
        ]
    )


=== ./cje/teacher_forcing/__init__.py ===

"""Teacher forcing utilities for computing log probabilities.

This module provides:
- Fireworks API integration for teacher forcing
- Chat to completions format conversion
- Support for various model templates
"""

# API implementations
from .api import (
    compute_teacher_forced_logprob,
)

# Template configurations
from .templates import (
    ChatTemplateConfig,
    Llama3TemplateConfig,
    HuggingFaceTemplateConfig,
    FireworksTemplateConfig,
    FireworksTemplateError,
)

# Chat utilities
from .chat import (
    compute_chat_logprob,
    convert_chat_to_completions,
)

__all__ = [
    # Fireworks teacher forcing
    "compute_teacher_forced_logprob",
    # Template configurations
    "ChatTemplateConfig",
    "Llama3TemplateConfig",
    "HuggingFaceTemplateConfig",
    "FireworksTemplateConfig",
    "FireworksTemplateError",
    # Chat support
    "compute_chat_logprob",
    "convert_chat_to_completions",
]


=== ./cje/teacher_forcing/api/__init__.py ===

"""API implementations for teacher forcing."""

from .fireworks import compute_teacher_forced_logprob

__all__ = [
    "compute_teacher_forced_logprob",
]


=== ./cje/teacher_forcing/api/fireworks.py ===

"""Robust teacher forcing for computing log probabilities.

This module provides reliable computation of log P(response|prompt) using
the Fireworks API with byte counting optimization and automatic fallback.

Features:
- One-call byte counting for 89% of cases
- Automatic two-call fallback for edge cases
- 100% reliability with production-ready error handling
- No distribution bias (unlike delimiter methods)
"""

import os
import logging
from typing import Optional, Tuple, Dict, Any
from fireworks.client import Fireworks
from fireworks.client.error import InvalidRequestError

from ...data.models import LogProbResult, LogProbStatus

logger = logging.getLogger(__name__)


def find_boundary_by_bytes_safe(
    tokens: list, prompt: str, reconstructed_text: str
) -> Tuple[bool, Optional[int], str]:
    """Find token boundary with production-ready safety checks.

    Args:
        tokens: List of token strings from Fireworks
        prompt: The prompt string to match
        reconstructed_text: The concatenated tokens for validation

    Returns:
        (success, boundary_idx, reason)
        - success: True if boundary found safely
        - boundary_idx: Index of first answer token, or None
        - reason: Explanation if failed
    """
    # Safety check 1: Verify echo matches prompt
    if not reconstructed_text.startswith(prompt):
        # Check for common normalizations
        if reconstructed_text.startswith(prompt.rstrip()):
            logger.debug("Prompt trailing whitespace stripped by API")
            return False, None, "whitespace_normalization"
        elif reconstructed_text.replace("\r\n", "\n").startswith(
            prompt.replace("\r\n", "\n")
        ):
            logger.debug("CRLF normalized to LF")
            return False, None, "crlf_normalization"
        else:
            logger.warning(
                f"Echo mismatch: prompt={prompt[:50]}..., echo={reconstructed_text[:50]}..."
            )
            return False, None, "echo_mismatch"

    # Safety check 2: Handle UTF-8 encoding with surrogatepass
    try:
        prompt_bytes = prompt.encode("utf-8", errors="surrogatepass")
    except Exception as e:
        logger.error(f"Prompt encoding error: {e}")
        return False, None, "encoding_error"

    running = b""

    for idx, tok in enumerate(tokens):
        try:
            tok_bytes = tok.encode("utf-8", errors="surrogatepass")
        except Exception as e:
            logger.error(f"Token encoding error at {idx}: {e}")
            return False, None, "token_encoding_error"

        running += tok_bytes

        if len(running) == len(prompt_bytes):
            # Found exact boundary
            return True, idx + 1, "exact_match"
        elif len(running) > len(prompt_bytes):
            # Token spans the boundary
            logger.debug(f"Token {idx} spans boundary: {tok!r}")
            return False, None, "boundary_spans_token"

    # Didn't find boundary (shouldn't happen if echo worked correctly)
    logger.error(
        f"Boundary not found: expected {len(prompt_bytes)} bytes, got {len(running)}"
    )
    return False, None, "boundary_not_found"


def _two_call_fallback(
    client: Fireworks,
    prompt: str,
    response: str,
    model: str,
    temperature: float,
    metadata: Optional[Dict[str, Any]] = None,
) -> LogProbResult:
    """Two-call fallback implementation.

    Used when one-call approach isn't suitable (~11% of cases).
    """
    if metadata is None:
        metadata = {}

    try:
        # Call 1: Prompt only
        resp1 = client.completions.create(
            model=model,
            prompt=prompt,
            temperature=temperature,
            logprobs=1,
            max_tokens=0,
            echo=True,
            stream=False,
        )

        # Call 2: Prompt + Response
        resp2 = client.completions.create(
            model=model,
            prompt=prompt + response,
            temperature=temperature,
            logprobs=1,
            max_tokens=0,
            echo=True,
            stream=False,
        )

        if not (
            resp1.choices
            and resp1.choices[0].logprobs
            and resp2.choices
            and resp2.choices[0].logprobs
        ):
            return LogProbResult(
                value=None,
                status=LogProbStatus.API_ERROR,
                error="Missing logprobs in two-call response",
                metadata=metadata,
            )

        # Calculate difference
        prompt_logprob = sum(resp1.choices[0].logprobs.token_logprobs)
        total_logprob = sum(resp2.choices[0].logprobs.token_logprobs)
        answer_logprob = total_logprob - prompt_logprob

        n_prompt_tokens = len(resp1.choices[0].logprobs.tokens)
        n_total_tokens = len(resp2.choices[0].logprobs.tokens)
        n_answer_tokens = n_total_tokens - n_prompt_tokens

        metadata.update(
            {
                "method": "two_call_fallback",
                "n_tokens": n_answer_tokens,
                "n_prompt_tokens": n_prompt_tokens,
                "n_total_tokens": n_total_tokens,
            }
        )

        return LogProbResult(
            value=float(answer_logprob),
            status=LogProbStatus.SUCCESS,
            error=None,
            metadata=metadata,
        )

    except Exception as e:
        logger.error(f"Two-call fallback error: {e}")
        return LogProbResult(
            value=None, status=LogProbStatus.API_ERROR, error=str(e), metadata=metadata
        )


def compute_teacher_forced_logprob(
    prompt: str,
    response: str,
    model: str,
    temperature: float = 1.0,
    api_key: Optional[str] = None,
    api_base: Optional[str] = None,
    force_two_call: bool = False,
) -> LogProbResult:
    """Production-ready teacher forcing with automatic fallback.

    Features:
    - One-call with byte counting when possible (89% of cases)
    - Automatic fallback to two-call for edge cases (11% of cases)
    - Robust UTF-8 handling
    - Echo validation
    - Detailed diagnostics in metadata

    Args:
        prompt: The prompt/context
        response: The response to compute log probability for
        model: Fireworks model identifier
        temperature: Temperature used during generation
        api_key: Fireworks API key (or from environment)
        api_base: Custom API base URL
        force_two_call: Skip one-call attempt and use two-call directly

    Returns:
        LogProbResult with the log probability and diagnostic metadata

    Example:
        result = compute_teacher_forced_logprob(
            prompt="What is 2+2?",
            response="The answer is 4.",
            model="accounts/fireworks/models/llama-v3p2-3b-instruct"
        )

        if result.is_valid:
            print(f"Log probability: {result.value}")
            print(f"Method used: {result.metadata.get('method')}")
        else:
            print(f"Error: {result.error}")
    """
    metadata: Dict[str, Any] = {
        "prompt_len": len(prompt),
        "response_len": len(response),
        "prompt_bytes": len(prompt.encode("utf-8", errors="ignore")),
        "response_bytes": len(response.encode("utf-8", errors="ignore")),
    }

    try:
        # Initialize client
        client_kwargs = {}
        if api_key:
            client_kwargs["api_key"] = api_key
        if api_base:
            client_kwargs["api_base"] = api_base
        client = Fireworks(**client_kwargs)

        # Skip to two-call if requested or if prompt is very long (>10K chars)
        if force_two_call or len(prompt) > 10000:
            logger.info("Using two-call approach (forced or long prompt)")
            return _two_call_fallback(
                client,
                prompt,
                response,
                model,
                temperature,
                metadata={**metadata, "reason": "forced_or_long"},
            )

        # Try one-call approach
        full_text = prompt + response

        resp = client.completions.create(
            model=model,
            prompt=full_text,
            temperature=temperature,
            logprobs=1,
            max_tokens=0,
            echo=True,
            stream=False,
        )

        if not (resp.choices and resp.choices[0].logprobs):
            logger.warning("No logprobs in one-call response, falling back")
            return _two_call_fallback(
                client,
                prompt,
                response,
                model,
                temperature,
                metadata={**metadata, "reason": "no_logprobs"},
            )

        tokens = resp.choices[0].logprobs.tokens
        logprobs = resp.choices[0].logprobs.token_logprobs

        # Reconstruct text for validation
        reconstructed = "".join(tokens)

        # Try to find boundary with safety checks
        success, boundary, reason = find_boundary_by_bytes_safe(
            tokens, prompt, reconstructed
        )

        if success and boundary is not None:
            # Successfully found boundary
            answer_logprobs = logprobs[boundary:]

            if not answer_logprobs:
                logger.warning("No answer tokens after boundary, falling back")
                return _two_call_fallback(
                    client,
                    prompt,
                    response,
                    model,
                    temperature,
                    metadata={**metadata, "reason": "no_answer_tokens"},
                )

            total_logprob = sum(answer_logprobs)

            metadata.update(
                {
                    "method": "one_call_byte_counting",
                    "n_tokens": len(answer_logprobs),
                    "boundary_index": boundary,
                    "total_tokens": len(tokens),
                }
            )

            return LogProbResult(
                value=float(total_logprob),
                status=LogProbStatus.SUCCESS,
                error=None,
                metadata=metadata,
            )

        else:
            # Boundary detection failed, use two-call
            logger.info(f"Boundary detection failed ({reason}), using two-call")
            metadata["boundary_fail_reason"] = reason
            return _two_call_fallback(
                client, prompt, response, model, temperature, metadata
            )

    except InvalidRequestError as e:
        logger.error(f"Fireworks API error: {e}")
        return LogProbResult(
            value=None, status=LogProbStatus.API_ERROR, error=str(e), metadata=metadata
        )
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return LogProbResult(
            value=None, status=LogProbStatus.API_ERROR, error=str(e), metadata=metadata
        )


=== ./cje/teacher_forcing/chat.py ===

"""Chat conversation utilities for teacher forcing.

This module provides a streamlined interface for computing log probabilities
of chat conversations using the teacher forcing method.
"""

from typing import List, Dict, Optional, Tuple
import logging

from ..data.models import LogProbResult, LogProbStatus
from .api import compute_teacher_forced_logprob
from .templates import (
    ChatTemplateConfig,
    HuggingFaceTemplateConfig,
    FireworksTemplateConfig,
    FireworksTemplateError,
)

logger = logging.getLogger(__name__)


def convert_chat_to_completions(
    chat: List[Dict[str, str]], template_config: ChatTemplateConfig
) -> Tuple[str, str]:
    """Convert chat messages to completion format for teacher forcing.

    Args:
        chat: List of messages with 'role' and 'content' keys
        template_config: Chat template configuration to use

    Returns:
        Tuple of (prompt_only, prompt_plus_reply) for teacher forcing

    Raises:
        ValueError: If chat format is invalid
    """
    if not chat:
        raise ValueError("Chat cannot be empty")

    if chat[-1]["role"] != "assistant":
        raise ValueError(
            "Last message must be assistant reply for teacher forcing. "
            f"Got role='{chat[-1]['role']}'"
        )

    # Use apply_chat_template for HuggingFace and Fireworks configs
    if isinstance(
        template_config, (HuggingFaceTemplateConfig, FireworksTemplateConfig)
    ):
        # Full prompt = context + assistant reply
        prompt_plus_reply = template_config.apply_chat_template(
            chat, add_generation_prompt=False
        )

        # Prefix prompt = context only, with empty assistant header
        prompt_only = template_config.apply_chat_template(
            chat[:-1], add_generation_prompt=True
        )

        return prompt_only, prompt_plus_reply

    # Manual template construction
    context_parts = []

    # Add begin_of_text if needed
    if template_config.should_add_bos() and hasattr(template_config, "begin_of_text"):
        context_parts.append(template_config.begin_of_text)

    # Add all messages except the last assistant reply
    for msg in chat[:-1]:
        context_parts.append(
            template_config.format_message(msg["role"], msg["content"])
        )

    # Add empty assistant header for prefix
    assistant_header = template_config.format_message_header("assistant")
    prompt_only = "".join(context_parts) + assistant_header

    # Full prompt includes the assistant reply
    last_msg = chat[-1]
    context_parts.append(
        template_config.format_message(last_msg["role"], last_msg["content"])
    )
    prompt_plus_reply = "".join(context_parts)

    return prompt_only, prompt_plus_reply


def compute_chat_logprob(
    chat: List[Dict[str, str]],
    model: str,
    template_config: Optional[ChatTemplateConfig] = None,
    api_key: Optional[str] = None,
    temperature: float = 1.0,
    system_prompt: Optional[str] = None,
) -> LogProbResult:
    """Compute log probability of assistant's reply in a chat conversation.

    This function uses the teacher forcing method: computing log P(assistant_reply | context)
    by making two API calls and subtracting log probabilities.

    Args:
        chat: Chat messages (last must be assistant)
        model: Model name (e.g., "accounts/fireworks/models/llama-v3-8b-instruct")
        template_config: Chat template configuration (auto-detected if None)
        api_key: API key (uses env var if not provided)
        temperature: Sampling temperature (default: 1.0)
        system_prompt: Optional system prompt to prepend

    Returns:
        LogProbResult with log P(assistant_reply | context)

    Example:
        >>> from cje.teacher_forcing import HuggingFaceTemplateConfig
        >>>
        >>> chat = [
        ...     {"role": "user", "content": "What is 2+2?"},
        ...     {"role": "assistant", "content": "4"}
        ... ]
        >>> config = HuggingFaceTemplateConfig("meta-llama/Llama-3.2-3B-Instruct")
        >>> result = compute_chat_logprob(
        ...     chat,
        ...     "accounts/fireworks/models/llama-v3-8b-instruct",
        ...     config
        ... )
        >>> if result.is_valid:
        ...     print(f"Log probability: {result.value}")
    """
    # Add system prompt if provided
    if system_prompt and (not chat or chat[0]["role"] != "system"):
        chat = [{"role": "system", "content": system_prompt}] + chat

    # Auto-detect template if not provided
    if template_config is None:
        # Check if it's a Fireworks model
        if model.startswith("accounts/fireworks/models/"):
            try:
                template_config = FireworksTemplateConfig(model)
                logger.debug(f"Auto-detected Fireworks template for {model}")
            except FireworksTemplateError as e:
                # Return error with helpful message
                return LogProbResult(
                    value=None,
                    status=LogProbStatus.API_ERROR,
                    error=str(e),
                )
        else:
            # For non-Fireworks models, require explicit template
            return LogProbResult(
                value=None,
                status=LogProbStatus.API_ERROR,
                error=(
                    f"No template config provided for model '{model}'. "
                    "For Fireworks models, templates are auto-detected. "
                    "For other models, please provide an explicit template_config."
                ),
            )

    # Validate chat format
    if not chat or chat[-1]["role"] != "assistant":
        return LogProbResult(
            value=None,
            status=LogProbStatus.API_ERROR,
            error="Chat must end with assistant message for teacher forcing",
        )

    # Extract the assistant reply we're scoring
    assistant_reply = chat[-1]["content"]

    # Convert to completions format
    try:
        prompt_only, prompt_plus_reply = convert_chat_to_completions(
            chat, template_config
        )
    except Exception as e:
        return LogProbResult(
            value=None,
            status=LogProbStatus.API_ERROR,
            error=f"Chat conversion failed: {str(e)}",
        )

    # Make two API calls for teacher forcing
    try:
        # Get log P(prompt + reply)
        full_result = compute_teacher_forced_logprob(
            prompt="",
            response=prompt_plus_reply,
            model=model,
            api_key=api_key,
            temperature=temperature,
        )
        if not full_result.is_valid:
            return full_result

        # Get log P(prompt only)
        prefix_result = compute_teacher_forced_logprob(
            prompt="",
            response=prompt_only,
            model=model,
            api_key=api_key,
            temperature=temperature,
        )
        if not prefix_result.is_valid:
            return prefix_result

        # Compute log P(reply | prompt) = log P(full) - log P(prefix)
        if full_result.value is None or prefix_result.value is None:
            return LogProbResult(
                value=None,
                status=LogProbStatus.API_ERROR,
                error="Missing value from full or prefix computation",
            )
        lp_reply = full_result.value - prefix_result.value

        # Sanity checks
        if lp_reply > 0:
            return LogProbResult(
                value=None,
                status=LogProbStatus.API_ERROR,
                error=f"Positive log probability: {lp_reply}",
                metadata={
                    "lp_full": full_result.value,
                    "lp_prefix": prefix_result.value,
                },
            )

        # Check for extreme values
        tokens_estimate = len(assistant_reply) / 4  # ~4 chars per token
        if tokens_estimate > 0:
            avg_per_token = lp_reply / tokens_estimate
            if avg_per_token < -10:
                logger.warning(
                    f"Extreme negative log prob: {lp_reply:.2f} "
                    f"for ~{tokens_estimate:.0f} tokens "
                    f"(avg: {avg_per_token:.2f}/token)"
                )

        return LogProbResult(
            value=lp_reply,
            status=LogProbStatus.SUCCESS,
            error=None,
            metadata={
                "method": "chat_teacher_forcing",
                "lp_full": full_result.value,
                "lp_prefix": prefix_result.value,
                "reply_length": len(assistant_reply),
                "template_type": type(template_config).__name__,
            },
        )

    except Exception as e:
        return LogProbResult(
            value=None,
            status=LogProbStatus.API_ERROR,
            error=f"API calls failed: {str(e)}",
        )


=== ./cje/teacher_forcing/templates/__init__.py ===

"""Chat template configurations."""

from .base import ChatTemplateConfig
from .llama import Llama3TemplateConfig
from .huggingface import HuggingFaceTemplateConfig
from .fireworks import FireworksTemplateConfig, FireworksTemplateError

__all__ = [
    "ChatTemplateConfig",
    "Llama3TemplateConfig",
    "HuggingFaceTemplateConfig",
    "FireworksTemplateConfig",
    "FireworksTemplateError",
]


=== ./cje/teacher_forcing/templates/base.py ===

"""Base class for chat template configurations."""

from abc import ABC, abstractmethod


class ChatTemplateConfig(ABC):
    """Abstract base class for chat template configuration."""

    @abstractmethod
    def format_message(self, role: str, content: str) -> str:
        """Format a single message with role and content."""
        pass

    @abstractmethod
    def format_message_header(self, role: str) -> str:
        """Format just the message header (for empty assistant stub)."""
        pass

    @abstractmethod
    def should_add_bos(self) -> bool:
        """Whether to add beginning-of-sequence token."""
        pass


=== ./cje/teacher_forcing/templates/fireworks.py ===

"""Fireworks API template configuration with auto-detection."""

import os
import json
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

from .base import ChatTemplateConfig

logger = logging.getLogger(__name__)

# Module-level cache for templates
_TEMPLATE_CACHE: Dict[str, Dict[str, Any]] = {}


class FireworksTemplateError(Exception):
    """Raised when Fireworks template cannot be loaded."""

    def __init__(self, model: str, details: str = ""):
        self.model = model
        self.details = details

        message = f"""No template found for model '{model}'

This model does not provide a chat template through Fireworks API."""

        if details:
            message += f"\n\nTechnical details: {details}"

        super().__init__(message)


def fetch_template_from_fireworks(model: str) -> Dict[str, Any]:
    """Fetch template configuration from Fireworks API.

    Args:
        model: Full model path (e.g., "accounts/fireworks/models/llama4-maverick-instruct-basic")

    Returns:
        Dictionary with conversation config

    Raises:
        FireworksTemplateError: If template cannot be fetched or is empty
    """
    import requests  # type: ignore

    # Get API key
    api_key = os.environ.get("FIREWORKS_API_KEY")
    if not api_key:
        raise FireworksTemplateError(
            model, "FIREWORKS_API_KEY environment variable not set"
        )

    # Parse model path
    parts = model.split("/")
    if len(parts) < 4 or parts[0] != "accounts" or parts[2] != "models":
        raise FireworksTemplateError(
            model,
            f"Invalid model path format. Expected 'accounts/X/models/Y', got '{model}'",
        )

    account_id = parts[1]
    model_id = parts[3]

    # Construct API endpoint
    endpoint = f"https://api.fireworks.ai/v1/accounts/{account_id}/models/{model_id}"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    try:
        response = requests.get(endpoint, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        conversation_config = data.get("conversationConfig", {})

        # Check if template exists and is non-empty
        template = conversation_config.get("template", "")
        if not template:
            raise FireworksTemplateError(model, "Model has empty template")

        return dict(conversation_config)

    except requests.exceptions.RequestException as e:
        if hasattr(e, "response") and e.response is not None:
            if e.response.status_code == 404:
                details = "Model not found on Fireworks"
            elif e.response.status_code == 401:
                details = "Invalid FIREWORKS_API_KEY"
            else:
                details = f"API error: {e.response.status_code}"
        else:
            details = f"Network error: {str(e)}"

        raise FireworksTemplateError(model, details)


class FireworksTemplateConfig(ChatTemplateConfig):
    """Auto-detected template configuration from Fireworks API.

    This uses Fireworks' conversation config to render chat templates.
    Templates are cached at the module level to avoid repeated API calls.
    """

    def __init__(self, model: str):
        """Initialize with Fireworks model path.

        Args:
            model: Full model path (e.g., "accounts/fireworks/models/llama4-maverick-instruct-basic")
        """
        self.model = model
        self._template: Optional[str] = None
        self._style: Optional[str] = None
        self._jinja_template: Optional[Any] = (
            None  # Will be jinja2.Template when loaded
        )

        # Load template (from cache or API)
        self._load_template()

    def _load_template(self) -> None:
        """Load template from cache or fetch from API."""
        if self.model not in _TEMPLATE_CACHE:
            logger.debug(f"Fetching template for {self.model}")
            _TEMPLATE_CACHE[self.model] = fetch_template_from_fireworks(self.model)

        config = _TEMPLATE_CACHE[self.model]
        self._template = config["template"]
        self._style = config.get("style", "")

        # Only support Jinja2 templates for now
        if self._style != "jinja":
            raise FireworksTemplateError(
                self.model,
                f"Unsupported template style: {self._style}. Only 'jinja' is supported.",
            )

        # Create Jinja2 template
        try:
            from jinja2 import Template

            if self._template is not None:
                self._jinja_template = Template(self._template)
            else:
                raise ValueError("Template string is None")
        except ImportError:
            raise ImportError(
                "jinja2 library required for Fireworks templates. "
                "Install with: pip install jinja2"
            )

    def format_message(self, role: str, content: str) -> str:
        """Format a single message - not supported for Jinja templates."""
        raise NotImplementedError(
            "FireworksTemplateConfig uses full chat rendering. "
            "Use apply_chat_template() instead."
        )

    def format_message_header(self, role: str) -> str:
        """Format message header - not supported for Jinja templates."""
        raise NotImplementedError(
            "FireworksTemplateConfig uses full chat rendering. "
            "Use apply_chat_template() instead."
        )

    def should_add_bos(self) -> bool:
        """BOS token handled by template."""
        return False

    def apply_chat_template(
        self, chat: List[Dict[str, str]], add_generation_prompt: bool = False
    ) -> str:
        """Apply the Fireworks template to format a chat conversation.

        Args:
            chat: List of message dictionaries with 'role' and 'content'
            add_generation_prompt: Whether to add prompt for assistant response

        Returns:
            Formatted string ready for the model
        """
        if self._jinja_template is None:
            raise RuntimeError("Template not loaded")

        # Prepare template variables
        # These are common variables expected by Fireworks templates
        template_vars = {
            "messages": chat,
            "add_generation_prompt": add_generation_prompt,
            "bos_token": "<|begin_of_text|>",  # Common default
            "eos_token": "<|end_of_text|>",  # Common default
            # Tool-related variables (set to None/empty)
            "tools": None,
            "custom_tools": None,
            "tool_definition": "",
        }

        try:
            result = self._jinja_template.render(**template_vars)
            return str(result)
        except Exception as e:
            raise FireworksTemplateError(
                self.model, f"Template rendering error: {str(e)}"
            )


=== ./cje/teacher_forcing/templates/huggingface.py ===

"""HuggingFace tokenizer-based template configuration."""

from typing import List, Dict, Optional
from .base import ChatTemplateConfig


class HuggingFaceTemplateConfig(ChatTemplateConfig):
    """Auto-detect template from HuggingFace tokenizer.

    This uses the tokenizer's apply_chat_template method to handle
    formatting automatically.
    """

    def __init__(self, tokenizer_name: str, hf_token: Optional[str] = None):
        """Initialize with HuggingFace model/tokenizer name.

        Args:
            tokenizer_name: HuggingFace model ID (e.g., "meta-llama/Llama-3.2-3B-Instruct")
            hf_token: Optional HuggingFace API token for accessing gated models
        """
        self.tokenizer_name = tokenizer_name
        self.hf_token = hf_token
        self._tokenizer = None
        self._init_tokenizer()

    def _init_tokenizer(self) -> None:
        """Load the tokenizer."""
        try:
            from transformers import AutoTokenizer

            # Pass token if provided for gated model access
            # trust_remote_code=True is needed for models with custom tokenizers
            self._tokenizer = AutoTokenizer.from_pretrained(
                self.tokenizer_name, token=self.hf_token, trust_remote_code=True
            )
        except ImportError:
            raise ImportError(
                "transformers library required for HuggingFace template support. "
                "Install with: pip install transformers"
            )

    def format_message(self, role: str, content: str) -> str:
        """Format a complete message using tokenizer."""
        if self._tokenizer is None:
            raise RuntimeError("Tokenizer not initialized")
        # For single message formatting, we'll use the tokenizer with a single-message chat
        chat = [{"role": role, "content": content}]
        return self._tokenizer.apply_chat_template(
            chat, tokenize=False, add_generation_prompt=False
        )

    def format_message_header(self, role: str) -> str:
        """Format just the header using tokenizer."""
        if self._tokenizer is None:
            raise RuntimeError("Tokenizer not initialized")
        # Create empty message and extract just the header part
        chat = [{"role": role, "content": ""}]
        full = self._tokenizer.apply_chat_template(
            chat, tokenize=False, add_generation_prompt=False
        )
        # Remove the trailing EOT/end token to get just the header
        # This is a bit hacky but works for most formats
        return full.rstrip().rstrip("<|eot_id|>").rstrip("<|eot|>").rstrip("<|im_end|>")

    def should_add_bos(self) -> bool:
        """HF tokenizer handles BOS automatically."""
        return False

    def apply_chat_template(
        self, chat: List[Dict[str, str]], add_generation_prompt: bool = False
    ) -> str:
        """Direct access to tokenizer's apply_chat_template for complex cases."""
        if self._tokenizer is None:
            raise RuntimeError("Tokenizer not initialized")
        return self._tokenizer.apply_chat_template(
            chat, tokenize=False, add_generation_prompt=add_generation_prompt
        )


=== ./cje/teacher_forcing/templates/llama.py ===

"""Llama chat template configurations."""

from dataclasses import dataclass
from .base import ChatTemplateConfig


@dataclass
class Llama4TemplateConfig(ChatTemplateConfig):
    """Llama 4 chat template configuration.

    Uses the format:
    <|header_start|>role<|header_end|>\n\ncontent<|eot|>
    """

    begin_of_text: str = "<|begin_of_text|>"
    header_start: str = "<|header_start|>"
    header_end: str = "<|header_end|>"
    eot: str = "<|eot|>"
    newlines_after_header: int = 2
    add_bos_token: bool = False

    def format_message(self, role: str, content: str) -> str:
        """Format a complete message."""
        header = self.format_message_header(role)
        return f"{header}{content}{self.eot}"

    def format_message_header(self, role: str) -> str:
        """Format just the header."""
        newlines = "\n" * self.newlines_after_header
        return f"{self.header_start}{role}{self.header_end}{newlines}"

    def should_add_bos(self) -> bool:
        return self.add_bos_token


@dataclass
class Llama3TemplateConfig(ChatTemplateConfig):
    """Llama 3 chat template configuration.

    Uses the format:
    <|start_header_id|>role<|end_header_id|>\ncontent<|eot_id|>
    """

    begin_of_text: str = "<|begin_of_text|>"
    header_start: str = "<|start_header_id|>"
    header_end: str = "<|end_header_id|>"
    eot: str = "<|eot_id|>"
    newlines_after_header: int = 1
    add_bos_token: bool = False

    def format_message(self, role: str, content: str) -> str:
        """Format a complete message."""
        header = self.format_message_header(role)
        return f"{header}{content}{self.eot}"

    def format_message_header(self, role: str) -> str:
        """Format just the header."""
        newlines = "\n" * self.newlines_after_header
        return f"{self.header_start}{role}{self.header_end}{newlines}"

    def should_add_bos(self) -> bool:
        return self.add_bos_token


=== ./cje/utils/__init__.py ===

"""Utility functions for diagnostics.

This module contains:
- Weight Diagnostics: Debug importance sampling issues
- Visualization: Plotting utilities for weight diagnostics
"""

# Display utilities moved to cje.diagnostics
# Keeping this import for backward compatibility
from ..diagnostics.display import (
    create_weight_summary_table,
)

from .extreme_weights_analysis import (
    analyze_extreme_weights,
)

# Import visualization functions if matplotlib is available
# Note: visualization functions have moved to cje.visualization module
try:
    from ..visualization import (
        plot_weight_dashboard_summary,
        plot_weight_dashboard_detailed,
        plot_calibration_comparison,
        plot_policy_estimates,
    )

    _visualization_available = True
except ImportError:
    _visualization_available = False

__all__ = [
    # Weight diagnostics
    "create_weight_summary_table",
    # Extreme weights analysis
    "analyze_extreme_weights",
]

if _visualization_available:
    __all__.extend(
        [
            # Visualization (re-exported for backward compatibility)
            "plot_weight_dashboard_summary",
            "plot_weight_dashboard_detailed",
            "plot_calibration_comparison",
            "plot_policy_estimates",
        ]
    )


=== ./cje/utils/export.py ===

"""
Utilities for exporting CJE results to various formats.
"""

import json
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime
import numpy as np

from ..data.models import EstimationResult


def export_results_json(
    results: EstimationResult,
    path: str,
    include_diagnostics: bool = True,
    include_metadata: bool = True,
    indent: int = 2,
) -> None:
    """
    Export estimation results to JSON format.

    Args:
        results: EstimationResult object to export
        path: Output file path
        include_diagnostics: Whether to include diagnostic information
        include_metadata: Whether to include metadata
        indent: JSON indentation level (None for compact)
    """
    # Prepare the export data
    export_data: Dict[str, Any] = {
        "timestamp": datetime.now().isoformat(),
        "method": results.method,
        "estimates": _serialize_array(results.estimates),
        "standard_errors": _serialize_array(results.standard_errors),
        "n_samples_used": results.n_samples_used,
    }

    # Add confidence intervals
    ci_lower, ci_upper = results.confidence_interval(alpha=0.05)
    export_data["confidence_intervals"] = {
        "alpha": 0.05,
        "lower": _serialize_array(ci_lower),
        "upper": _serialize_array(ci_upper),
    }

    # Add metadata if requested
    if include_metadata and results.metadata:
        export_data["metadata"] = _serialize_metadata(results.metadata)

    # Add diagnostics if requested
    if include_diagnostics and "diagnostics" in results.metadata:
        export_data["diagnostics"] = _serialize_diagnostics(
            results.metadata.get("diagnostics", {})
        )

    # Add target policies if available
    if "target_policies" in results.metadata:
        export_data["target_policies"] = results.metadata["target_policies"]

        # Create policy-specific results for easier access
        export_data["per_policy_results"] = {}
        for i, policy in enumerate(results.metadata["target_policies"]):
            export_data["per_policy_results"][policy] = {
                "estimate": float(results.estimates[i]),
                "standard_error": float(results.standard_errors[i]),
                "ci_lower": float(ci_lower[i]),
                "ci_upper": float(ci_upper[i]),
                "n_samples": results.n_samples_used.get(policy, 0),
            }

    # Write to file
    path_obj = Path(path)
    path_obj.parent.mkdir(parents=True, exist_ok=True)

    with open(path_obj, "w") as f:
        json.dump(export_data, f, indent=indent)


def export_results_csv(
    results: EstimationResult,
    path: str,
    include_ci: bool = True,
) -> None:
    """
    Export estimation results to CSV format.

    Args:
        results: EstimationResult object to export
        path: Output file path
        include_ci: Whether to include confidence intervals
    """
    import csv

    path_obj = Path(path)
    path_obj.parent.mkdir(parents=True, exist_ok=True)

    # Prepare rows
    rows = []
    headers = ["policy", "estimate", "standard_error"]

    if include_ci:
        headers.extend(["ci_lower", "ci_upper"])
        ci_lower, ci_upper = results.confidence_interval(alpha=0.05)

    headers.extend(["n_samples", "method"])

    # Add data for each policy
    target_policies = results.metadata.get("target_policies", [])
    for i, policy in enumerate(target_policies):
        row = {
            "policy": policy,
            "estimate": results.estimates[i],
            "standard_error": results.standard_errors[i],
            "n_samples": results.n_samples_used.get(policy, 0),
            "method": results.method,
        }

        if include_ci:
            row["ci_lower"] = ci_lower[i]
            row["ci_upper"] = ci_upper[i]

        rows.append(row)

    # Write CSV
    with open(path_obj, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        writer.writerows(rows)


def _serialize_array(arr: np.ndarray) -> list:
    """Convert numpy array to JSON-serializable list."""
    if isinstance(arr, np.ndarray):
        return [float(x) if not np.isnan(x) else None for x in arr]
    return list(arr)


def _serialize_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:
    """Serialize metadata, handling special types."""
    serialized: Dict[str, Any] = {}

    for key, value in metadata.items():
        # Skip complex objects that aren't easily serializable
        if key in ["diagnostics", "dr_diagnostics", "dr_calibration_data"]:
            continue

        if isinstance(value, np.ndarray):
            serialized[key] = _serialize_array(value)
        elif isinstance(value, (np.integer, np.floating)):
            serialized[key] = float(value)
        elif isinstance(value, (dict, list, str, int, float, bool, type(None))):
            serialized[key] = value
        else:
            # Try to convert to string for other types
            try:
                serialized[key] = str(value)
            except:
                pass  # Skip if can't serialize

    return serialized


def _serialize_diagnostics(diagnostics: Dict[str, Any]) -> Dict[str, Any]:
    """Serialize diagnostics, handling nested structures."""
    serialized: Dict[str, Any] = {}

    for key, value in diagnostics.items():
        if isinstance(value, dict):
            # Recursively serialize nested dicts
            serialized[key] = _serialize_diagnostics(value)
        elif isinstance(value, np.ndarray):
            serialized[key] = _serialize_array(value)
        elif isinstance(value, (np.integer, np.floating)):
            serialized[key] = float(value)
        elif hasattr(value, "__dict__"):
            # Handle diagnostic objects with attributes
            serialized[key] = {
                k: float(v) if isinstance(v, (np.integer, np.floating)) else v
                for k, v in value.__dict__.items()
                if not k.startswith("_")
            }
        else:
            serialized[key] = value

    return serialized


=== ./cje/utils/extreme_weights_analysis.py ===

"""Analyze and log extreme importance weights for debugging."""

import numpy as np
import json
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
import logging

# Import compute_ess from diagnostics module
from ..diagnostics import compute_ess

logger = logging.getLogger(__name__)


def analyze_extreme_weights(
    dataset: Any,  # Dataset object
    sampler: Any,  # PrecomputedSampler object
    raw_weights_dict: Dict[str, np.ndarray],
    calibrated_weights_dict: Dict[str, np.ndarray],
    n_extreme: int = 5,
    output_dir: Optional[Path] = None,
    near_zero_threshold: float = 1e-10,
) -> Tuple[Dict[str, Any], str]:
    """Analyze extreme weights and generate detailed report.

    Args:
        dataset: Dataset with samples
        sampler: PrecomputedSampler with log probabilities
        raw_weights_dict: Raw importance weights by policy
        calibrated_weights_dict: Calibrated weights by policy
        n_extreme: Number of extreme samples to analyze
        output_dir: Directory to save reports (optional)
        near_zero_threshold: Threshold for near-zero weights (default: 1e-10)

    Returns:
        Tuple of (json_report, text_report)
    """

    # Initialize report structure
    report = {
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "n_samples": dataset.n_samples,
            "n_valid_samples": sampler.n_valid_samples,
            "policies_analyzed": list(raw_weights_dict.keys()),
            "n_extreme_analyzed": n_extreme,
        },
        "per_policy_analysis": {},
        "cross_policy_insights": {
            "consistently_extreme": [],
            "high_variance_samples": [],
        },
    }

    # Track extreme samples across policies
    all_extreme_high: Dict[str, List[str]] = {}  # sample_id -> list of policies
    all_extreme_low: Dict[str, List[str]] = {}  # sample_id -> list of policies

    # Analyze each policy
    for policy_name in raw_weights_dict.keys():
        raw_weights = raw_weights_dict[policy_name]
        cal_weights = calibrated_weights_dict.get(policy_name, raw_weights)

        # Get valid data for this policy
        policy_data = sampler.get_data_for_policy(policy_name)
        if not policy_data:
            continue

        # Calculate statistics
        finite_raw = raw_weights[np.isfinite(raw_weights) & (raw_weights > 0)]

        policy_analysis: Dict[str, Any] = {
            "statistics": {
                "raw_weight_range": (
                    [float(np.min(finite_raw)), float(np.max(finite_raw))]
                    if len(finite_raw) > 0
                    else [0, 0]
                ),
                "calibrated_weight_range": [
                    float(np.min(cal_weights)),
                    float(np.max(cal_weights)),
                ],
                "n_clipped_high": int(
                    np.sum(raw_weights >= 100.0)
                ),  # Assuming clip at 100
                "n_near_zero": int(np.sum(raw_weights < near_zero_threshold)),
                "ess_raw": f"{compute_ess(raw_weights) / len(raw_weights) * 100:.1f}%",
                "ess_calibrated": f"{compute_ess(cal_weights) / len(cal_weights) * 100:.1f}%",
            },
            "extreme_samples": {
                "highest_weights": [],
                "lowest_weights": [],
            },
        }

        # Find extreme samples
        sorted_indices = np.argsort(raw_weights)
        highest_indices = sorted_indices[-n_extreme:][::-1]  # Reverse for descending
        lowest_indices = sorted_indices[:n_extreme]

        # Analyze highest weight samples
        for rank, idx in enumerate(highest_indices, 1):
            if idx >= len(policy_data):
                continue

            sample_data = policy_data[idx]
            sample_id = sample_data.get("prompt_id", f"sample_{idx}")

            # Track for cross-policy analysis
            if sample_id not in all_extreme_high:
                all_extreme_high[sample_id] = []
            all_extreme_high[sample_id].append(policy_name)

            # Get prompt and response (truncate for display)
            prompt = sample_data.get("prompt", "")[:100]
            response = sample_data.get("response", "")[:100]

            # Get log probabilities
            base_logp = sample_data.get("base_policy_logprob", 0)
            target_logp = sample_data.get("target_policy_logprobs", {}).get(
                policy_name, 0
            )
            log_ratio = target_logp - base_logp

            extreme_sample = {
                "rank": rank,
                "sample_id": sample_id,
                "prompt": prompt
                + ("..." if len(sample_data.get("prompt", "")) > 100 else ""),
                "response_preview": response
                + ("..." if len(sample_data.get("response", "")) > 100 else ""),
                "base_logprob": float(base_logp),
                "target_logprob": float(target_logp),
                "log_ratio": float(log_ratio),
                "raw_weight": float(raw_weights[idx]),
                "calibrated_weight": float(cal_weights[idx]),
                "reward": float(sample_data.get("reward", 0)),
                "explanation": _explain_weight(log_ratio),
            }

            policy_analysis["extreme_samples"]["highest_weights"].append(extreme_sample)

        # Analyze lowest weight samples
        for rank, idx in enumerate(lowest_indices, 1):
            if idx >= len(policy_data):
                continue

            sample_data = policy_data[idx]
            sample_id = sample_data.get("prompt_id", f"sample_{idx}")

            # Track for cross-policy analysis
            if sample_id not in all_extreme_low:
                all_extreme_low[sample_id] = []
            all_extreme_low[sample_id].append(policy_name)

            # Get prompt and response (truncate for display)
            prompt = sample_data.get("prompt", "")[:100]
            response = sample_data.get("response", "")[:100]

            # Get log probabilities
            base_logp = sample_data.get("base_policy_logprob", 0)
            target_logp = sample_data.get("target_policy_logprobs", {}).get(
                policy_name, 0
            )
            log_ratio = target_logp - base_logp

            extreme_sample = {
                "rank": rank,
                "sample_id": sample_id,
                "prompt": prompt
                + ("..." if len(sample_data.get("prompt", "")) > 100 else ""),
                "response_preview": response
                + ("..." if len(sample_data.get("response", "")) > 100 else ""),
                "base_logprob": float(base_logp),
                "target_logprob": float(target_logp),
                "log_ratio": float(log_ratio),
                "raw_weight": float(raw_weights[idx]),
                "calibrated_weight": float(cal_weights[idx]),
                "reward": float(sample_data.get("reward", 0)),
                "explanation": _explain_weight(log_ratio),
            }

            policy_analysis["extreme_samples"]["lowest_weights"].append(extreme_sample)

        report["per_policy_analysis"][policy_name] = policy_analysis

    # Cross-policy insights
    for sample_id, policies in all_extreme_high.items():
        if len(policies) > 1:
            report["cross_policy_insights"]["consistently_extreme"].append(
                {
                    "sample_id": sample_id,
                    "policies_extreme_high": policies,
                    "interpretation": "High weight across multiple policies suggests base policy underestimates this response",
                }
            )

    for sample_id, policies in all_extreme_low.items():
        if len(policies) > 1:
            report["cross_policy_insights"]["consistently_extreme"].append(
                {
                    "sample_id": sample_id,
                    "policies_extreme_low": policies,
                    "interpretation": "Low weight across multiple policies suggests base policy overestimates this response",
                }
            )

    # Generate text report
    text_report = _format_text_report(report)

    # Save reports if output directory provided
    if output_dir:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # Save JSON report
        json_path = output_dir / "extreme_weights_analysis.json"
        with open(json_path, "w") as f:
            json.dump(report, f, indent=2)
        logger.info(f"Saved JSON report to {json_path}")

        # Save text report
        text_path = output_dir / "extreme_weights_analysis.txt"
        with open(text_path, "w") as f:
            f.write(text_report)
        logger.info(f"Saved text report to {text_path}")

    return report, text_report


def _explain_weight(log_ratio: float) -> str:
    """Generate human-readable explanation for weight."""
    if log_ratio > 20:
        return f"Target policy MUCH more likely (Δ={log_ratio:.1f} nats)"
    elif log_ratio > 5:
        return f"Target policy more likely (Δ={log_ratio:.1f} nats)"
    elif log_ratio > -5:
        return f"Similar likelihood (Δ={log_ratio:.1f} nats)"
    elif log_ratio > -20:
        return f"Target policy less likely (Δ={log_ratio:.1f} nats)"
    else:
        return f"Target policy MUCH less likely (Δ={log_ratio:.1f} nats)"


def _format_text_report(report: Dict[str, Any]) -> str:
    """Format report as human-readable text."""
    lines = []

    # Header
    lines.append("=" * 80)
    lines.append("EXTREME WEIGHTS ANALYSIS REPORT")
    lines.append("=" * 80)
    lines.append(f"Generated: {report['metadata']['timestamp']}")
    lines.append(
        f"Samples: {report['metadata']['n_valid_samples']} valid / {report['metadata']['n_samples']} total"
    )
    lines.append(f"Policies: {', '.join(report['metadata']['policies_analyzed'])}")
    lines.append("=" * 80)
    lines.append("")

    # Per-policy analysis
    for policy, analysis in report["per_policy_analysis"].items():
        lines.append(f"POLICY: {policy}")
        lines.append("-" * 40)

        stats = analysis["statistics"]
        lines.append(
            f"Raw weight range: [{stats['raw_weight_range'][0]:.2e}, {stats['raw_weight_range'][1]:.2e}]"
        )
        lines.append(
            f"Calibrated range: [{stats['calibrated_weight_range'][0]:.3f}, {stats['calibrated_weight_range'][1]:.3f}]"
        )
        lines.append(f"ESS improvement: {stats['ess_raw']} → {stats['ess_calibrated']}")
        lines.append(
            f"Extreme weights: {stats['n_clipped_high']} very high (≥100), {stats['n_near_zero']} near-zero (<1e-10)"
        )
        lines.append("")

        # Highest weights
        lines.append(
            f"TOP {len(analysis['extreme_samples']['highest_weights'])} HIGHEST WEIGHTS:"
        )
        for sample in analysis["extreme_samples"]["highest_weights"]:
            lines.append(
                f"{sample['rank']}. Sample {sample['sample_id']} (raw: {sample['raw_weight']:.2e} → cal: {sample['calibrated_weight']:.3f})"
            )
            lines.append(f"   Prompt: {sample['prompt']}")
            lines.append(f"   Response: {sample['response_preview']}")
            lines.append(
                f"   Base logp: {sample['base_logprob']:.2f}, Target logp: {sample['target_logprob']:.2f} (Δ={sample['log_ratio']:+.2f})"
            )
            lines.append(f"   → {sample['explanation']}")
            lines.append("")

        # Lowest weights
        lines.append(
            f"BOTTOM {len(analysis['extreme_samples']['lowest_weights'])} LOWEST WEIGHTS:"
        )
        for sample in analysis["extreme_samples"]["lowest_weights"]:
            lines.append(
                f"{sample['rank']}. Sample {sample['sample_id']} (raw: {sample['raw_weight']:.2e} → cal: {sample['calibrated_weight']:.3f})"
            )
            lines.append(f"   Prompt: {sample['prompt']}")
            lines.append(f"   Response: {sample['response_preview']}")
            lines.append(
                f"   Base logp: {sample['base_logprob']:.2f}, Target logp: {sample['target_logprob']:.2f} (Δ={sample['log_ratio']:+.2f})"
            )
            lines.append(f"   → {sample['explanation']}")
            lines.append("")

        lines.append("")

    # Cross-policy patterns
    if report["cross_policy_insights"]["consistently_extreme"]:
        lines.append("=" * 80)
        lines.append("CROSS-POLICY PATTERNS")
        lines.append("=" * 80)
        lines.append("Samples consistently extreme across policies:")
        for pattern in report["cross_policy_insights"]["consistently_extreme"]:
            if "policies_extreme_high" in pattern:
                lines.append(
                    f"  • {pattern['sample_id']}: High in {', '.join(pattern['policies_extreme_high'])}"
                )
            else:
                lines.append(
                    f"  • {pattern['sample_id']}: Low in {', '.join(pattern['policies_extreme_low'])}"
                )
            lines.append(f"    {pattern['interpretation']}")
        lines.append("")

    return "\n".join(lines)


=== ./cje/visualization/__init__.py ===

"""Visualization utilities for CJE framework.

This module provides plotting functions organized by domain:
- Weight diagnostics and dashboards
- DR diagnostics and dashboards
- Calibration comparison plots
- Policy estimate visualizations
"""

# Import core visualization functions
from .calibration import plot_calibration_comparison
from .estimates import plot_policy_estimates

# Import weight dashboards
from .weight_dashboards import (
    plot_weight_dashboard_summary,
    plot_weight_dashboard_detailed,
)

# Import DR dashboards
from .dr_dashboards import plot_dr_dashboard

__all__ = [
    # Calibration
    "plot_calibration_comparison",
    # Policy estimates
    "plot_policy_estimates",
    # Weight dashboards
    "plot_weight_dashboard_summary",
    "plot_weight_dashboard_detailed",
    # DR dashboards
    "plot_dr_dashboard",
]


=== ./cje/visualization/calibration.py ===

"""Calibration visualization utilities."""

from pathlib import Path
from typing import Optional, Tuple
import numpy as np
import matplotlib.pyplot as plt


def plot_calibration_comparison(
    judge_scores: np.ndarray,
    oracle_labels: np.ndarray,
    calibrated_scores: Optional[np.ndarray] = None,
    n_bins: int = 10,
    save_path: Optional[Path] = None,
    figsize: tuple = (12, 5),
) -> plt.Figure:
    """Plot calibration comparison showing transformation and improvement.

    Shows the calibration transformation and its effect on oracle alignment.

    Args:
        judge_scores: Raw judge scores
        oracle_labels: True oracle labels
        calibrated_scores: Calibrated judge scores (optional)
        n_bins: Number of bins for grouping
        save_path: Optional path to save figure
        figsize: Figure size

    Returns:
        matplotlib Figure
    """
    # Create figure with single subplot
    fig, ax = plt.subplots(1, 1, figsize=(8, 6))

    # Compute calibration metrics
    def compute_calibration_error(
        predictions: np.ndarray, labels: np.ndarray
    ) -> Tuple[float, float]:
        """Compute Expected Calibration Error (ECE) and RMSE."""
        bins = np.linspace(0, 1, n_bins + 1)
        bin_indices = np.digitize(predictions, bins) - 1

        ece = 0.0
        total_samples = 0
        squared_errors = []

        for i in range(n_bins):
            mask = bin_indices == i
            n_in_bin = mask.sum()
            if n_in_bin > 0:
                pred_in_bin = predictions[mask].mean()
                true_in_bin = labels[mask].mean()

                # ECE: weighted average of bin-wise calibration errors
                ece += n_in_bin * abs(pred_in_bin - true_in_bin)
                total_samples += n_in_bin

                # For RMSE
                squared_errors.extend((predictions[mask] - labels[mask]) ** 2)

        ece = ece / total_samples if total_samples > 0 else 0.0
        rmse = np.sqrt(np.mean(squared_errors)) if squared_errors else 0.0

        return ece, rmse

    # Create a 2D histogram for density visualization
    H, xedges, yedges = np.histogram2d(judge_scores, oracle_labels, bins=20)
    H = H.T  # Transpose for correct orientation
    
    # Apply logarithmic transformation to make low counts more visible
    # Add 1 to avoid log(0), then apply log scaling
    H_log = np.log1p(H)  # log(1 + H) to handle zeros gracefully
    
    # Apply minimal smoothing to the log-transformed data
    from scipy.ndimage import gaussian_filter
    H_log_smooth = gaussian_filter(H_log, sigma=0.5)
    
    # Create mesh grid for contours
    X, Y = np.meshgrid(xedges[:-1] + (xedges[1] - xedges[0])/2, 
                        yedges[:-1] + (yedges[1] - yedges[0])/2)
    
    # Create custom colormap for filled contours
    import matplotlib.colors as mcolors
    # White to much darker blue/navy for high contrast
    colors = ['#FFFFFF', '#F0F8FF', '#E0F2FF', '#D6EDFF', '#CCE7FF', '#99CEFF', '#66B2FF', '#3395FF', '#0078FF', '#0055CC', '#003D99', '#002866', '#001A33']
    cmap = mcolors.LinearSegmentedColormap.from_list('light_blues', colors, N=256)
    
    # Create filled contours using log-scale data
    # Levels are now in log space
    max_log_count = H_log_smooth.max()
    min_log_count = H_log_smooth[H_log_smooth > 0].min() if np.any(H_log_smooth > 0) else 0
    
    # Create levels in log space for better visibility of low-density regions
    filled_levels = np.linspace(min_log_count, max_log_count, 25)
    
    # Create filled contours
    contourf = ax.contourf(X, Y, H_log_smooth, levels=filled_levels, cmap=cmap, alpha=0.65, extend='both')
    
    # Add colorbar for density scale with custom labels showing actual counts
    cbar = plt.colorbar(contourf, ax=ax, pad=0.02)
    cbar.set_label('Sample Count', fontsize=9)
    
    # Create custom tick positions and labels
    # Choose nice round numbers for actual counts
    max_count = np.expm1(max_log_count)  # Convert back from log space
    
    # Select appropriate tick values based on data range
    if max_count > 500:
        tick_counts = [0, 5, 10, 25, 50, 100, 250, 500, 1000]
    elif max_count > 100:
        tick_counts = [0, 5, 10, 25, 50, 100, 200]
    elif max_count > 50:
        tick_counts = [0, 5, 10, 25, 50, 100]
    else:
        tick_counts = [0, 2, 5, 10, 25, 50]
    
    # Filter to only include ticks within data range
    tick_counts = [t for t in tick_counts if t <= max_count]
    
    # Convert counts to log space for positioning
    tick_positions = [np.log1p(count) for count in tick_counts]
    
    # Set the ticks and labels
    cbar.set_ticks(tick_positions)
    cbar.set_ticklabels([str(int(count)) for count in tick_counts])
    
    # Now create the regular 2D histogram for contour lines (not smoothed)
    H, xedges, yedges = np.histogram2d(judge_scores, oracle_labels, bins=20)
    H = H.T  # Transpose for correct orientation
    
    # Create mesh grid for contours
    X, Y = np.meshgrid(xedges[:-1] + (xedges[1] - xedges[0])/2, 
                        yedges[:-1] + (yedges[1] - yedges[0])/2)
    
    # Adjust contour levels based on dataset size
    n_samples = len(judge_scores)
    if H.max() > 5:
        # Scale contour levels based on data size
        if n_samples < 500:
            levels = [5, 10, 25, 50]
        elif n_samples < 1000:
            levels = [10, 25, 50, 100]
        elif n_samples < 5000:
            levels = [10, 25, 50, 100, 200, 500]
        else:
            levels = [10, 50, 100, 250, 500, 1000]
        
        # Only use levels that exist in the data
        levels = [l for l in levels if l < H.max()]
        
        if levels:
            # Draw contour lines
            contours = ax.contour(X, Y, H, levels=levels, colors='darkgray', 
                                 linewidths=0.7, alpha=0.7)
            # Label the contours with sample counts
            ax.clabel(contours, inline=True, fontsize=8, fmt='%d')
    
    # Compute binned statistics for empirical relationship
    bin_width = 0.05
    bin_edges = np.arange(0, 1.0 + bin_width, bin_width)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    
    binned_means = []
    binned_counts = []
    
    for i in range(len(bin_edges) - 1):
        if i == len(bin_edges) - 2:
            # Last bin: include right edge
            mask = (judge_scores >= bin_edges[i]) & (judge_scores <= bin_edges[i + 1])
        else:
            # All other bins: exclude right edge
            mask = (judge_scores >= bin_edges[i]) & (judge_scores < bin_edges[i + 1])
        if np.any(mask):
            binned_means.append(np.mean(oracle_labels[mask]))
            binned_counts.append(np.sum(mask))
        else:
            binned_means.append(np.nan)
            binned_counts.append(0)
    
    # Filter out empty bins
    valid_bins = ~np.isnan(binned_means)
    bin_centers_valid = bin_centers[valid_bins]
    binned_means_valid = np.array(binned_means)[valid_bins]
    binned_counts_valid = np.array(binned_counts)[valid_bins]
    
    # Plot smoothed empirical relationship using local polynomial regression (LOWESS)
    from scipy.signal import savgol_filter
    from scipy.interpolate import interp1d
    
    if len(bin_centers_valid) > 3:
        # Create smooth interpolation through binned means
        # Use cubic spline for smooth curve
        interp_func = interp1d(
            bin_centers_valid, 
            binned_means_valid, 
            kind='cubic' if len(bin_centers_valid) > 3 else 'linear',
            bounds_error=False,
            fill_value='extrapolate'
        )
        
        # Create fine grid for smooth curve
        # Extend to the actual data range, not just valid bin centers
        x_min = max(0, judge_scores.min())
        x_max = min(1, judge_scores.max())
        x_smooth = np.linspace(x_min, x_max, 200)
        y_smooth = interp_func(x_smooth)
        
        # Clip to valid range
        y_smooth = np.clip(y_smooth, 0, 1)
        
        # Plot smoothed empirical curve
        ax.plot(
            x_smooth,
            y_smooth,
            "-",
            color="darkblue",
            alpha=0.9,
            linewidth=3,
            label="Empirical mean E[Oracle|Judge]",
            zorder=10,  # Make sure it's on top
        )
        
        # Also plot the binned points for reference
        ax.scatter(
            bin_centers_valid,
            binned_means_valid,
            s=40,
            alpha=0.7,
            color="darkblue",
            edgecolor='white',
            linewidth=0.5,
            zorder=11,
        )
    
    # Plot the calibration function if available
    if calibrated_scores is not None:
        # Sort for smooth curve
        sorted_idx = np.argsort(judge_scores)
        judge_sorted = judge_scores[sorted_idx]
        calibrated_sorted = calibrated_scores[sorted_idx]

        # Plot calibration function
        ax.plot(
            judge_sorted,
            calibrated_sorted,
            "-",
            color="red",
            alpha=0.9,
            linewidth=3,
            label="Isotonic calibration f(Judge)",
            zorder=10,  # Make sure it's on top
        )

    # Add diagonal reference
    ax.plot([0, 1], [0, 1], "--", color="gray", alpha=0.5, label="Perfect calibration (y=x)")

    # Labels and formatting
    ax.set_xlabel("Judge Score")
    ax.set_ylabel("Oracle Label / Calibrated Reward")
    ax.set_title("Judge→Oracle Calibration")
    ax.grid(True, alpha=0.3)
    ax.set_xlim((0, 1))
    ax.set_ylim((0, 1))
    ax.legend(loc="upper left", fontsize=9)
    
    # Compute comprehensive statistics
    total_samples = len(judge_scores)
    
    # Compute calibration metrics
    ece_before, rmse_before = compute_calibration_error(judge_scores, oracle_labels)
    
    # Build comprehensive stats text
    if calibrated_scores is not None:
        ece_after, rmse_after = compute_calibration_error(calibrated_scores, oracle_labels)
        
        stats_text = (
            f"Samples: {total_samples:,}\n"
            f"ECE: {ece_before:.3f} → {ece_after:.3f}\n"
            f"RMSE: {rmse_before:.3f} → {rmse_after:.3f}\n"
            f"Judge: [{judge_scores.min():.3f}, {judge_scores.max():.3f}]\n"
            f"Oracle: [{oracle_labels.min():.3f}, {oracle_labels.max():.3f}]\n"
            f"Calibrated: [{calibrated_scores.min():.3f}, {calibrated_scores.max():.3f}]"
        )
    else:
        stats_text = (
            f"Samples: {total_samples:,}\n"
            f"ECE: {ece_before:.3f}\n"
            f"RMSE: {rmse_before:.3f}\n"
            f"Judge: [{judge_scores.min():.3f}, {judge_scores.max():.3f}]\n"
            f"Oracle: [{oracle_labels.min():.3f}, {oracle_labels.max():.3f}]"
        )
    
    ax.text(
        0.98,
        0.02,
        stats_text,
        transform=ax.transAxes,
        fontsize=8,
        horizontalalignment="right",
        verticalalignment="bottom",
        bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.85),
    )

    # Save and return figure
    if save_path:
        save_path = Path(save_path)
        fig.savefig(save_path.with_suffix(".png"), dpi=150, bbox_inches="tight")

    return fig


=== ./cje/visualization/dr_dashboards.py ===

"""Doubly Robust (DR) diagnostics dashboards for CJE framework.

Provides comprehensive visualization of DR estimation diagnostics including:
- Direct method vs IPS contributions
- Orthogonality checks
- Influence function tail behavior
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from typing import Dict, Any, Tuple, List


def plot_dr_dashboard(
    estimation_result: Any, figsize: Tuple[float, float] = (15, 5)
) -> Tuple[Figure, Dict[str, Any]]:
    """Create a compact 3-panel DR diagnostic dashboard.

    Panel A: DM vs IPS contributions per policy
    Panel B: Orthogonality check (score mean ± 2SE)
    Panel C: EIF tail behavior (CCDF)

    Args:
        estimation_result: Result from DR estimator with diagnostics
        figsize: Figure size (width, height)

    Returns:
        (fig, summary_metrics) tuple
    """
    # Check for DRDiagnostics object first (new way)
    from ..diagnostics import DRDiagnostics

    if isinstance(estimation_result.diagnostics, DRDiagnostics):
        # Use the new diagnostic object
        dr_diags = estimation_result.diagnostics.dr_diagnostics_per_policy
    elif "dr_diagnostics" in estimation_result.metadata:
        # Fallback to old way
        dr_diags = estimation_result.metadata["dr_diagnostics"]
    else:
        raise ValueError("No DR diagnostics found in estimation result")

    policies = list(dr_diags.keys())
    n_policies = len(policies)

    # Set up figure
    fig, axes = plt.subplots(1, 3, figsize=figsize)
    fig.suptitle("DR Diagnostics Dashboard", fontsize=14, fontweight="bold")

    # Color palette
    colors = plt.cm.get_cmap("tab10")(np.linspace(0, 1, n_policies))

    # Panel A: DM vs IPS contributions
    _plot_dr_contributions(axes[0], dr_diags, policies, colors)

    # Panel B: Orthogonality check
    _plot_orthogonality_check(axes[1], dr_diags, policies, colors, estimation_result)

    # Panel C: EIF tail behavior
    _plot_eif_tail_behavior(axes[2], estimation_result, policies, colors)

    plt.tight_layout()

    # Compute summary metrics
    summary_metrics = _compute_dr_summary_metrics(dr_diags, estimation_result)

    return fig, summary_metrics


def _plot_dr_contributions(
    ax: Any, dr_diags: Dict, policies: List[str], colors: Any
) -> None:
    """Plot direct method vs IPS correction contributions."""
    n_policies = len(policies)
    x = np.arange(n_policies)
    width = 0.35

    dm_means = [dr_diags[p]["dm_mean"] for p in policies]
    ips_corrs = [dr_diags[p]["ips_corr_mean"] for p in policies]
    dr_estimates = [dr_diags[p]["dr_estimate"] for p in policies]

    # Bar plots for DM and IPS
    ax.bar(x - width / 2, dm_means, width, label="DM", color="steelblue", alpha=0.7)
    ax.bar(
        x + width / 2,
        ips_corrs,
        width,
        label="IPS Correction",
        color="coral",
        alpha=0.7,
    )

    # Add DR estimate markers
    ax.scatter(
        x, dr_estimates, color="black", s=50, zorder=5, label="DR Estimate", marker="D"
    )

    ax.set_xlabel("Policy")
    ax.set_ylabel("Value")
    ax.set_title("A: Contributions (DM vs IPS)")
    ax.set_xticks(x)
    ax.set_xticklabels(policies, rotation=45, ha="right")
    ax.legend(loc="best")
    ax.grid(axis="y", alpha=0.3)
    ax.axhline(y=0, color="gray", linestyle="--", alpha=0.5)


def _plot_orthogonality_check(
    ax: Any, dr_diags: Dict, policies: List[str], colors: Any, estimation_result: Any
) -> None:
    """Plot orthogonality check with score means and confidence intervals."""
    n_policies = len(policies)

    # Check if we have score information (requires influence functions)
    has_scores = any("score_mean" in dr_diags[p] for p in policies)

    if not has_scores:
        # No scores available - show informative message
        ax.text(
            0.5,
            0.5,
            "Score test unavailable\n(influence functions not stored)",
            transform=ax.transAxes,
            ha="center",
            va="center",
            fontsize=12,
            color="gray",
            style="italic",
        )
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis("off")
        ax.set_title("B: Orthogonality Check (unavailable)")
    else:
        for i, policy in enumerate(policies):
            diag = dr_diags[policy]
            score_mean = diag.get("score_mean", 0.0)
            score_se = diag.get("score_se", 0.0)

            # Plot point with error bars (2 SE)
            ax.errorbar(
                i,
                score_mean,
                yerr=2 * score_se,
                fmt="o",
                color=colors[i],
                markersize=8,
                capsize=5,
                capthick=2,
                label=policy,
            )

            # Add p-value annotation
            p_val = diag.get("score_p", 1.0)
            if p_val < 0.05:
                ax.text(
                    i,
                    score_mean + 2.5 * score_se,
                    f"p={p_val:.3f}",
                    ha="center",
                    fontsize=8,
                    color="red",
                )

        ax.axhline(y=0, color="black", linestyle="-", linewidth=1)
        ax.set_xlabel("Policy")
        ax.set_ylabel("Score Mean")
        ax.set_title("B: Orthogonality Check (mean ± 2SE)")
        ax.set_xticks(range(n_policies))
        ax.set_xticklabels(policies, rotation=45, ha="right")
        ax.grid(axis="y", alpha=0.3)

    # Add note for TMLE
    if estimation_result.method == "tmle":
        ax.text(
            0.5,
            0.95,
            "TMLE: bars should straddle 0",
            transform=ax.transAxes,
            ha="center",
            va="top",
            fontsize=9,
            style="italic",
            color="gray",
        )


def _plot_eif_tail_behavior(
    ax: Any, estimation_result: Any, policies: List[str], colors: Any
) -> None:
    """Plot empirical influence function tail behavior (CCDF)."""

    # Check if actual influence functions are available
    has_empirical_ifs = False
    ifs_data = None

    # First check the first-class location (new API)
    if estimation_result.influence_functions is not None:
        ifs_data = estimation_result.influence_functions
        if ifs_data and all(policy in ifs_data for policy in policies):
            has_empirical_ifs = True
    # Fallback to legacy location in metadata
    elif "dr_influence" in estimation_result.metadata:
        ifs_data = estimation_result.metadata["dr_influence"]
        if ifs_data and all(policy in ifs_data for policy in policies):
            has_empirical_ifs = True

    if has_empirical_ifs:
        # Use empirical influence functions for exact CCDF
        for i, policy in enumerate(policies):
            ifs = ifs_data[policy]
            if isinstance(ifs, np.ndarray) and len(ifs) > 0:
                # Compute empirical CCDF
                abs_ifs = np.abs(ifs)
                sorted_ifs = np.sort(abs_ifs)[::-1]  # Descending
                ccdf = np.arange(1, len(sorted_ifs) + 1) / len(sorted_ifs)

                # Plot with appropriate sampling for large n
                if len(sorted_ifs) > 10000:
                    # Downsample for plotting efficiency
                    indices = np.logspace(
                        0, np.log10(len(sorted_ifs) - 1), 1000, dtype=int
                    )
                    ax.loglog(
                        sorted_ifs[indices],
                        ccdf[indices],
                        label=policy,
                        color=colors[i],
                        linewidth=2,
                    )
                else:
                    ax.loglog(
                        sorted_ifs, ccdf, label=policy, color=colors[i], linewidth=2
                    )

        # Add reference lines at p95 and p99
        ax.axhline(y=0.05, color="gray", linestyle=":", alpha=0.5, label="p95")
        ax.axhline(y=0.01, color="gray", linestyle="--", alpha=0.5, label="p99")
    else:
        # No influence functions available - show message
        ax.text(
            0.5,
            0.5,
            "Influence functions not available\n(set store_influence=True)",
            ha="center",
            va="center",
            fontsize=10,
            color="gray",
            transform=ax.transAxes,
        )
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)

    ax.set_xlabel("|IF| (log scale)")
    ax.set_ylabel("CCDF (log scale)")
    ax.set_title("C: EIF Tail Behavior")
    ax.legend(loc="best")
    ax.grid(True, which="both", alpha=0.3)


def _compute_dr_summary_metrics(
    dr_diags: Dict, estimation_result: Any
) -> Dict[str, Any]:
    """Compute summary metrics from DR diagnostics."""
    summary_metrics = {}

    # IF tail ratios
    if_tail_ratios = [d.get("if_tail_ratio_99_5", 0.0) for d in dr_diags.values()]
    if if_tail_ratios:
        summary_metrics["worst_if_tail_ratio"] = max(if_tail_ratios)

    # R² values
    r2_values = [d.get("r2_oof", np.nan) for d in dr_diags.values() if "r2_oof" in d]
    if r2_values and not all(np.isnan(r2_values)):
        valid_r2 = [r for r in r2_values if not np.isnan(r)]
        if valid_r2:
            summary_metrics["best_r2_oof"] = max(valid_r2)
            summary_metrics["worst_r2_oof"] = min(valid_r2)

    # RMSE values
    rmse_values = [
        d.get("residual_rmse", np.nan)
        for d in dr_diags.values()
        if "residual_rmse" in d
    ]
    if rmse_values and not all(np.isnan(rmse_values)):
        valid_rmse = [r for r in rmse_values if not np.isnan(r)]
        if valid_rmse:
            summary_metrics["avg_residual_rmse"] = np.mean(valid_rmse)

    if estimation_result.method == "tmle":
        score_means = [abs(d.get("score_mean", 0.0)) for d in dr_diags.values()]
        if score_means:
            summary_metrics["tmle_max_abs_score"] = max(score_means)

    return summary_metrics


=== ./cje/visualization/estimates.py ===

"""Policy estimate visualization utilities."""

from pathlib import Path
from typing import Dict, Optional
import numpy as np
import matplotlib.pyplot as plt


def plot_policy_estimates(
    estimates: Dict[str, float],
    standard_errors: Dict[str, float],
    oracle_values: Optional[Dict[str, float]] = None,
    base_policy: str = "base",
    figsize: tuple = (10, 6),
    save_path: Optional[Path] = None,
) -> plt.Figure:
    """Create forest plot of policy performance estimates with confidence intervals.

    Shows policy estimates as a forest plot with optional oracle comparison.

    Args:
        estimates: Dict mapping policy names to estimates
        standard_errors: Dict mapping policy names to standard errors
        oracle_values: Optional dict of oracle ground truth values
        base_policy: Name of base policy (for reference line)
        figsize: Figure size (width, height)
        save_path: Optional path to save figure

    Returns:
        matplotlib Figure
    """
    fig, ax = plt.subplots(figsize=figsize)

    # Sort policies: base first, then others alphabetically
    policies = []
    if base_policy in estimates:
        policies.append(base_policy)
    for p in sorted(estimates.keys()):
        if p != base_policy:
            policies.append(p)

    y_positions = np.arange(len(policies))[::-1]  # Reverse so first policy is at top

    # Identify best policy (excluding base)
    non_base_policies = [p for p in policies if p != base_policy]
    if non_base_policies:
        best_policy = max(non_base_policies, key=lambda p: estimates[p])
    else:
        best_policy = None

    # Plot each policy
    for i, policy in enumerate(policies):
        y = y_positions[i]
        est = estimates[policy]
        se = standard_errors[policy]

        # Confidence interval
        ci_lower = est - 1.96 * se
        ci_upper = est + 1.96 * se

        # Determine color
        if policy == base_policy:
            color = "gray"
            marker = "s"  # Square for base
        elif policy == best_policy:
            color = "green"
            marker = "o"
        else:
            color = "steelblue"
            marker = "o"

        # Plot CI line
        ax.plot([ci_lower, ci_upper], [y, y], color=color, linewidth=2, alpha=0.7)

        # Plot estimate point
        ax.scatter(est, y, color=color, s=100, marker=marker, zorder=5, label=None)

        # Add oracle value if available
        if oracle_values and policy in oracle_values:
            oracle_val = oracle_values[policy]
            ax.scatter(
                oracle_val,
                y,
                color="red",
                s=100,
                marker="d",
                alpha=0.7,
                zorder=4,
                label="Oracle Truth" if i == 0 else None,
            )

    # Add vertical line at base estimate
    if base_policy in estimates:
        ax.axvline(
            estimates[base_policy],
            color="gray",
            linestyle=":",
            alpha=0.5,
            label=f"{base_policy} (reference)",
        )

    # Labels and formatting
    ax.set_yticks(y_positions)
    ax.set_yticklabels(policies)
    ax.set_xlabel("Estimated Performance")
    ax.set_title("Policy Performance Estimates (95% CI)")
    ax.grid(True, alpha=0.3, axis="x")

    # Add RMSE if oracle values available
    if oracle_values:
        # Calculate RMSE vs oracle
        squared_errors = []
        for policy in policies:
            if policy in oracle_values:
                error = estimates[policy] - oracle_values[policy]
                squared_errors.append(error**2)
        if squared_errors:
            rmse = np.sqrt(np.mean(squared_errors))
            ax.text(
                0.02,
                0.98,
                f"RMSE vs Oracle: {rmse:.3f}",
                transform=ax.transAxes,
                verticalalignment="top",
                bbox=dict(boxstyle="round", facecolor="white", alpha=0.8),
            )

    # Legend
    handles = [
        plt.Line2D(
            [0], [0], color="steelblue", marker="o", linestyle="", label="CJE Estimate"
        ),
    ]
    if oracle_values:
        handles.append(
            plt.Line2D(
                [0], [0], color="red", marker="d", linestyle="", label="Oracle Truth"
            )
        )
    if best_policy:
        handles.append(
            plt.Line2D(
                [0], [0], color="green", marker="o", linestyle="", label="Best Policy"
            )
        )

    ax.legend(handles=handles, loc="lower right", fontsize=9)

    # Adjust layout
    plt.tight_layout()

    if save_path:
        save_path = Path(save_path)
        fig.savefig(save_path.with_suffix(".png"), dpi=150, bbox_inches="tight")

    return fig


=== ./cje/visualization/weight_dashboards.py ===

"""Weight diagnostics dashboards for CJE framework.

This module provides two complementary weight diagnostic visualizations:
1. plot_weight_dashboard_summary: 6-panel overview across all policies
2. plot_weight_dashboard_detailed: Individual panels per policy with judge score analysis
"""

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from typing import Dict, Optional, List, Tuple, Any
from pathlib import Path

# Import shared utilities
from ..diagnostics import compute_ess


# ============================================================================
# Summary Dashboard (6-panel multi-policy overview)
# ============================================================================


def plot_weight_dashboard_summary(
    raw_weights_dict: Dict[str, np.ndarray],
    calibrated_weights_dict: Optional[Dict[str, np.ndarray]] = None,
    n_samples: Optional[int] = None,
    save_path: Optional[Path] = None,
    figsize: tuple = (14, 12),
    random_seed: int = 42,
    diagnostics: Optional[Any] = None,
) -> Tuple[plt.Figure, Dict[str, Any]]:
    """Create comprehensive 6-panel weight diagnostics dashboard.

    Displays essential weight diagnostics across all policies:
    - Panel A: ESS comparison (raw vs calibrated)
    - Panel B: Maximum weight concentration
    - Panel C: Weight distributions
    - Panel D: Tail behavior (CCDF)
    - Panel E: Sample efficiency
    - Panel F: Summary table with recommendations

    Args:
        raw_weights_dict: Dict mapping policy names to raw weight arrays
        calibrated_weights_dict: Optional dict of calibrated weights
        n_samples: Total number of samples
        save_path: Optional path to save figure
        figsize: Figure size (width, height)
        random_seed: Random seed for reproducibility
        diagnostics: Optional IPSDiagnostics or DRDiagnostics object

    Returns:
        Tuple of (matplotlib Figure, metrics dict)
    """
    # Check if we have a diagnostics object to use
    from ..diagnostics import IPSDiagnostics, DRDiagnostics

    if diagnostics is not None and isinstance(
        diagnostics, (IPSDiagnostics, DRDiagnostics)
    ):
        # Extract weight info from diagnostics
        if n_samples is None:
            n_samples = diagnostics.n_samples_valid

    policies = list(raw_weights_dict.keys())
    n_policies = len(policies)
    metrics = {}

    # Use calibrated weights if provided, otherwise use raw
    use_calibrated = calibrated_weights_dict is not None
    weights_to_plot = calibrated_weights_dict if use_calibrated else raw_weights_dict

    # Set random seed for reproducibility
    np.random.seed(random_seed)

    # Infer n_samples if not provided - but track per-policy
    if n_samples is None:
        n_samples = len(next(iter(raw_weights_dict.values())))

    # Compute metrics for all policies
    for policy in policies:
        raw_w = raw_weights_dict[policy]
        cal_w = (
            calibrated_weights_dict.get(policy, raw_w)
            if calibrated_weights_dict
            else raw_w
        )

        # Track actual sample size per policy
        policy_n_samples = len(raw_w)

        # ESS metrics
        ess_raw = compute_ess(raw_w)
        ess_cal = compute_ess(cal_w)

        # Sample efficiency: how many samples contribute X% of weight
        sorted_w = np.sort(cal_w)[::-1]
        cumsum_w = np.cumsum(sorted_w)
        total_w = cumsum_w[-1]

        n_for_50 = np.searchsorted(cumsum_w, 0.5 * total_w) + 1
        n_for_90 = np.searchsorted(cumsum_w, 0.9 * total_w) + 1

        # Count extreme weights
        n_above_10 = np.sum(cal_w > 10)
        n_above_100 = np.sum(cal_w > 100)

        metrics[policy] = {
            "ess_raw": ess_raw,
            "ess_cal": ess_cal,
            "ess_raw_frac": ess_raw / policy_n_samples,
            "ess_cal_frac": ess_cal / policy_n_samples,
            "ess_improvement": ess_cal / max(ess_raw, 1e-10),
            "max_weight_raw": np.max(raw_w),
            "max_weight_cal": np.max(cal_w),
            "n_for_50pct": n_for_50,
            "n_for_90pct": n_for_90,
            "n_samples": policy_n_samples,
            "n_above_10": n_above_10,
            "n_above_100": n_above_100,
        }

    # Create figure with 3x2 grid
    fig = plt.figure(figsize=figsize)
    gs = gridspec.GridSpec(
        3, 2, hspace=0.35, wspace=0.35, left=0.08, right=0.95, top=0.94, bottom=0.06
    )

    # Row 1: Core metrics
    ax_ess = fig.add_subplot(gs[0, 0])
    _plot_ess_comparison(ax_ess, metrics, policies)

    ax_max = fig.add_subplot(gs[0, 1])
    _plot_max_weight_comparison(ax_max, metrics, policies)

    # Row 2: Distribution analysis
    ax_transform = fig.add_subplot(gs[1, 0])
    _plot_weight_histograms(
        ax_transform, raw_weights_dict, calibrated_weights_dict, policies
    )

    ax_tail = fig.add_subplot(gs[1, 1])
    if weights_to_plot is not None:
        _plot_tail_ccdf_combined(ax_tail, weights_to_plot, policies)

    # Row 3: Efficiency and summary
    ax_eff = fig.add_subplot(gs[2, 0])
    _plot_sample_efficiency(ax_eff, metrics, policies)

    ax_table = fig.add_subplot(gs[2, 1])
    _plot_summary_table(ax_table, metrics, policies, use_calibrated)

    # Main title
    title = "Weight Diagnostics Dashboard"
    if use_calibrated:
        title += " (Calibrated Weights)"
    plt.suptitle(title, fontsize=14, fontweight="bold")

    # Save if requested
    if save_path:
        save_path = Path(save_path)
        fig.savefig(save_path.with_suffix(".png"), dpi=150, bbox_inches="tight")

    return fig, metrics


# ============================================================================
# Detailed Dashboard (per-policy with judge scores)
# ============================================================================


def plot_weight_dashboard_detailed(
    raw_weights_dict: Dict[str, np.ndarray],
    calibrated_weights_dict: Optional[Dict[str, np.ndarray]] = None,
    n_samples: Optional[int] = None,
    save_path: Optional[Path] = None,
    figsize: tuple = (16, 10),
    random_seed: int = 42,
    diagnostics: Optional[Any] = None,
    **kwargs: Any,
) -> Tuple[plt.Figure, Dict[str, Any]]:
    """Create per-policy weight dashboards with ordering index visualization.

    Creates a grid of subplots, one dashboard per policy, each showing:
    - Weight smoothing by ordering index (g_oof when available, else judge score)
    - ESS and tail diagnostics
    - Clear per-policy view

    Args:
        raw_weights_dict: Dict mapping policy names to raw weight arrays
        calibrated_weights_dict: Optional dict of calibrated weights
        n_samples: Total number of samples
        save_path: Optional path to save figure
        figsize: Figure size (width, height)
        random_seed: Random seed for reproducibility
        diagnostics: Optional IPSDiagnostics or DRDiagnostics object
        **kwargs: Must include either 'judge_scores' dict or 'sampler'
                 Can also include 'ordering_indices' dict and 'calibrator'

    Returns:
        Tuple of (matplotlib Figure, metrics dict)
    """
    np.random.seed(random_seed)

    policies = list(raw_weights_dict.keys())
    n_policies = len(policies)

    # Get judge scores and ordering indices
    judge_scores_dict = kwargs.get("judge_scores", {})
    ordering_indices_dict = kwargs.get("ordering_indices", {})
    sampler = kwargs.get("sampler")
    calibrator = kwargs.get("calibrator")

    # Extract judge scores from sampler if not provided directly
    if not judge_scores_dict and sampler is not None:
        judge_scores_dict = {}
        for policy in policies:
            data = sampler.get_data_for_policy(policy)
            if data:
                scores = np.array([d.get("judge_score", np.nan) for d in data])
                valid = ~np.isnan(scores)
                if valid.sum() > 0:
                    judge_scores_dict[policy] = scores[valid]

    # Try to compute ordering indices (g_oof) if not provided but calibrator available
    if not ordering_indices_dict and calibrator is not None and sampler is not None:
        if hasattr(calibrator, "predict_oof"):
            ordering_indices_dict = {}
            for policy in policies:
                data = sampler.get_data_for_policy(policy)
                if data:
                    # Get judge scores and compute fold IDs from prompt_ids
                    judge_scores = np.array(
                        [d.get("judge_score", np.nan) for d in data]
                    )
                    # Compute folds on-demand from prompt_ids
                    from ..data.folds import get_fold

                    fold_list = [
                        (
                            get_fold(d.get("prompt_id"), 5, 42)
                            if d.get("prompt_id")
                            else None
                        )
                        for d in data
                    ]

                    # Check if we have valid fold IDs
                    if all(v is not None for v in fold_list) and len(fold_list) == len(
                        judge_scores
                    ):
                        fold_ids = np.asarray(fold_list, dtype=int)
                        try:
                            # Compute cross-fitted predictions
                            g_oof = calibrator.predict_oof(judge_scores, fold_ids)
                            if g_oof is not None and not np.all(g_oof == 0):
                                ordering_indices_dict[policy] = g_oof
                        except:
                            pass  # Fall back to judge scores

    # Determine grid layout
    if n_policies <= 2:
        rows, cols = 1, n_policies
    elif n_policies <= 4:
        rows, cols = 2, 2
    elif n_policies <= 6:
        rows, cols = 2, 3
    else:
        rows = int(np.ceil(n_policies / 3))
        cols = 3

    # Create figure
    fig, axes = plt.subplots(rows, cols, figsize=figsize, squeeze=False)

    # Flatten axes for easier iteration
    axes_flat = axes.flatten()

    # Hide extra subplots
    for i in range(n_policies, len(axes_flat)):
        axes_flat[i].axis("off")

    # Metrics storage
    all_metrics = {}

    # Plot each policy
    for idx, policy in enumerate(policies):
        ax = axes_flat[idx]

        # Get data for this policy
        raw_w = raw_weights_dict[policy]
        cal_w = (
            calibrated_weights_dict.get(policy, raw_w)
            if calibrated_weights_dict
            else raw_w
        )
        judge_scores = judge_scores_dict.get(policy, None)
        ordering_index = ordering_indices_dict.get(policy, None)

        # Use ordering index if available, otherwise fall back to judge scores
        plot_index = ordering_index if ordering_index is not None else judge_scores
        index_label = (
            "Calibrated Reward g(s)" if ordering_index is not None else "Judge Score"
        )

        # Compute metrics for this policy
        ess_raw = compute_ess(raw_w)
        ess_cal = compute_ess(cal_w)
        uplift = ess_cal / max(ess_raw, 1e-12)

        # Top 1% mass
        w_sorted = np.sort(raw_w)[::-1]
        k = max(1, int(len(w_sorted) * 0.01))
        top1_raw = w_sorted[:k].sum() / max(w_sorted.sum(), 1e-12)

        w_sorted = np.sort(cal_w)[::-1]
        k = max(1, int(len(w_sorted) * 0.01))
        top1_cal = w_sorted[:k].sum() / max(w_sorted.sum(), 1e-12)

        # Store metrics
        all_metrics[policy] = {
            "ess_raw": ess_raw,
            "ess_cal": ess_cal,
            "ess_improvement": uplift,
            "top1_raw": top1_raw,
            "top1_cal": top1_cal,
            "n_samples": len(raw_w),
            "ordering_index_used": (
                "g_oof" if ordering_index is not None else "judge_score"
            ),
        }

        if plot_index is not None and len(plot_index) == len(raw_w):
            # Plot weight smoothing by ordering index
            _plot_single_policy_weight_smoothing(
                ax,
                plot_index,
                raw_w,
                cal_w,
                policy,
                ess_raw,
                ess_cal,
                uplift,
                top1_raw,
                top1_cal,
                index_label=index_label,
            )
        else:
            # Fallback: simple histogram comparison
            _plot_single_policy_weight_histogram(
                ax, raw_w, cal_w, policy, ess_raw, ess_cal, uplift, top1_raw, top1_cal
            )

    # Main title
    fig.suptitle(
        f"Weight Diagnostics by Policy (n={n_samples or len(next(iter(raw_weights_dict.values())))})",
        fontsize=14,
        fontweight="bold",
        y=0.98,
    )

    plt.tight_layout(rect=[0, 0, 1, 0.96])

    # Save if requested
    if save_path:
        save_path = Path(save_path)
        fig.savefig(save_path.with_suffix(".png"), dpi=150, bbox_inches="tight")

    return fig, all_metrics


# ============================================================================
# Helper functions for summary dashboard
# ============================================================================


def _plot_ess_comparison(ax: Any, metrics: Dict, policies: List[str]) -> None:
    """Plot ESS as effective samples (not percentage)."""
    n_policies = len(policies)
    x = np.arange(n_policies)
    width = 0.35

    # Get values
    raw_ess = [metrics[p]["ess_raw"] for p in policies]
    cal_ess = [metrics[p]["ess_cal"] for p in policies]
    improvements = [metrics[p]["ess_improvement"] for p in policies]

    # Use consistent tab10 colormap
    colors = plt.cm.get_cmap("tab10")

    # Plot bars
    bars1 = ax.bar(
        x - width / 2, raw_ess, width, label="Raw", color=colors(0), alpha=0.7
    )
    bars2 = ax.bar(
        x + width / 2, cal_ess, width, label="Calibrated", color=colors(1), alpha=0.7
    )

    # Labels on bars with improvement factor
    for i, (r, c, imp) in enumerate(zip(raw_ess, cal_ess, improvements)):
        ax.text(i - width / 2, r + 5, f"{r:.0f}", ha="center", fontsize=8)
        ax.text(i + width / 2, c + 5, f"{c:.0f}", ha="center", fontsize=8)
        if imp > 1.5:  # Only show significant improvements
            ax.text(
                i + width / 2,
                c / 2,
                f"+{imp:.1f}×",
                ha="center",
                fontsize=7,
                style="italic",
                color="darkgreen",
            )

    ax.set_xticks(x)
    ax.set_xticklabels(policies, rotation=45, ha="right")
    ax.set_ylabel("Effective Samples")
    ax.set_title("A. Effective Sample Size")
    ax.legend(loc="upper left", fontsize=9)
    ax.grid(True, alpha=0.3, axis="y")

    # Add reference lines
    if policies:
        min_n_samples = min(metrics[p]["n_samples"] for p in policies)
        max_n_samples = max(metrics[p]["n_samples"] for p in policies)

        if max_n_samples > min_n_samples * 1.1:
            ax.text(
                0.02,
                0.98,
                f"n varies: {min_n_samples}-{max_n_samples}",
                transform=ax.transAxes,
                fontsize=7,
                va="top",
            )

        # 50% line - good threshold
        ax.axhline(
            min_n_samples * 0.5, color="green", linestyle="--", alpha=0.3, linewidth=1
        )
        ax.text(
            n_policies - 0.5,
            min_n_samples * 0.5 + 10,
            "50% (good)",
            fontsize=7,
            color="green",
            alpha=0.7,
            ha="right",
        )

        # 10% line - warning threshold
        ax.axhline(
            min_n_samples * 0.1, color="orange", linestyle="--", alpha=0.3, linewidth=1
        )
        ax.text(
            n_policies - 0.5,
            min_n_samples * 0.1 + 10,
            "10% (marginal)",
            fontsize=7,
            color="orange",
            alpha=0.7,
            ha="right",
        )


def _plot_max_weight_comparison(ax: Any, metrics: Dict, policies: List[str]) -> None:
    """Plot maximum weights showing weight concentration risk."""
    n_policies = len(policies)
    x = np.arange(n_policies)
    width = 0.35

    raw_max = [metrics[p]["max_weight_raw"] for p in policies]
    cal_max = [metrics[p]["max_weight_cal"] for p in policies]

    # Use log scale if any weight > 10
    if max(raw_max + cal_max) > 10:
        ax.set_yscale("log")

    colors = plt.cm.get_cmap("tab10")

    bars1 = ax.bar(
        x - width / 2, raw_max, width, label="Raw", color=colors(0), alpha=0.7
    )
    bars2 = ax.bar(
        x + width / 2, cal_max, width, label="Calibrated", color=colors(1), alpha=0.7
    )

    # Add value labels
    for i, (r, c) in enumerate(zip(raw_max, cal_max)):
        ax.text(
            i - width / 2,
            r * 1.1 if r > 0 else 0.1,
            f"{r:.0f}",
            ha="center",
            fontsize=8,
            fontweight="bold",
        )
        ax.text(
            i + width / 2,
            c * 1.1 if c > 0 else 0.1,
            f"{c:.0f}",
            ha="center",
            fontsize=8,
            fontweight="bold",
        )

    # Reference lines
    ax.axhline(1, color="gray", linestyle=":", alpha=0.5)
    ax.axhline(10, color="orange", linestyle="--", alpha=0.5)
    ax.axhline(100, color="red", linestyle="--", alpha=0.5)

    ax.text(n_policies - 0.5, 1.2, "Target", fontsize=7, color="gray", alpha=0.7)
    ax.text(n_policies - 0.5, 12, "High", fontsize=7, color="orange", alpha=0.7)
    ax.text(n_policies - 0.5, 120, "Extreme", fontsize=7, color="red", alpha=0.7)

    ax.set_xticks(x)
    ax.set_xticklabels(
        policies,
        rotation=45 if n_policies > 3 else 0,
        ha="right" if n_policies > 3 else "center",
    )
    ax.set_ylabel("Weight of Most Important Sample")
    ax.set_title("B. Weight Concentration: Single Sample Dominance")
    ax.legend(loc="upper left", fontsize=9)
    ax.grid(True, alpha=0.3, axis="y")


def _plot_sample_efficiency(ax: Any, metrics: Dict, policies: List[str]) -> None:
    """Plot how many samples contribute 50% and 90% of weight."""
    n_policies = len(policies)

    # Prepare data
    data = []
    for p in policies:
        n_50 = metrics[p]["n_for_50pct"]
        n_90 = metrics[p]["n_for_90pct"]
        n_total = metrics[p]["n_samples"]
        n_rest = n_total - n_90

        # Percentages for stacking
        pct_50 = 100 * n_50 / n_total
        pct_90_50 = 100 * (n_90 - n_50) / n_total
        pct_rest = 100 * n_rest / n_total

        data.append((pct_50, pct_90_50, pct_rest, n_50, n_90))

    # Create stacked bars
    x = np.arange(n_policies)
    colors = plt.cm.get_cmap("tab10")

    bars1 = ax.bar(
        x,
        [d[0] for d in data],
        label=f"Samples carrying 50% of weight",
        color=colors(3),
        alpha=0.8,
    )

    bars2 = ax.bar(
        x,
        [d[1] for d in data],
        bottom=[d[0] for d in data],
        label="Additional samples for 90% weight",
        color=colors(1),
        alpha=0.6,
    )

    bottom_sum = [d[0] + d[1] for d in data]
    bars3 = ax.bar(
        x,
        [d[2] for d in data],
        bottom=bottom_sum,
        label="Samples with minimal weight (<10%)",
        color="lightgray",
        alpha=0.4,
    )

    # Add text annotations
    for i, (p50, p90_50, prest, n50, n90) in enumerate(data):
        policy = policies[i]
        n_total = metrics[policy]["n_samples"]

        if p50 > 3:  # Only show if segment is large enough
            label_50 = f"{n50}\n({n50/n_total*100:.0f}%)"
            ax.text(
                i,
                p50 / 2,
                label_50,
                ha="center",
                va="center",
                fontsize=8,
                color="white",
                fontweight="bold",
            )

    ax.set_xticks(x)
    ax.set_xticklabels(
        policies,
        rotation=45 if n_policies > 3 else 0,
        ha="right" if n_policies > 3 else "center",
    )
    ax.set_ylabel("% of Total Samples")
    ax.set_ylim(0, 100)
    ax.set_title("E. Sample Efficiency: How Many Samples Actually Matter?")
    ax.legend(loc="upper right", fontsize=8)
    ax.grid(True, alpha=0.3, axis="y")


def _plot_weight_histograms(
    ax: Any, raw_weights_dict: Dict, calibrated_weights_dict: Dict, policies: List[str]
) -> None:
    """Plot weight distribution histograms comparing raw vs calibrated."""
    for i, policy in enumerate(policies[:3]):  # Limit to 3 policies for clarity
        raw_w = raw_weights_dict[policy]
        cal_w = (
            calibrated_weights_dict.get(policy, raw_w)
            if calibrated_weights_dict
            else raw_w
        )

        # Create log-spaced bins
        raw_positive = raw_w[raw_w > 0]
        cal_positive = cal_w[cal_w > 0]

        if len(raw_positive) > 0 and len(cal_positive) > 0:
            min_val = min(raw_positive.min(), cal_positive.min())
            max_val = max(raw_positive.max(), cal_positive.max())
            bins = np.logspace(np.log10(max(min_val, 1e-6)), np.log10(max_val), 40)

            ax.hist(
                raw_positive, bins=bins, alpha=0.3, label=f"{policy} raw", density=True
            )
            ax.hist(
                cal_positive,
                bins=bins,
                alpha=0.5,
                label=f"{policy} cal",
                density=True,
                histtype="step",
                linewidth=2,
            )

    ax.set_xscale("log")
    ax.set_xlabel("Weight")
    ax.set_ylabel("Density")
    ax.set_title("C. Weight Distributions")
    ax.legend(loc="best", fontsize=8)
    ax.grid(True, alpha=0.3)


def _plot_tail_ccdf_combined(ax: Any, weights_dict: Dict, policies: List[str]) -> None:
    """CCDF on log-log scale, all policies overlaid."""
    colors = plt.cm.get_cmap("Set2")(np.linspace(0, 1, len(policies)))

    for policy, color in zip(policies, colors):
        weights = weights_dict[policy]

        # Sort weights and compute CCDF
        w_sorted = np.sort(weights[weights > 0])
        if len(w_sorted) == 0:
            continue

        # CCDF: fraction of weights >= x
        ccdf = 1.0 - np.arange(len(w_sorted)) / len(w_sorted)

        ax.loglog(w_sorted, ccdf, label=policy, linewidth=2, alpha=0.7, color=color)

    ax.set_xlabel("Weight")
    ax.set_ylabel("P(W ≥ x)")
    ax.set_title("D. Tail Behavior (CCDF)")
    ax.legend(loc="lower left", fontsize=9)
    ax.grid(True, which="both", alpha=0.3)

    # Add reference lines
    ax.axvline(1.0, color="gray", linestyle="--", alpha=0.5)
    ax.axvline(10.0, color="orange", linestyle="--", alpha=0.5)
    ax.axvline(100.0, color="red", linestyle="--", alpha=0.5)


def _plot_summary_table(
    ax: Any, metrics: Dict, policies: List[str], use_calibrated: bool
) -> None:
    """Summary table with status and recommendations."""
    ax.axis("off")

    # Prepare table data
    headers = ["Policy", "ESS", "Status", "Recommendation"]
    rows = []

    for policy in policies:
        m = metrics[policy]
        ess_frac = m["ess_cal_frac"] if use_calibrated else m["ess_raw_frac"]
        ess_val = m["ess_cal"] if use_calibrated else m["ess_raw"]

        # Status based on ESS
        if ess_frac > 0.5:
            status = "Excellent"
            rec = "Ready for production"
        elif ess_frac > 0.2:
            status = "Good"
            rec = "Usable with caution"
        elif ess_frac > 0.1:
            status = "Marginal"
            rec = "Consider more data"
        else:
            status = "Poor"
            rec = "Insufficient overlap"

        # Add calibration recommendation if relevant
        if not use_calibrated and m["ess_improvement"] > 2.0:
            rec = f"Use calibration ({m['ess_improvement']:.1f}× gain)"

        rows.append(
            [
                policy[:12],  # Truncate long names
                f"{ess_val:.0f} ({100*ess_frac:.0f}%)",
                status,
                rec[:20],  # Truncate long recommendations
            ]
        )

    # Create table
    table = ax.table(
        cellText=rows,
        colLabels=headers,
        cellLoc="left",
        loc="center",
        colWidths=[0.2, 0.25, 0.2, 0.35],
    )

    table.auto_set_font_size(False)
    table.set_fontsize(9)
    table.scale(1.0, 2.0)

    # Style header
    for i in range(len(headers)):
        table[(0, i)].set_facecolor("#E8E8E8")
        table[(0, i)].set_text_props(weight="bold")

    # Color code by status
    for i, policy in enumerate(policies):
        ess_frac = (
            metrics[policy]["ess_cal_frac"]
            if use_calibrated
            else metrics[policy]["ess_raw_frac"]
        )
        if ess_frac > 0.5:
            color = "#90EE90"  # Light green
        elif ess_frac > 0.2:
            color = "#FFFACD"  # Light yellow
        elif ess_frac > 0.1:
            color = "#FFE4B5"  # Light orange
        else:
            color = "#FFB6C1"  # Light red
        table[(i + 1, 2)].set_facecolor(color)


# ============================================================================
# Helper functions for detailed dashboard
# ============================================================================


def _plot_single_policy_weight_smoothing(
    ax: Any,
    ordering_index: np.ndarray,
    raw_w: np.ndarray,
    cal_w: np.ndarray,
    policy: str,
    ess_raw: float,
    ess_cal: float,
    uplift: float,
    top1_raw: float,
    top1_cal: float,
    index_label: str = "Judge Score",
) -> None:
    """Plot weights vs ordering index with calibration effect.

    The ordering index can be either judge scores or calibrated rewards g(s),
    depending on what was used for SIMCal calibration.
    """

    # Filter to valid values
    mask = (
        np.isfinite(ordering_index)
        & np.isfinite(raw_w)
        & np.isfinite(cal_w)
        & (raw_w > 0)
        & (cal_w > 0)
    )
    S = ordering_index[mask]
    W_raw = raw_w[mask]
    W_cal_actual = cal_w[mask]

    n = len(S)

    # Sort by ordering index
    sort_idx = np.argsort(S)
    S_sorted = S[sort_idx]
    W_raw_sorted = W_raw[sort_idx]
    W_cal_actual_sorted = W_cal_actual[sort_idx]

    # Plot raw weights vs judge scores
    if n > 2000:
        # Subsample for visibility
        step = max(1, n // 1000)
        indices = np.arange(0, n, step)
        ax.scatter(
            S_sorted[indices],
            W_raw_sorted[indices],
            s=2,
            alpha=0.2,
            color="C0",
            label="raw weights",
            rasterized=True,
        )
    else:
        ax.scatter(
            S,
            W_raw,
            s=3,
            alpha=0.3,
            color="C0",
            label="raw weights",
        )

    # Plot actual calibrated weights as thick line
    ax.plot(
        S_sorted,
        W_cal_actual_sorted,
        color="C2",
        linewidth=2.5,
        label="calibrated",
        zorder=10,
    )

    # Add horizontal line at y=1 (target mean)
    ax.axhline(1.0, color="gray", linestyle=":", alpha=0.5, linewidth=1)

    # Set log scale for y-axis only
    ax.set_yscale("log")

    # Compute variance ratio for annotation
    var_ratio_actual = np.var(W_cal_actual) / np.var(W_raw) if np.var(W_raw) > 0 else 0

    # Title with comprehensive diagnostics
    ax.set_title(
        f"{policy}\n"
        f"ESS: {ess_raw:.0f}→{ess_cal:.0f} ({uplift:.1f}×), "
        f"Var ratio: {var_ratio_actual:.2f}",
        fontsize=10,
    )
    ax.set_xlabel(index_label, fontsize=9)
    ax.set_ylabel("Weight (log scale)", fontsize=9)
    ax.legend(loc="best", fontsize=8, frameon=False)
    ax.grid(True, alpha=0.3, which="both", linestyle=":")
    ax.tick_params(labelsize=8)


def _plot_single_policy_weight_histogram(
    ax: Any,
    raw_w: np.ndarray,
    cal_w: np.ndarray,
    policy: str,
    ess_raw: float,
    ess_cal: float,
    uplift: float,
    top1_raw: float,
    top1_cal: float,
) -> None:
    """Fallback: histogram comparison when judge scores unavailable."""

    # Create log-spaced bins
    raw_positive = raw_w[raw_w > 0]
    cal_positive = cal_w[cal_w > 0]

    if len(raw_positive) > 0 and len(cal_positive) > 0:
        min_val = min(raw_positive.min(), cal_positive.min())
        max_val = max(raw_positive.max(), cal_positive.max())
        bins = np.logspace(np.log10(max(min_val, 1e-6)), np.log10(max_val), 40)

        # Plot histograms
        ax.hist(
            raw_positive, bins=bins, alpha=0.4, color="C0", label="raw", density=True
        )
        ax.hist(
            cal_positive,
            bins=bins,
            alpha=0.6,
            color="C1",
            label="calibrated",
            density=True,
            histtype="step",
            linewidth=2,
        )

        ax.set_xscale("log")
        ax.set_xlabel("Weight", fontsize=9)
        ax.set_ylabel("Density", fontsize=9)

    # Title with diagnostics
    ax.set_title(
        f"{policy} (no judge scores)\nESS: {ess_raw:.0f}→{ess_cal:.0f} ({uplift:.1f}×), "
        f"Top1%: {100*top1_raw:.1f}%→{100*top1_cal:.1f}%",
        fontsize=10,
    )
    ax.legend(loc="best", fontsize=8)
    ax.grid(True, alpha=0.3)
    ax.tick_params(labelsize=8)


# ============================================================================
# Backward compatibility aliases
# ============================================================================


========================================
EXPORT SUMMARY
========================================

Total Python files included: 98
Total lines in export:    29210
File size: 1.0M
Generated: Wed Aug 27 14:06:35 PDT 2025
