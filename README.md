# Causal Judge Evaluation (CJE)

Fast, accurate model evaluation using AI judges with causal inference.

## Overview

CJE solves the problem of evaluating LLM improvements by:
- Using AI judges instead of expensive human evaluation
- Applying causal inference to correct for judge biases
- Achieving 70%+ variance reduction and 10Ã— GPU efficiency
- Providing calibrated uncertainty estimates

## Installation

```bash
# Clone the repository
git clone https://github.com/yolandelandsberg/causal-judge-evaluation.git
cd causal-judge-evaluation

# Install with Poetry (recommended)
make dev-setup

# Or install directly
poetry install
```

## Quick Start

```python
from cje.config.unified import simple_config

# Create and run evaluation
config = simple_config(
    dataset_name="./data/test.jsonl",
    logging_model="gpt-3.5-turbo",
    logging_provider="openai",
    target_model="gpt-4",
    target_provider="openai",
    judge_model="gpt-4o",
    judge_provider="openai",
    estimator_name="DRCPO"
)

results = config.run()
print(f"Target policy estimate: {results['results']['DRCPO']['estimates'][0]:.3f}")
```

## Key Features

- **Multiple Estimators**: IPS, SNIPS, CalibratedIPS, DRCPO, MRDR
- **Uncertainty Quantification**: Built-in support for judge uncertainty
- **Provider Support**: OpenAI, Anthropic, Fireworks, Together, Google
- **Efficient**: Teacher forcing for unbiased log probabilities
- **Robust**: Automatic calibration and cross-fitting

## Documentation

- [Full Documentation](https://causal-judge-evaluation.readthedocs.io)
- [API Reference](https://causal-judge-evaluation.readthedocs.io/api)
- [Arena 10K Experiment](experiments/arena_10k_oracle/README.md)

## Development

```bash
# Run tests
make test

# Run linting (required before commits)
make lint

# Build documentation
make docs
```

## License

MIT License - see LICENSE file for details.